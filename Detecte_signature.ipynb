{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection de signatures dans des images : \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  \n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "\n",
    "pdf_folder = \"Assurances/Documents Assurances\"\n",
    "image_output_folder = \"Assurances/Converted_Images\"\n",
    "train_folder = os.path.join(image_output_folder, \"train\")\n",
    "test_folder = os.path.join(image_output_folder, \"test\")\n",
    "\n",
    "\n",
    "os.makedirs(train_folder, exist_ok=True)\n",
    "os.makedirs(test_folder, exist_ok=True)\n",
    "\n",
    "def convert_pdf_to_images(pdf_path, output_dir):\n",
    "    \"\"\"\n",
    "    Convertit un PDF en images par page et les sauvegarde dans le répertoire spécifié.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path) \n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc[page_num]\n",
    "            pix = page.get_pixmap()\n",
    "            image_path = os.path.join(output_dir, f\"{os.path.basename(pdf_path).replace('.pdf', '')}_page_{page_num + 1}.png\")\n",
    "            pix.save(image_path)  \n",
    "        doc.close()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la conversion du fichier PDF : {pdf_path}. Erreur : {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "all_images = []\n",
    "invalid_files = []\n",
    "print(\"Conversion des fichiers PDF en images...\")\n",
    "for root, _, files in os.walk(pdf_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(root, file)\n",
    "            print(f\"Traitement du fichier : {pdf_path}\")\n",
    "            if convert_pdf_to_images(pdf_path, image_output_folder):\n",
    "                print(f\"PDF converti avec succès : {file}\")\n",
    "            else:\n",
    "                print(f\"Fichier invalide ou conversion échouée : {file}\")\n",
    "                invalid_files.append(pdf_path)\n",
    "\n",
    "if invalid_files:\n",
    "    print(\"\\nFichiers PDF invalides ou corrompus détectés :\")\n",
    "    for invalid_file in invalid_files:\n",
    "        print(invalid_file)\n",
    "else:\n",
    "    print(\"\\nTous les fichiers PDF ont été traités avec succès.\")\n",
    "\n",
    "\n",
    "for root, _, files in os.walk(image_output_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\"):\n",
    "            all_images.append(os.path.join(root, file))\n",
    "\n",
    "\n",
    "train_images, test_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(\"\\nOrganisation des images en ensembles d'entraînement et de test...\")\n",
    "for image_path in train_images:\n",
    "    shutil.move(image_path, train_folder)\n",
    "\n",
    "for image_path in test_images:\n",
    "    shutil.move(image_path, test_folder)\n",
    "\n",
    "print(f\"\\nNombre total d'images générées : {len(all_images)}\")\n",
    "print(f\"Images d'entraînement : {len(os.listdir(train_folder))}\")\n",
    "print(f\"Images de test : {len(os.listdir(test_folder))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Vérification des fichiers ====\n",
      "\n",
      "Train: 1010 images, 1010 labels\n",
      "Valid: 218 images, 218 labels\n",
      "\n",
      "[INFO] Les nombres d'images et de labels sont corrects.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "signature_yaml_path = \"./yolo_config/signature.yaml\" \n",
    "pretrained_model = \"./yolov11n.pt\"  \n",
    "train_images_path = \"./datasets/signature/train/images\"\n",
    "train_labels_path = \"./datasets/signature/train/labels\"\n",
    "valid_images_path = \"./datasets/signature/valid/images\"\n",
    "valid_labels_path = \"./datasets/signature/valid/labels\"\n",
    "output_model_path = \"Assurances/YOLO_Trained/best4.pt\"\n",
    "\n",
    "\n",
    "def count_files(directory, extension=None):\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    if extension:\n",
    "        files = [f for f in files if f.endswith(extension)]\n",
    "    return len(files), files\n",
    "\n",
    "\n",
    "print(\"\\n==== Vérification des fichiers ====\\n\")\n",
    "\n",
    "\n",
    "train_img_count, train_images = count_files(train_images_path, \".jpg\")  \n",
    "train_lbl_count, train_labels = count_files(train_labels_path, \".txt\")\n",
    "print(f\"Train: {train_img_count} images, {train_lbl_count} labels\")\n",
    "if train_img_count != train_lbl_count:\n",
    "    print(\"[ERREUR] Le nombre d'images et de labels dans TRAIN ne correspond pas!\")\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "valid_img_count, valid_images = count_files(valid_images_path, \".jpg\")\n",
    "valid_lbl_count, valid_labels = count_files(valid_labels_path, \".txt\")\n",
    "print(f\"Valid: {valid_img_count} images, {valid_lbl_count} labels\")\n",
    "if valid_img_count != valid_lbl_count:\n",
    "    print(\"[ERREUR] Le nombre d'images et de labels dans VALID ne correspond pas!\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"\\n[INFO] Les nombres d'images et de labels sont corrects.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: ['10376-Mansour-Mohamed_jpg.rf.85a2438d63b50b73d1bdefb87576755f.jpg', '10565-Vader-Bonnie_jpg.rf.60e94e363ff41352a6a6116c95cd3252.jpg', '1058-Khoury-Farjallah_jpg.rf.d70b93a3e541261b2b1fc2127e5e51f6.jpg', '10776-Roach-Christopher_jpg.rf.0d70ee0dec16989ed46e504b4b359fd6.jpg', '10776-Roach-Christopher_jpg.rf.0d880c1a8b2c905b8ea665b07001ddde.jpg', '10776-Roach-Christopher_jpg.rf.5ca37281d083777e98270fee6acdf10e.jpg', '10785_SchroederAlan_jpg.rf.1b138eb0fe1c75427b7b82792a4a1642.jpg', '10785_SchroederAlan_jpg.rf.2dccb462fe01ee69c063b037305ad225.jpg', '10926-Mohan-Venkatachala_jpg.rf.14819c5f25cbee446919f142c7f90bea.jpg', '11172_Van-FlandernGeoffrey_jpg.rf.c27a9111707d7dfff2b8aa48401b9ad3.jpg', '11250-Farrell-Estelle_jpg.rf.82701e154733d5be17bf2fe70924d4f1.jpg', '11508-Stanton-Robert_jpg.rf.15ea840bf2d78382d869ab890354d16e.jpg', '11508-Stanton-Robert_jpg.rf.d5341d1e491e3c60accc25a270c80945.jpg', '11697_EsmendeSean_jpg.rf.3d5998add4c6af29bff46c4bbb1e2414.jpg', '11697_EsmendeSean_jpg.rf.3fccf65940a1ca40d1202c7891085f3b.jpg', '11697_EsmendeSean_jpg.rf.badadfd417e367df94345cce100dd21d.jpg', '11763-Kuhn-Michael_jpg.rf.ab70de879b54d23ec3797dcb59f862c1.jpg', '11763-Kuhn-Michael_jpg.rf.f4c30f67b5d1ed9c76809eb3973a03db.jpg', '11998-Chambers-Aaron_jpg.rf.65803689f1e305aacd549437b5c6c90c.jpg', '12045-Charles-James_jpg.rf.6b0d67b1add2e6a6e614b156bb324ca5.jpg', '12158-Emmer-Anthony_jpg.rf.62abe516b475de405c5a5ca4fa3e6845.jpg', '12185-Cash-Bradley_jpg.rf.5eed5c4310742d3568535019fac65b32.jpg', '12232-Rayes-Ayman_jpg.rf.5194cc4af91bb980874e829e963ef374.jpg', '12232-Rayes-Ayman_jpg.rf.b733659538dac301f358ffc5e237ad6f.jpg', '12286_AtluriPrasant_jpg.rf.185f33a931c5a5d2b61c8f2b5e55a271.jpg', '12409-Shadid-Hythem_jpg.rf.1f262d2c18b5621bebccea363544e65f.jpg', '12409-Shadid-Hythem_jpg.rf.30b04044be6661fb144f96f91522f070.jpg', '12409-Shadid-Hythem_jpg.rf.8fdf8d4583756a2d8d2d76aa11c9c1fc.jpg', '12527-Thampi-Samuel_jpg.rf.89885a2260a5b63993c737919b387c53.jpg', '12559-Sohal-Ajendra_jpg.rf.7927719188bcb0e1f7fcef25fbc2fb0f.jpg', '12559-Sohal-Ajendra_jpg.rf.822e156aa355fa64ebdd3506612ae061.jpg', '12559-Sohal-Ajendra_jpg.rf.8fd165a3fe97ec5987ebef7fe43d95e6.jpg', '12588-Cole-George_jpg.rf.194e6669142fce87661397348932e9ca.jpg', '12588-Cole-George_jpg.rf.69408315487c0c761001ccd2761d7fc6.jpg', '12588-Cole-George_jpg.rf.cd552dbdc09fe06c50ef4cb020c1ba82.jpg', '12782-Derian-Thomas-Craig_jpg.rf.91158f9a4926786dd58d093c461ac038.jpg', '12782-Derian-Thomas-Craig_jpg.rf.f6099c68c8d30b2c014ffe41900a1a1a.jpg', '12851-Padula-Vincent_jpg.rf.a40a6831be8f6536ce324f044664dfe7.jpg', '12851-Padula-Vincent_jpg.rf.d832645a25f857d9df9ffdeb0ef105e6.jpg', '13006-Kao-Cyrus_jpg.rf.c562a5062a1a9fa01350d5c7e3bf53d8.jpg', '1336-Doninger-Nicholas_jpg.rf.8105528044f8d03150ae32324428f45f.jpg', '1339-Madireddi-L-Neena_jpg.rf.43a4dfd7d12111e6ebcdc1fd74a336a9.jpg', '13429-Withiam-Leitch-Sherry_jpg.rf.db0efbe1e0e85b5d175784bd20b3d810.jpg', '13461-Brooks-Harry-Leon_jpg.rf.4127f4329bee6175c1a3a1f1d61e3fa2.jpg', '13461-Brooks-Harry-Leon_jpg.rf.8fe2a545e0c07e38dcf9298adf1f07da.jpg', '13461-Brooks-Harry-Leon_jpg.rf.ab0b8e448fee935d08d0d38bcbb6e546.jpg', '13478-Siegert-John_jpg.rf.332e809f34cf78d8c4eacbf0cc74f6b2.jpg', '13485-Zaydon-Jr-Thomas_jpg.rf.9d294e4cc05a468c1863326d079d2395.jpg', '13649-Kezmarsky-Mary-Ann_jpg.rf.a787803f5fd33911066317be6de3187e.jpg', '13653-Simon-Robert_jpg.rf.a354bf552874d646a17e9c72eb9476df.jpg', '13754-Brooks-Francis_jpg.rf.4ac053a56c9a67f0c25e7d0e263827fc.jpg', '13754-Brooks-Francis_jpg.rf.967913426867ee6acadf842603a74404.jpg', '13754-Brooks-Francis_jpg.rf.d9466e8b66209070780cb150da2f0c37.jpg', '13759-Eilender-Lawrence_jpg.rf.78dd09c27281ebf146f2084052039424.jpg', '13874-Forrest-Andrew_jpg.rf.4d2d7095b22d09998dc3584e9584658a.jpg', '13874-Forrest-Andrew_jpg.rf.bc09eb71ce2afed4fa782aaae822abf1.jpg', '14045-Mohan-Vivek_jpg.rf.58fd46dd343335db543992ddb534de69.jpg', '14045-Mohan-Vivek_jpg.rf.9b34cd4b115f59efb1a5f8e569ebb553.jpg', '14045-Mohan-Vivek_jpg.rf.d7c90a826edc87a414ec134aac1fd0ba.jpg', '14068-Vitello-William_jpg.rf.c40509641858ff2e9936e2c533ce3fad.jpg', '14145-Redfield-Joel_jpg.rf.8f49b5ff6794b5c294f3f37491343938.jpg', '14145-Redfield-Joel_jpg.rf.f08dac42e760fe9c2042ceacaf9c11ac.jpg', '14202_BalaramAjay_jpg.rf.4ec5182f3541f588bc6f5486a15f70e9.jpg', '14227-Moise-Rudolph_jpg.rf.e3373cbbfd95fcc94faea86e05572d06.jpg', '14365-Waxweiler-Weston_jpg.rf.99e9cf1034660e5ff03115a1c36efafa.jpg', '14454-Bassi-Sharon_jpg.rf.0e88d13db4293f052abd76bd968add1a.jpg', '14454-Bassi-Sharon_jpg.rf.96b980e7506cabcf638e71e3a8d3ebc6.jpg', '14489-Fallen-Mark_jpg.rf.930e931b7f5e2ae686c77c6d6b0e8396.jpg', '14491-Wagner-Elliott_jpg.rf.a830940091e1510cb9da22997ec0162f.jpg', '14491-Wagner-Elliott_jpg.rf.c438737fd051b9b6545ad59eefb33d8c.jpg', '14616-Cage-Jason-Matthew_jpg.rf.a14496818ad1356b78a0299fe35d2b2f.jpg', '14822-Kiltinen-Anne_jpg.rf.d3c93d6b2f44ca682a27083f0fcc63a4.jpg', '15083-Ducharme-Stanley_jpg.rf.834442ee2fbab0c35e324f430b216e4d.jpg', '15385-Walker-III-John_jpg.rf.0191d3c7fe696cfc6973085dcb9b39f8.jpg', '15385-Walker-III-John_jpg.rf.b429b28c984ac1344cda8441d382ab38.jpg', '15431-Chen-Qing-Min_jpg.rf.9a56de9a04778b105ad1f53e1ce08f4e.jpg', '15445-Zimmerman-Ryan_jpg.rf.7ebae389d0468f33310d75daa664ed47.jpg', '15744-Gluck-Gabriel_jpg.rf.ac38cc0be601f3737e7aa44f4db8d515.jpg', '15799-Hubbard-Jr-James_jpg.rf.2ff3bdce17bb41d6aa0326dab74246e0.jpg', '15817-Durrani-Timur_jpg.rf.9c8488f6f1fa3df044f67b9852ca6f76.jpg', '15839-Scott-Douglas_jpg.rf.49ed239e71afa311d2927920b60fb265.jpg', '16004-Wilson-Jr-Richard_jpg.rf.2915c95f14cede213eaa93ba8742658c.jpg', '16004-Wilson-Jr-Richard_jpg.rf.85f70e5cd635ccdcf96d9e19bb2ae072.jpg', '16004-Wilson-Jr-Richard_jpg.rf.f3de3ad4bff2ea89d7b72a5d6a44f5cd.jpg', '16435-Carrillo-Adriana_jpg.rf.20cd9231c4097d886e248318e200b331.jpg', '16435-Carrillo-Adriana_jpg.rf.ac9b6cf28771a583c858a219fc84c46f.jpg', '16601-Aull-Susan_jpg.rf.00d9efce2b98da5d0840e0f29b532950.jpg', '16601-Aull-Susan_jpg.rf.950b61e2728e07edc551623561a26c94.jpg', '16908_Hurty-IIA-Wayne_jpg.rf.5c825df18fd94fd703889e2ab7bc7781.jpg', '16908_Hurty-IIA-Wayne_jpg.rf.df48e4ecff208d321732b8c458c6f21b.jpg', '16987_CallahanElvira_jpg.rf.bc6bbe5f438f60f061e6d5e32caa87f7.jpg', '17104-Wiarda-Nicholas_jpg.rf.594a44b0a43b27c2b8aa36011508ea91.jpg', '17104-Wiarda-Nicholas_jpg.rf.d7463c0e92f9b0e6604a7094c0048107.jpg', '1721-Samuel-David_jpg.rf.94d0c37e293c7f939d97cda5713971ad.jpg', '17318_PanzerDale_jpg.rf.bd78a3dad706b32699e735fc9688a874.jpg', '17851_WareJames_jpg.rf.1fe7d41b773c54a5863aebe017f6fa59.jpg', '17853-Zhou-Hanbing_jpg.rf.3848151c52b2fd29eddb5758bfbe09d4.jpg', '1801_CaffreyPatrick_jpg.rf.4232321f6343d648521a061537f2e043.jpg', '1801_CaffreyPatrick_jpg.rf.73882d93d4414188ff2a8b350de426ef.jpg', '19059-Flasterstein-Bernardo_jpg.rf.2dc5d5073e8375c9cf4f24d5e75f388f.jpg', '19078-Gendreau-Joseph_jpg.rf.ae21b5b4333f658fb833195d2adde139.jpg', '2114-Beekman-Ryan_jpg.rf.65ddd729a0770b55d8fa33a13d894124.jpg', '2257-Schiffman-Jeffrey_jpg.rf.ddef2fc842a885a25565488b10473f42.jpg', '2398-Wang-Paul_jpg.rf.19501808213a7f2f4c690ff2d416e232.jpg', '2676-Berman-Daniel_jpg.rf.5c75186f240a116927e85d0c80214a35.jpg', '2676-Berman-Daniel_jpg.rf.885adeb6ef7acdda88d3541ea675a7e5.jpg', '2689-Zager-Arnold_jpg.rf.5697a6e87f0d4a7a597e9458ae0a5e83.jpg', '2689-Zager-Arnold_jpg.rf.878587f0e1ace4b9bf5ae987848a44e1.jpg', '2957-Kaplan-Eric_jpg.rf.8e377aa7154c5dd4f6bd318bfc54e94c.jpg', '3078-Shakov-Emil_jpg.rf.49498e305fa7490c44018c60a93bf4be.jpg', '3078-Shakov-Emil_jpg.rf.ebd3839a2dbc7f357e52a27385b4f512.jpg', '3245-Glover-Jennifer_jpg.rf.a8d3d4e1c25c6992922d8174020d9062.jpg', '330-Suchy-Theodore_jpg.rf.a15eb10963a711eb209b3bc29e6dd1de.jpg', '3358_EganKristin_jpg.rf.1569da2eec352f30e3e312926ae85583.jpg', '3464-Rogozinski-Abraham_jpg.rf.1a9af5f70d90912c036f6dc2fc7cf35e.jpg', '3491-Kondi-Edward_jpg.rf.5ba8a31b4c9d8755f549b08a038672db.jpg', '3586-Kostick-Alexandra_jpg.rf.aeea63e9cc74e103ecaf15dfcb0f92d4.jpg', '3726-Maroof-David_jpg.rf.3b6e5271a0f8a64d9e6a12d2909ce58f.jpg', '3758-Leff-Randy_jpg.rf.c5dd93d585a0fd19f25a0444650f581f.jpg', '3816-Prasad-Anuj_jpg.rf.f005ea6898c845f88cb9c9e9f7b05beb.jpg', '385-Finkelstein-Mario_jpg.rf.9b8b31ad3f5986dd093711901ca2f88c.jpg', '385-Finkelstein-Mario_jpg.rf.cb883482da89c040fc059135a63398cc.jpg', '4169-Bellows-David_jpg.rf.5f0570bcd5f8c1c947f577aed63b50b7.jpg', '4169-Bellows-David_jpg.rf.f6263e88799dbafb72f4614eeecc07f4.jpg', '4183-Kleis-Jeffrey_jpg.rf.66ec780f54b49bf0fc0c2773bf211a10.jpg', '4183-Kleis-Jeffrey_jpg.rf.988d60c0af2b5c147abc2ba35bda2a3f.jpg', '4602-Ernst-Jordan_jpg.rf.a4697e6afdf5045bedfcd35d0d812b8c.jpg', '4602-Ernst-Jordan_jpg.rf.ff761700d2d1b8ef8a1d078d2e0712d2.jpg', '4617-Beylin-Mark_jpg.rf.34e9b8d4f4d99793efd8eee7da3e968b.jpg', '4636-Wallach-Robert_jpg.rf.3fece640a960caf803d74eac0cff655d.jpg', '4636-Wallach-Robert_jpg.rf.7214eac13208eed81ef0d65efa838367.jpg', '4636-Wallach-Robert_jpg.rf.8fbd8997acd447ef99efa0ea72f56435.jpg', '4730-Cantz-Paul_jpg.rf.508b46f3352aad0b3da64bfb4ad05a8b.jpg', '553-Tellioglu-Tahir_jpg.rf.6ecf93dcb7e37be7f0eb9c9a0a022bf9.jpg', '606-Ghaly-Tamer_jpg.rf.210e68b49b3d65604872a0c51212ee39.jpg', '6208-Segalman-Joel_jpg.rf.1cc85d10b0431f2d915be526936aba40.jpg', '62422-Groysman-Robert_jpg.rf.d84de46af48f710da2239afed1696444.jpg', '62575-Parks-Adam_jpg.rf.f61110a9d65facefcdb85de970976c26.jpg', '64341-Gepp-Karin_jpg.rf.0da17b02ce508cf8e220286c8d067b02.jpg', '64341-Gepp-Karin_jpg.rf.137c9a825d0557674ac9949ebf24ec60.jpg', '64341-Gepp-Karin_jpg.rf.f9c891f63db4e4496e62542124986cb7.jpg', '64347_TolchinNicholas_jpg.rf.5c6ba92669f44d0e54fd14e6f68178b5.jpg', '64399-Torstrick-Alex_jpg.rf.5260ede7e95c285091576d59bc3c8ceb.jpg', '64399-Torstrick-Alex_jpg.rf.e8e6f1673c715d8f4454f30b81bd6d87.jpg', '64399-Torstrick-Alex_jpg.rf.f7f3d843e6e4ee5a7dc177a687fb9201.jpg', '64772-Fields-Richard_jpg.rf.4ca5470827c4299a0babf2ac33d1fd76.jpg', '64772-Fields-Richard_jpg.rf.fe9e156bc3bbf2c9fd464339de991ea6.jpg', '64793-Waldrop-III-Robert_jpg.rf.4e40b3b1090584b7ae06405c59a02db4.jpg', '64793-Waldrop-III-Robert_jpg.rf.6f86172a916709f36596fb91cb57c701.jpg', '64796-Kim-Sunhee_jpg.rf.41e64169aad7057762f476b1b329d9cc.jpg', '64920-Omeogu-Chinyere_jpg.rf.a52a47a746ca4158759b056b10576927.jpg', '6528-Widman-Lawrence_jpg.rf.bc4cf30a8dd1513a0c4783816b68fc7e.jpg', '6528-Widman-Lawrence_jpg.rf.fc2aab6d6416c2d966a4f0047fb7ddcd.jpg', '6528-Widman-Lawrence_jpg.rf.feb1f4a709b54dbbf3e82d55ec31379f.jpg', '65760-Ehrmann-Paul_jpg.rf.29a5c292d95755862202e3ea467e4e3e.jpg', '65760-Ehrmann-Paul_jpg.rf.5a2a95972460fdf00aff084e2e115a8d.jpg', '65760-Ehrmann-Paul_jpg.rf.7ad6f514566e9191cf8f50bc896b56e7.jpg', '65839-Wnuk-Nathan_jpg.rf.521597a6ff1ffdb1525a813cad05d748.jpg', '65839-Wnuk-Nathan_jpg.rf.dc2e6d3f5bd00c674a1a4d403fc51fb2.jpg', '65919-Lee-Ivan_jpg.rf.be76151749199a731eb7b4fbc4c74225.jpg', '65980-Linares-Adriana_jpg.rf.1c421e886ad064bfcaa5b1d3488fad70.jpg', '6601-Schneider-Andrew_jpg.rf.73aabd87dce04c5a54b97edb5376d5bc.jpg', '6601-Schneider-Andrew_jpg.rf.8209497617c6f6c3574eae866a3a6bd4.jpg', '66159-Lehman-Andrew_jpg.rf.be8806f5f4046c8d920991751b89f636.jpg', '66159-Lehman-Andrew_jpg.rf.e4d003f37f17f90916367ca4a1fcd279.jpg', '66227-Botti-Torey_jpg.rf.f80c882d768095efce53c35907473017.jpg', '66277-Diab-Khalil_jpg.rf.196dce9d20fa2aac3974304f6c203816.jpg', '66304-Babakhani-Arneh_jpg.rf.2c4239696a8cdb3d456dd435b386c28a.jpg', '66917-Burton-Alex_jpg.rf.34538d302e0c283b918e4f0f2be5a08a.jpg', '67793-Glover-Hillel_jpg.rf.698bcb3c6f3bdfe33ba6852be0178327.jpg', '67793-Glover-Hillel_jpg.rf.c1c6a4c055c2add9ce36c3b78da74d99.jpg', '6904-Herstik-Ivan_jpg.rf.17f5906cd6c0060396623b349e1def69.jpg', '6904-Herstik-Ivan_jpg.rf.6b730c0d36cf456050036aeed1c2537c.jpg', '6904-Herstik-Ivan_jpg.rf.fda99d5864d622418808baabc3b59d99.jpg', '7120-Karidas-Steven_jpg.rf.243243245eaf8039bc657c50f3763b73.jpg', '7120-Karidas-Steven_jpg.rf.4249015fbb74f67538ae0b0cbb8df4d2.jpg', '7120-Karidas-Steven_jpg.rf.9c2dbb2e1977cc2ca5a51384ec386cf6.jpg', '734-Sahasrabudhe-Amit_jpg.rf.23ee3968f9e80e3835cd52758444a6a8.jpg', '734-Sahasrabudhe-Amit_jpg.rf.5a784606fc3ba2f49992e91cba35d261.jpg', '7551-Hobbs-Andre_jpg.rf.0ca55441ff8630752bcb0da7fa817df0.jpg', '7787-Weprin-Lawrence_jpg.rf.baf48355c36faec8109c5cdc86d5740a.jpg', '7966-Schwartz-Nathan_jpg.rf.3d10ace07abcf3be55bcce602566f765.jpg', '7966-Schwartz-Nathan_jpg.rf.c2c3d3d3eb64de4c91b2eb1a27fb0ea4.jpg', '8034-Friedman-Robert_jpg.rf.325dc8b2679986cfa72d1fc9f35ce4cd.jpg', '8363-Schubkegel-Andrew_jpg.rf.93a2920e299d5aaffa950047ae4a4a4f.jpg', '8398-Friedmann-Paul_jpg.rf.e311c910eaf7ac2a0bd5b5d224adf32a.jpg', '8410-Glassman-Stuart_jpg.rf.c4cbbea5024166a821506323ce7b9d66.jpg', '8453-Curcin-Aleksandar_jpg.rf.3184d9761c524d927e2ac50502721f7b.jpg', '8453-Curcin-Aleksandar_jpg.rf.9e76edcc44deecd207a3322504df4a5f.jpg', '8577-McCrary-Brian_jpg.rf.7b4edecde1e09d060de53bfbbf96fa82.jpg', '8588-Budoff-Jeffrey_jpg.rf.1e9420edd29bc987e5782fa0d699deea.jpg', '8935_AccettaDonald_jpg.rf.1f8f5580cde2fe35250fe69a8ca02530.jpg', '8935_AccettaDonald_jpg.rf.754ab7c430f435a686e3b49f5e8cd321.jpg', '9082-Richless-Lloyd_jpg.rf.f1037c1f90b72d9bf5141912a3b6ca24.jpg', '916-Saiz-Paul_jpg.rf.5e15d4d5ed2c6d51a6c54b01579ea6a7.jpg', '916-Saiz-Paul_jpg.rf.e0ab0e6269c3500bec568cd30b62361c.jpg', '9243-Ruht-Barry_jpg.rf.2781e0a6d30cfa4a47030307cda99f6b.jpg', '9243-Ruht-Barry_jpg.rf.2b1b15b0deaf01a0d38789a5de0c38f2.jpg', '9243-Ruht-Barry_jpg.rf.5c6f894fbf7c5d12c756e885d0aac6f5.jpg', '9245_ReiserJody_jpg.rf.d2fa2fc83555f5b252b77583b2dac0bf.jpg', '9245_ReiserJody_jpg.rf.e3b7ddf81249fb77c4d52bc31c7929f6.jpg', '9352_VenderMichael_jpg.rf.4fda685c59aaff20b04475716e35c97b.jpg', '9364-Bailie-David_jpg.rf.c0c159f9dc39188ebb814fbe9d59ca37.jpg', '9407-Thomas-W-Bryan_jpg.rf.301955a40b578a8ffc8b7e07d3265ef8.jpg', '9407-Thomas-W-Bryan_jpg.rf.3c4a7499317f5abfaa9c0fe9c6d0a9ab.jpg', '9407-Thomas-W-Bryan_jpg.rf.7e07d6ed5c580cfb32928f048d4eca2b.jpg', '9425-Fenichel-Gladys_jpg.rf.449df2901d797b5146cd6e37c5b418b6.jpg', '9719-Lanoff-Martin_jpg.rf.640537c8aa518a5065d9a16f57c3c3c5.jpg', '9719-Lanoff-Martin_jpg.rf.fb37359ab0452361e20d1b9737f0dc54.jpg', '9735-Kesselman-Paul_jpg.rf.043b59738c2756e55aedc0551e48ef22.jpg', '9735-Kesselman-Paul_jpg.rf.8942e5e831289b9ceafb0af5d2467c2e.jpg', '9740-Memmo-Pietro_jpg.rf.27fb8b10c0c5b0b195b9f80a477d4b47.jpg', '9740-Memmo-Pietro_jpg.rf.637df6522c1749f6bd5f24932b0ae812.jpg', '9839-Ghatan-H-Eliot_jpg.rf.ba14211262e781b73d3890d26706ba5d.jpg', 'aao54e00_2_jpg.rf.495b6bacc5d4409a36e8821e5942800a.jpg', 'abm69c00_jpg.rf.b9ef10cacb2ada2b99e419d009f8d2e5.jpg', 'aex05f00_1_jpg.rf.23d24d3043f7615a2112886c20084fae.jpg', 'agw39d00_jpg.rf.96be1c7f4f36df8d0cbec9b19888001d.jpg', 'aji32e00-page02_2_jpg.rf.40a59de5acd86899b33c3571aa737156.jpg', 'amw93e00_jpg.rf.578912bef2fa4abdf3c448dfc93c7e49.jpg', 'axp01f00_1_png_jpg.rf.2adbe4f38228b82137ddcf7044526cb7.jpg', 'axp01f00_1_png_jpg.rf.d05557f5c6d35a2aae09998211c52020.jpg', 'bad45f00_jpg.rf.abd50434b350514de9f2be18b60a73d7.jpg', 'bfk68c00-page03_3_jpg.rf.0255ec355093aea6ed7e3d30c9e7884d.jpg', 'bfx94e00_jpg.rf.5ce1ed013df439657d8bd21bd637c725.jpg', 'bhw64a00_jpg.rf.baf926492335afcfc31af861a3e5a4fb.jpg', 'bkz54f00_1_jpg.rf.a98eb2cf97c0a16f346070574824bfe1.jpg', 'boa85f00_jpg.rf.1a20cfed49aee059e600d2564e95e328.jpg', 'btt85f00-page2_3_jpg.rf.b3b490cb3c69bf1e499910213d201a7b.jpg', 'cel93f00_jpg.rf.063e4e1ed03fc3395fc60ec73b8910db.jpg', 'cgy54f00_1_jpg.rf.4f8de30552284c9b83229182a9c33345.jpg', 'chw80e00_1_jpg.rf.2cc0bcf9f7611fcfeaa96f9032f11f1b.jpg', 'cir10f00_jpg.rf.418c616fbd47e4bd85a953b2c479bbfd.jpg', 'cjy33f00-page02_2_jpg.rf.b540fee0fe351f735de1947c7364000a.jpg', 'cnk41e00-page02_1_jpg.rf.ee0e17356cf26a2e3b94af292e850689.jpg', 'cqt45f00-page02_1_jpg.rf.881b4be72f6a93f52e6ad4fbc1f01c76.jpg', 'cry54f00_1_jpg.rf.bb1213a801f874b1d7f85f7254015cd6.jpg', 'cxk72e00-page03_3_jpg.rf.a570bcfffa11bc7ee9f05d35bf6782c3.jpg', 'dhr55d00-page02_2_jpg.rf.31e4fefc3ddce81ec94512fce2f75268.jpg', 'dic45f00_1_jpg.rf.6a4c6827f330a069177e3f25c7e0e9af.jpg', 'djz54f00_jpg.rf.81715eea2dd8bb4117aeb0d4ad0bc2b4.jpg', 'dlu7aa00_1_jpg.rf.b2197fc08ddf31f902b7ce44741776e5.jpg', 'dmy31e00-page02_1_jpg.rf.14dc5e4aab4a6816ed84322f531cac43.jpg', 'dpi68d00_jpg.rf.f134806189102f7a36f6650ea2f7d259.jpg', 'dqn43c00_jpg.rf.483c05f816c020f38b4c4e46141f6e02.jpg', 'drm00d00_jpg.rf.f0ec568f3ecb12ed719a52a2b085e9a9.jpg', 'dsj50c00-page04_4_jpg.rf.c65624f65adae8111f099279b1ea5a97.jpg', 'eez35f00-page02_2_jpg.rf.8a18f8605f56151ef085664fb5538a43.jpg', 'erw85a00_png_jpg.rf.7d4ba27de6780920a8cfd9c669268adc.jpg', 'fny38c00-page05_5_jpg.rf.c4f3f57d193782aa0fd2cdff5172680e.jpg', 'Frame_100.jpg', 'Frame_102.jpg', 'Frame_104.jpg', 'Frame_106.jpg', 'Frame_110.jpg', 'Frame_112.jpg', 'Frame_114.jpg', 'Frame_116.jpg', 'Frame_118.jpg', 'Frame_120.jpg', 'Frame_122.jpg', 'Frame_124.jpg', 'Frame_126.jpg', 'Frame_128.jpg', 'Frame_130.jpg', 'Frame_136.jpg', 'Frame_138.jpg', 'Frame_140.jpg', 'Frame_142.jpg', 'Frame_144.jpg', 'Frame_146.jpg', 'Frame_148.jpg', 'Frame_150.jpg', 'Frame_152.jpg', 'Frame_156.jpg', 'Frame_158.jpg', 'Frame_162.jpg', 'Frame_164.jpg', 'Frame_166.jpg', 'Frame_168.jpg', 'Frame_170.jpg', 'Frame_172.jpg', 'Frame_174.jpg', 'Frame_176.jpg', 'Frame_178.jpg', 'Frame_180.jpg', 'Frame_182.jpg', 'Frame_188.jpg', 'Frame_190.jpg', 'Frame_192.jpg', 'Frame_194.jpg', 'Frame_196.jpg', 'Frame_240.jpg', 'Frame_242.jpg', 'Frame_248.jpg', 'Frame_250.jpg', 'Frame_252.jpg', 'Frame_254.jpg', 'Frame_256.jpg', 'Frame_258.jpg', 'Frame_260.jpg', 'Frame_262.jpg', 'Frame_264.jpg', 'Frame_266.jpg', 'Frame_270.jpg', 'Frame_272.jpg', 'Frame_274.jpg', 'Frame_278.jpg', 'Frame_280.jpg', 'Frame_282.jpg', 'Frame_284.jpg', 'Frame_286.jpg', 'Frame_288.jpg', 'Frame_290.jpg', 'Frame_292.jpg', 'Frame_294.jpg', 'Frame_296.jpg', 'Frame_304.jpg', 'Frame_306.jpg', 'Frame_308.jpg', 'Frame_310.jpg', 'Frame_314.jpg', 'Frame_316.jpg', 'Frame_318.jpg', 'Frame_324.jpg', 'Frame_330.jpg', 'Frame_332.jpg', 'Frame_334.jpg', 'Frame_336.jpg', 'Frame_340.jpg', 'Frame_344.jpg', 'Frame_350.jpg', 'Frame_352.jpg', 'Frame_354.jpg', 'Frame_356.jpg', 'Frame_360.jpg', 'Frame_362.jpg', 'Frame_364.jpg', 'Frame_366.jpg', 'Frame_368.jpg', 'Frame_370.jpg', 'Frame_372.jpg', 'Frame_374.jpg', 'Frame_378.jpg', 'Frame_380.jpg', 'Frame_382.jpg', 'Frame_384.jpg', 'Frame_386.jpg', 'Frame_388.jpg', 'Frame_390.jpg', 'Frame_392.jpg', 'Frame_394.jpg', 'Frame_396.jpg', 'Frame_398.jpg', 'Frame_402.jpg', 'Frame_404.jpg', 'Frame_406.jpg', 'Frame_408.jpg', 'Frame_410.jpg', 'Frame_412.jpg', 'Frame_416.jpg', 'Frame_418.jpg', 'Frame_420.jpg', 'Frame_424.jpg', 'Frame_426.jpg', 'Frame_428.jpg', 'Frame_430.jpg', 'Frame_432.jpg', 'Frame_436.jpg', 'Frame_438.jpg', 'Frame_46.jpg', 'Frame_50.jpg', 'Frame_52.jpg', 'Frame_54.jpg', 'Frame_56.jpg', 'Frame_60.jpg', 'Frame_62.jpg', 'Frame_64.jpg', 'Frame_66.jpg', 'Frame_70.jpg', 'Frame_72.jpg', 'Frame_74.jpg', 'Frame_76.jpg', 'Frame_78.jpg', 'Frame_80.jpg', 'Frame_82.jpg', 'Frame_84.jpg', 'Frame_86.jpg', 'Frame_88.jpg', 'Frame_90.jpg', 'Frame_92.jpg', 'Frame_96.jpg', 'Frame_98.jpg', 'fsh23c00-page04_4_jpg.rf.e6dc45050c89b96432f7876ff0c8443b.jpg', 'fwe69c00_jpg.rf.97873d77a7539fdb383d1b4f44240ebc.jpg', 'gcb93e00-page03_1_jpg.rf.7803aa1e74e69d05d5327783cb41959e.jpg', 'ghz25e00_jpg.rf.aae3c701f69993e17594bb1192dedc67.jpg', 'gsa_LAL60495-Lease-Amendment-Executed-LAL60495-Lease-Amendment-10-_Redacted-02_png_jpg.rf.20496a4bec8215533688458184d2613b.jpg', 'gsa_LAL60495-Lease-Amendment-Executed-LAL60495-Lease-Amendment-10-_Redacted-02_png_jpg.rf.336ab12e3c69139943eed6c2c81ebd4e.jpg', 'gsa_LAL61018-SLA-3-_Z-01_png_jpg.rf.2ea2ad85fdd010d2c9fb23f38378c944.jpg', 'gsa_LAL61983-Lease_Z-04_png_jpg.rf.46c21bde1dd003eb08f5f5529483f76d.jpg', 'gsa_LAL61983-Lease_Z-04_png_jpg.rf.5f5027d4dc3287051cfc10267e6c4101.jpg', 'gsa_LAR16906-SLA-1-_Z-03_png_jpg.rf.34f38985e4d483baa27e8634d6a99db9.jpg', 'gsa_LAR16906-SLA-1-_Z-03_png_jpg.rf.d6aafed5ebff5c1630ab83ff18c6e586.jpg', 'gsa_LAR16906-SLA-1-_Z-03_png_jpg.rf.dea7ccef0061192c33284d81df061110.jpg', 'gsa_LAR17002-Lease_Z-04_png_jpg.rf.365dde74b8845f229da6f0215b8cc24d.jpg', 'gsa_LAZ02191-Lease-1_Z-05_png_jpg.rf.c371292e837b43bb273d28240202d4f3.jpg', 'Handwritten-signature-samples-from-the-own-dataset_png.rf.837efbac0d67d09513a1bb752e3a1e86.jpg', 'hby31f00_1_jpg.rf.b97a4b06314322f0884466d10415984b.jpg', 'hdx55e00_jpg.rf.06bd8d2bb83725359e9230a965145d67.jpg', 'hpz95d00_jpg.rf.a6058716600d31fe35e24071c080a1b9.jpg', 'hst85f00_jpg.rf.71f24a00cc64fe545d9a580a85ce3fcb.jpg', 'hti31a00_1_jpg.rf.5724392e429bd126a69e3a3dad04d9a3.jpg', 'huz50e00_1_jpg.rf.fed6b8e74556f493aaca8dfde23e734f.jpg', 'icu98c00_jpg.rf.72ede66f5c9441f09af7e227335a7c51.jpg', 'idr55d00_jpg.rf.fb4c4b9a8a564d9b15fa5af0ed6cfd73.jpg', 'images_png.rf.8dcc0e0c421edcaa23e035a94c37308a.jpg', 'image_100_png_jpg.rf.83c7f72c61f0929e4f95b16d04a4e1ed.jpg', 'image_101_png_jpg.rf.4641e72d7bac7f7dc89cacc8c94e5008.jpg', 'image_101_png_jpg.rf.64a88f98680d11f915a9d46e23fcd72c.jpg', 'image_101_png_jpg.rf.d6dccc54e86d7a13817afd70663701fe.jpg', 'image_102_png_jpg.rf.97e16c6bbcaad8fe2f2373e05dd1a495.jpg', 'image_102_png_jpg.rf.a8ac792e84eda794b193e9b25eb33caa.jpg', 'image_103_png_jpg.rf.83f9eaeb25b9b8ad7115161240c542fa.jpg', 'image_103_png_jpg.rf.8b3f0a424361edfe905033b7614960a6.jpg', 'image_103_png_jpg.rf.bd9a4f4e30eaaf9c1d070c8250c40369.jpg', 'image_104_png_jpg.rf.7d64f52f9ac83dca98b12b1eea8828ef.jpg', 'image_104_png_jpg.rf.8f8ebf4eb512a90f008137cb056753a0.jpg', 'image_104_png_jpg.rf.cb6256656f49534037a618c5fbf896c8.jpg', 'image_106_png_jpg.rf.a0d46fdb19f2a0982681e82edaa6c7f1.jpg', 'image_107_png_jpg.rf.cc85cb3e2561d0328e027f4fcb289ccb.jpg', 'image_108_png_jpg.rf.8e44e7fa6fa7f90177bb72269855a5cb.jpg', 'image_108_png_jpg.rf.9874363a78eeeaf8313ed45f9ccb61b7.jpg', 'image_109_png_jpg.rf.50075cc4ade56d575236e40a718de97b.jpg', 'image_10_jpg.rf.9a8e7a64a31cada84b9a24acc215f32d.jpg', 'image_10_png_jpg.rf.122398348e4c66917a389fef1f726053.jpg', 'image_10_png_jpg.rf.4a6068a6cf0231aecb1edab4329e5812.jpg', 'image_110_png_jpg.rf.0e871a4ac3fb697bf0680b8b90ede038.jpg', 'image_110_png_jpg.rf.8a9d4c3044c3077c956d8635abfd1129.jpg', 'image_110_png_jpg.rf.d737c38c888fb82f2075206c3f9cf134.jpg', 'image_112_png_jpg.rf.d65b5255e7e95df98567877bf0dd3471.jpg', 'image_113_png_jpg.rf.12b9033f4cfd5a77e33f1a956d57b30e.jpg', 'image_113_png_jpg.rf.45c08993586246605a457f2c8f5798a2.jpg', 'image_113_png_jpg.rf.46bc174e7d15e6066843b7441d3a1e15.jpg', 'image_114_png_jpg.rf.45506e0b1ee5d77cf4efc49ef8117023.jpg', 'image_115_png_jpg.rf.377416619a722ae6b189f107d9d8c85b.jpg', 'image_116_png_jpg.rf.6e8eddbda449b450d951635a31438407.jpg', 'image_116_png_jpg.rf.a637d42247d58e2e35a4ec1da72648ca.jpg', 'image_117_png_jpg.rf.da8985235b563b008321616e950d4ce8.jpg', 'image_118_png_jpg.rf.faf07b8e35c5e3bc2b969928e826db34.jpg', 'image_119_png_jpg.rf.0ce53e89e0bf577539f91f71399b2be9.jpg', 'image_119_png_jpg.rf.4524ab21aba4cd5bec68427f6f4627ce.jpg', 'image_119_png_jpg.rf.847f0aaad7c1c1dc118c9c922a466a80.jpg', 'image_119_png_jpg.rf.c4b13f645f1f04f7b8902e5cba15bd8c.jpg', 'image_11_jpg.rf.fb034cffc7cbd67296f151dad93c39aa.jpg', 'image_11_png_jpg.rf.ca2f044152f79c12735f2c9d217528e5.jpg', 'image_120_png_jpg.rf.f7522b3a09af0eb75d9de065d8553869.jpg', 'image_121_png_jpg.rf.fcbaf45ce5d3956de172c795814f6d47.jpg', 'image_122_png_jpg.rf.12c391c3f233d97a56123ba868bed2e3.jpg', 'image_123_png_jpg.rf.3f900d556fc8f48ae30cf2b969b1ec5d.jpg', 'image_123_png_jpg.rf.833200d3b8a7a09988b53d41a650bb14.jpg', 'image_123_png_jpg.rf.a9938dc206d792802c117eadd0a45fcc.jpg', 'image_125_png_jpg.rf.13dac31d187207fd8eb12086509247a9.jpg', 'image_125_png_jpg.rf.4785b092e3d5685227031b0f9e851887.jpg', 'image_125_png_jpg.rf.855d4e1c6ce29a92dae0f0eb756d9277.jpg', 'image_126_png_jpg.rf.1ee4f98ee3245663f17e127db150c3ca.jpg', 'image_128_png_jpg.rf.0c99ad0a09d2fe77f7fbd5665ef77127.jpg', 'image_128_png_jpg.rf.14bcfde88ace503768feb985c4dc59bf.jpg', 'image_128_png_jpg.rf.414ec00f9005467c0a8623d20ea32451.jpg', 'image_129_png_jpg.rf.76ec2e0404795983d4d1b5fc10ac7a4c.jpg', 'image_129_png_jpg.rf.f7a629719c4e9bc17197fbf183c61cfe.jpg', 'image_12_jpg.rf.ddf09f23cc60bf018fa660fcc8bee3d9.jpg', 'image_12_png_jpg.rf.f426a25494d59d52141690d98a2c17e5.jpg', 'image_132_png_jpg.rf.98aacda1a7e81f27730abeff92383eac.jpg', 'image_132_png_jpg.rf.f1abb24921b243575f424d96e38ab83f.jpg', 'image_133_png_jpg.rf.26ed9e8653da438ead8532f8a9a64ea1.jpg', 'image_134_png_jpg.rf.22a1bc497d6fdce49f739126e3209ce9.jpg', 'image_134_png_jpg.rf.33c4518912d2b6bbb69176c78db7c7db.jpg', 'image_134_png_jpg.rf.9eed0af23c5f15f86484bf17850803fd.jpg', 'image_134_png_jpg.rf.bfe7f0e281423973f24fa10e4410f06d.jpg', 'image_135_png_jpg.rf.11c6674c9127cad4256d45f6328b2ba4.jpg', 'image_138_png_jpg.rf.349f0163b77d90755384dbac2816e6c1.jpg', 'image_138_png_jpg.rf.4c9da15031dfabb4b426d2266ec9bdd7.jpg', 'image_138_png_jpg.rf.5f8284e5c227d82d55cdcdb247c191d3.jpg', 'image_138_png_jpg.rf.c0bf906917aaf8c42fe5c71c83b691d2.jpg', 'image_139_png_jpg.rf.d55544924d710320781906d4d9f5f1c8.jpg', 'image_13_png_jpg.rf.4043c7715ea1ef23fbef478b17cb990b.jpg', 'image_13_png_jpg.rf.4fe1c0a1b4c710327b33d45f6764792d.jpg', 'image_13_png_jpg.rf.5e5a32b6db0fedc8f6463290b84b7650.jpg', 'image_140_png_jpg.rf.15e7d4ace689678adfe4f9a512679805.jpg', 'image_141_png_jpg.rf.38b271d804518c43216b6fe18d55c3e2.jpg', 'image_141_png_jpg.rf.7da9d11d1b91c0f0ce57bca7ce69daea.jpg', 'image_141_png_jpg.rf.82565d3607a1512ae7155c7ad230da94.jpg', 'image_142_png_jpg.rf.0a4b332e3fc810ba9b15a001ee04a346.jpg', 'image_142_png_jpg.rf.c33ba49e65bec766aa41b1c7d6284d51.jpg', 'image_143_png_jpg.rf.b2fe1066377a83c6b86882c5c445be19.jpg', 'image_144_png_jpg.rf.2bf1b7f09dd3619c7d2a12ba0773b168.jpg', 'image_144_png_jpg.rf.48a55586b5eaf63365fc001e657796e4.jpg', 'image_144_png_jpg.rf.64ef358c97ffffba61caf5e5c1873cae.jpg', 'image_144_png_jpg.rf.b9221ccd454547f6931c8dff3e0a8e38.jpg', 'image_145_png_jpg.rf.1b50650cca5261e9e3390ea6b9c00909.jpg', 'image_145_png_jpg.rf.30769b8558d319c4ba4b4072b0566861.jpg', 'image_145_png_jpg.rf.3c070a86b3d3dd165391739d05676032.jpg', 'image_145_png_jpg.rf.ae28990c813b2698c8a8e6ffc835dd5c.jpg', 'image_146_png_jpg.rf.057d666b61f75cae4ab3fb03d1996be0.jpg', 'image_146_png_jpg.rf.6b5d4d4fa5e32b7eec31761644954677.jpg', 'image_146_png_jpg.rf.baea3ccef2a05aa549cb98a072c85392.jpg', 'image_147_png_jpg.rf.9a0d6b6e1518447da9f9b22170d13753.jpg', 'image_148_png_jpg.rf.2f789226322d9a3848d1931adca29035.jpg', 'image_148_png_jpg.rf.3dfdde45c41dc390d8c4f9a63ba900c4.jpg', 'image_148_png_jpg.rf.50585f9737c850a5b1d8cd6d8ea9cfbc.jpg', 'image_148_png_jpg.rf.ee75f9ba2969fc389e5c01af88ba6b29.jpg', 'image_149_png_jpg.rf.030484427228297419b1f36d15f74957.jpg', 'image_149_png_jpg.rf.7d7cb6687c3ece8d4b5665199aaa7855.jpg', 'image_14_png_jpg.rf.4175c9eccc215fc769ca624c016c16aa.jpg', 'image_14_png_jpg.rf.b20988e756845de79ece34d5989cac55.jpg', 'image_14_png_jpg.rf.bfcc722b9acdfc604e173c6ec53d40df.jpg', 'image_150_png_jpg.rf.b50f9d8c755fc8e2299a2c11fa41a4ef.jpg', 'image_150_png_jpg.rf.b80c7b4d4b224039bb1059f23f097599.jpg', 'image_152_png_jpg.rf.70c992e6b363cc47a3fd803d9bcf5be4.jpg', 'image_153_png_jpg.rf.89bd8540f437089998d35b6533f9179d.jpg', 'image_153_png_jpg.rf.98d3ae8adaf796d26188ef60d17ab517.jpg', 'image_153_png_jpg.rf.9c5744f98449d18108149549906b89ef.jpg', 'image_154_png_jpg.rf.686544601e0a632a385ae7d9725da7f9.jpg', 'image_154_png_jpg.rf.b55bad42826d0c27fc1b942f21b69b8c.jpg', 'image_155_png_jpg.rf.fcb2ae4bf8363aa17dda844ac6bd7f02.jpg', 'image_156_png_jpg.rf.e1053155e8c2c0cd147553cbdc69922b.jpg', 'image_157_png_jpg.rf.a0edc32838d852e7646e313319524ced.jpg', 'image_157_png_jpg.rf.aec3338f8732515b3c8536ee8003ff21.jpg', 'image_158_png_jpg.rf.c07c4ca05768a796c286a616ea9e1e05.jpg', 'image_158_png_jpg.rf.c92a237aabde2b70ef5c5c03b570d5d4.jpg', 'image_158_png_jpg.rf.d0b8a24ed8d4f2b94de07f13e27e2a38.jpg', 'image_158_png_jpg.rf.e014ec5ec1ed07774fb348936b58519a.jpg', 'image_15_png_jpg.rf.206d5e80c8c8f7d7892c442b760c6bd2.jpg', 'image_15_png_jpg.rf.438f9a1b9c88e1353e7abbd51062408e.jpg', 'image_160_png_jpg.rf.017010ef6887362049ca111dd6b3bba1.jpg', 'image_160_png_jpg.rf.61dda3378aadf7fe765a8c76743c66f1.jpg', 'image_161_png_jpg.rf.1541ec2fae3b0b2ff2d94d7fa8aa3cfe.jpg', 'image_162_png_jpg.rf.4680a82a9226b69424095c13d2faecf7.jpg', 'image_162_png_jpg.rf.92f400fa6aed4de545bbbae7239856bb.jpg', 'image_163_png_jpg.rf.3c499f5dfacd78449a2e6d9b7c14d9c3.jpg', 'image_163_png_jpg.rf.3d7bbfa4d84074564d530c52fe18e15b.jpg', 'image_164_png_jpg.rf.db53eddab14149876c8a15ad84fc47b0.jpg', 'image_164_png_jpg.rf.dc3d27d55f5614ba3a1d7f33581f371a.jpg', 'image_164_png_jpg.rf.e03739926bc97e1e47e26c2de71edd9f.jpg', 'image_165_png_jpg.rf.2cc52b7e3c23791f4d778a65de552552.jpg', 'image_165_png_jpg.rf.91cebbe3f77134d41e0979f057a932a5.jpg', 'image_166_png_jpg.rf.0a79eccd42a1ab6eb345a67f544a9b87.jpg', 'image_166_png_jpg.rf.61f3dafe48a2eebcacf39c427bcefb53.jpg', 'image_167_png_jpg.rf.3447716aa462fd5c4fbed39a8fd79345.jpg', 'image_167_png_jpg.rf.47357ed7bf174fb7164edfe68a6fb8d1.jpg', 'image_167_png_jpg.rf.ef33fd160115f26a48b1660518519e5c.jpg', 'image_168_png_jpg.rf.24bd76c51a14902a3fcc0a36a26591f3.jpg', 'image_168_png_jpg.rf.c0b0b76a8039adcce9ee6e3a22546404.jpg', 'image_168_png_jpg.rf.c20d40569ed2cdd237ba7495a2fc7d22.jpg', 'image_169_png_jpg.rf.2f3d6e6ca08495f12950bee6ce702b77.jpg', 'image_169_png_jpg.rf.f050f1747da5ccc29395e0c00da3b868.jpg', 'image_16_png_jpg.rf.595385757c273d984042d68e35fbb291.jpg', 'image_16_png_jpg.rf.8446a8e1b5b9b0d64989e097360e44a0.jpg', 'image_170_png_jpg.rf.36f69d6bc4afec42895fb5004ee2ffce.jpg', 'image_170_png_jpg.rf.8da29d4e928e5a7359517e9ec133621f.jpg', 'image_170_png_jpg.rf.d808679dc1f76f4240fab241f224648e.jpg', 'image_171_png_jpg.rf.0da3c65888e0b4a29fcd3cafe9a40925.jpg', 'image_171_png_jpg.rf.32c791a345c1131dd7d81bb24a0c3aad.jpg', 'image_171_png_jpg.rf.fcf9296ba9fdbdd36b7cebb195adb952.jpg', 'image_172_png_jpg.rf.e08ed4094bcb7e766a3f3291ab98174f.jpg', 'image_173_png_jpg.rf.3f35027a19f51921159b4cc95744db91.jpg', 'image_173_png_jpg.rf.51df6c6a2946aafdc961d6799bf71861.jpg', 'image_173_png_jpg.rf.682068c0c7f4abb76ffdb0af24ccbf68.jpg', 'image_174_png_jpg.rf.59467f9059768c36804f5e29f63a239b.jpg', 'image_176_png_jpg.rf.1474a285cf6f1daf394dac27d431062d.jpg', 'image_176_png_jpg.rf.8baf62d1fb705f6e38a9513685efbe8e.jpg', 'image_178_png_jpg.rf.34d846af74c1646e16f9a74cad0ee002.jpg', 'image_178_png_jpg.rf.712611ff58807140c091375bd527b570.jpg', 'image_178_png_jpg.rf.c0f31d3253154757cda9499885a7bead.jpg', 'image_178_png_jpg.rf.cf3e6242fa028917aec103c4767580df.jpg', 'image_179_png_jpg.rf.a8640c1496abb65131fad0a15ea08522.jpg', 'image_17_jpg.rf.6fd0187d37d0b6439e1cb6e081b08d41.jpg', 'image_17_png_jpg.rf.85c9cff03e7e3a31b6c8247f6ebb5811.jpg', 'image_17_png_jpg.rf.f3b2ec1b4f00ad4fdc2bd5ebe93456e8.jpg', 'image_180_png_jpg.rf.4e6bc12fd92002b2a9bb97330da4ce1b.jpg', 'image_180_png_jpg.rf.abb203229723f6cf7e1e4f72124dd362.jpg', 'image_180_png_jpg.rf.b7f9ceda3082ce3781e18c6b13f24715.jpg', 'image_181_png_jpg.rf.3cea87703cd71a6577db7a4f148e1a0c.jpg', 'image_181_png_jpg.rf.4f574fc2ff7ed53725068dce811a25af.jpg', 'image_181_png_jpg.rf.ee7b470cb25e1f76fd0f4790dfd5107f.jpg', 'image_181_png_jpg.rf.f9a5987c2259656a1c80cbac1b66f840.jpg', 'image_182_png_jpg.rf.40db16b75b778341ee46c7e0a605a40f.jpg', 'image_182_png_jpg.rf.6e727798b7da67c52c0a354da34e54b9.jpg', 'image_182_png_jpg.rf.91a80ced4bfe538d3d594da934e8876a.jpg', 'image_182_png_jpg.rf.9c09d11d05bcff7e91318ed4df8b315a.jpg', 'image_183_png_jpg.rf.614422d454a01207515619dd959b61f9.jpg', 'image_183_png_jpg.rf.ba20b977212860f1d6272218669f667b.jpg', 'image_184_png_jpg.rf.4fe15505f58479a73a873ed4a71de99c.jpg', 'image_184_png_jpg.rf.86f055c2589c3b37916e1b7ed75debb8.jpg', 'image_185_png_jpg.rf.aa7519ce1a140b26f66630b5c51b4e91.jpg', 'image_185_png_jpg.rf.abdafbb7bbe7610c544af3dfe03977db.jpg', 'image_185_png_jpg.rf.fd83dee0fe9ab119182a883f00df3ac9.jpg', 'image_186_png_jpg.rf.40d13aca152d2ac1708ab49b7cf85790.jpg', 'image_186_png_jpg.rf.964f15a69eba83fc597d127e5d28fe50.jpg', 'image_186_png_jpg.rf.adb209b64890046b8c79b34561330d9a.jpg', 'image_186_png_jpg.rf.ea0081d6dc99ba377190e5ad295cc881.jpg', 'image_187_png_jpg.rf.8c628b48e414a5f44704d272782c6a73.jpg', 'image_187_png_jpg.rf.9fabc21e325cd5a8e7e6baed59af25f3.jpg', 'image_187_png_jpg.rf.aee1ed0122094c506545925c44f18250.jpg', 'image_188_png_jpg.rf.a772534bc793db905d42b7adb1f1fd6c.jpg', 'image_18_jpg.rf.09cc4c5ffcfd12e452ced703354ad68c.jpg', 'image_18_png_jpg.rf.c7cac5fa77791dae2c367589c42d42b4.jpg', 'image_191_png_jpg.rf.b09aa5cd6a09c35bb02ea5b0dbc41001.jpg', 'image_192_png_jpg.rf.44445d55db2a09f9f3b69d85b28c30c5.jpg', 'image_193_png_jpg.rf.2d26dbf02072fc8b0338762cec18ae02.jpg', 'image_193_png_jpg.rf.6f355dc048a7e474ed19e5ec41158ced.jpg', 'image_194_png_jpg.rf.dffaa0c2a6e68cd2319bb5fd93a07d43.jpg', 'image_194_png_jpg.rf.e9a1e41320947e33fe364352490111cf.jpg', 'image_195_png_jpg.rf.458ba506b34b8c519805fb316434653c.jpg', 'image_196_png_jpg.rf.9a14b2ffc766d01fdd056169f05f2cd5.jpg', 'image_197_png_jpg.rf.3a495091e83a2251246a89ff46a97f02.jpg', 'image_197_png_jpg.rf.3ea8d2f0d0490090ff80bfdb49bbe16b.jpg', 'image_197_png_jpg.rf.4a11f2943a9c6b75954b690953877801.jpg', 'image_1_png_jpg.rf.44b0f5aaa444f48aef0eefc1d48a53d8.jpg', 'image_1_png_jpg.rf.835190744ee27fcd6fc1edb7e90e102e.jpg', 'image_20_png_jpg.rf.7b492c0059141e68a0a7f67ae9d7b0d6.jpg', 'image_21_jpg.rf.56925655f1ca781ea36ed54015fc6862.jpg', 'image_21_png_jpg.rf.52db7f301612c2b4f3a1a34355ef77f8.jpg', 'image_21_png_jpg.rf.d484736170224ecb497a551e53860f5e.jpg', 'image_22_jpg.rf.7b93ee2aec5d9e01ebb23fdce1ade528.jpg', 'image_22_png_jpg.rf.89cca66d5ccd44d8fc838e5391937604.jpg', 'image_22_png_jpg.rf.8ac70487a74a3aefb08ddf5d0e051366.jpg', 'image_23_jpg.rf.39b931c2ad1076588ae38f5726a5e127.jpg', 'image_23_png_jpg.rf.3029083918c3648ab4e2ab1a73068176.jpg', 'image_24_jpg.rf.58e143d8b4cce708410ea17fe7a2d304.jpg', 'image_24_png_jpg.rf.885646228ad9cd5cbd79ce4a710def2c.jpg', 'image_24_png_jpg.rf.c72ca27973dc772bcf50e21572cf9704.jpg', 'image_25_jpg.rf.523e16fabdcb6835135718bd9244d066.jpg', 'image_25_png_jpg.rf.7b63e7daf03fc1138fe4cc98a06f8b1d.jpg', 'image_25_png_jpg.rf.920363b2a31fb9695bd464facb4cf087.jpg', 'image_25_png_jpg.rf.f2dca12b10484ec65e5152a75a5b4e6a.jpg', 'image_26_jpg.rf.d0a6f53d281dd30e5df919a30bb27090.jpg', 'image_26_png_jpg.rf.98e8467c42950a5a72c72563b48e3384.jpg', 'image_27_jpg.rf.ef6054e2118a7a78f70f0db7389d7515.jpg', 'image_27_png_jpg.rf.162830aab131332ade650e3db2cb080d.jpg', 'image_29_jpg.rf.6ae5813e58548d9f53534052aea76489.jpg', 'image_29_png_jpg.rf.3cfa243d02344a5a9fa40cebb39224ba.jpg', 'image_29_png_jpg.rf.e710db8c12ce2addfd89e20a62cc3221.jpg', 'image_29_png_jpg.rf.ea30fddb3910fbbe56b59f94c7c7ff04.jpg', 'image_30_png_jpg.rf.73294668e14a78e49ebdf66128655149.jpg', 'image_30_png_jpg.rf.ac80cd5c37bdfb105813117ea31717f1.jpg', 'image_30_png_jpg.rf.bdf8bc7b93ba4cc109970c566d2b82cc.jpg', 'image_30_png_jpg.rf.c4bf5243c6c6d06cddb3f8f537b05aea.jpg', 'image_32_jpg.rf.0f04b5d5a9938b14f54d1d715b74a845.jpg', 'image_32_png_jpg.rf.36e8fc99588c72db0bdc55925fa166db.jpg', 'image_32_png_jpg.rf.edcdfd2efe1e99b545a29acb931fc26b.jpg', 'image_33_png_jpg.rf.1d55d80293fab3c81edb1a5392c854bf.jpg', 'image_33_png_jpg.rf.7716746f679f5ad33defcb92c3a9de89.jpg', 'image_33_png_jpg.rf.89994681065cf97bbf48fa7dee981263.jpg', 'image_34_png_jpg.rf.fd4669f724a76b49722c69ca024f579a.jpg', 'image_35_jpg.rf.27391c9de5249698f1b168cfcf6ec23f.jpg', 'image_35_png_jpg.rf.115d3f11b54120bcce37d72d8456cfb0.jpg', 'image_35_png_jpg.rf.7b2a24831fbfeb1faaef9fdd1f0c1ec8.jpg', 'image_35_png_jpg.rf.7f704370bbc63591bae59f4c0a49dc90.jpg', 'image_35_png_jpg.rf.aae46428ba8e59f9612b9e97f9adcd60.jpg', 'image_36_png_jpg.rf.515351c1f0e26c84e65de0c70f0d6023.jpg', 'image_37_png_jpg.rf.440580173d41f03af905c4690f106fd5.jpg', 'image_37_png_jpg.rf.583b6bc6bedec56602fb20ba6820cfdd.jpg', 'image_37_png_jpg.rf.8bd68873a8cb3fc3170a63bb76eabd56.jpg', 'image_37_png_jpg.rf.9fa7b00e14bc29d9915201e585abaff8.jpg', 'image_38_jpg.rf.d3654a1f3e2931644718dfd5d3ded434.jpg', 'image_38_png_jpg.rf.3454907f0abfc2a0374d9f44a3dc0f73.jpg', 'image_38_png_jpg.rf.f9a33937afef1fb5834e93b5c51072f3.jpg', 'image_39_jpg.rf.02136e1c5376a997ac7392ef4df26271.jpg', 'image_39_png_jpg.rf.740a25f8d9731d56b1065de2e5b188e9.jpg', 'image_39_png_jpg.rf.f55b59e34d100a72e9a504556715d5a6.jpg', 'image_3_png_jpg.rf.37755630cf736f478b72a28d0fc198ab.jpg', 'image_3_png_jpg.rf.379ed12075caded2fa1ab16a1e0567ab.jpg', 'image_3_png_jpg.rf.c28f31407d9317143a13179168fee430.jpg', 'image_3_png_jpg.rf.d253184f36d07651cba1490d2dda2214.jpg', 'image_41_png_jpg.rf.4fca2adcafbf8b7964f48ccdb46c8bbf.jpg', 'image_42_png_jpg.rf.1733b83fde9a84f22a45755729773449.jpg', 'image_42_png_jpg.rf.34f8cedc3e11c506e41ba9722233e7ff.jpg', 'image_42_png_jpg.rf.aee8bb418879709d29d55a9a6fb6e769.jpg', 'image_43_png_jpg.rf.63d8072f129f2bf56933490e91a38453.jpg', 'image_44_jpg.rf.aa6a9f0dd7daaa9c9c4b25a8a46281e0.jpg', 'image_45_jpg.rf.6c0c1f778fec6904cc63b8e30f3c431c.jpg', 'image_45_png_jpg.rf.16a1315fde5c1031b8b22adffdb0c4ee.jpg', 'image_45_png_jpg.rf.d555870e0c1e9e011aa12d9c34a40b22.jpg', 'image_46_jpg.rf.b5f24e58f1d02f3d58d55094dc31af2f.jpg', 'image_46_png_jpg.rf.73ad2075b692f3e6703f22fdfde04cc8.jpg', 'image_46_png_jpg.rf.87eacb4625da122884fc1f183b1f26e2.jpg', 'image_47_png_jpg.rf.cc633651972297b9ef4bfc224eb889bd.jpg', 'image_48_png_jpg.rf.3bbd67ee914f6f081c4a97435e06c8c3.jpg', 'image_48_png_jpg.rf.9674bbc741ee96b090521be9b990d7be.jpg', 'image_49_png_jpg.rf.843cde83432d608401cfd515c4f42232.jpg', 'image_4_jpg.rf.41bd445eb505dacf93583e24468c299a.jpg', 'image_4_png_jpg.rf.7208274fd653b1123cc8e83e3281a07d.jpg', 'image_4_png_jpg.rf.99d026ac734b4986ec26b3a80c32a78b.jpg', 'image_50_png_jpg.rf.8b8163a18edaab1960a11b7b3d937f58.jpg', 'image_51_png_jpg.rf.6514b6a75554cedee665ad455c15918f.jpg', 'image_51_png_jpg.rf.b22ad6f3f4c59cf46651b015344d9185.jpg', 'image_53_png_jpg.rf.061e23951980f4594582673f6f26fd13.jpg', 'image_54_png_jpg.rf.47cc229ac9ae05f3e010fadfc4a7d04c.jpg', 'image_54_png_jpg.rf.d8f99fe9929a04e0264e5168ffc85b61.jpg', 'image_55_png_jpg.rf.a1fb4971061f2ed24c00f432ccaa0f97.jpg', 'image_55_png_jpg.rf.ec9608c043b5c6a00ee4de8e6b00fd4c.jpg', 'image_56_png_jpg.rf.5df54b919c9605c409d5d617c12313f1.jpg', 'image_56_png_jpg.rf.6a595b58146fa37a96953ff2ce945d07.jpg', 'image_57_png_jpg.rf.61102a65b2fbbc48e21346289f8fc841.jpg', 'image_57_png_jpg.rf.93b33bfa1fc5fe17096f50d342c6072e.jpg', 'image_58_png_jpg.rf.85558b5ee1664ddc292a33ec5684279f.jpg', 'image_59_png_jpg.rf.197bf587e1ad3bd7da378e35511b4671.jpg', 'image_5_jpg.rf.8f55d2f176382318a70e1e08be9f44cf.jpg', 'image_5_png_jpg.rf.dc1d98cf10db44183d73353d4fce0a42.jpg', 'image_60_png_jpg.rf.5bb5ca9557b96d8400b057e82eeeeacb.jpg', 'image_61_png_jpg.rf.61d35b90376a57b1ae175ad19dc829c1.jpg', 'image_61_png_jpg.rf.e9d897d99a526e7cee3e655064688717.jpg', 'image_62_png_jpg.rf.564b8a661c39d857d9bb3384a8fb8aec.jpg', 'image_62_png_jpg.rf.87e0dab31ecc2333ddbab2708694d034.jpg', 'image_62_png_jpg.rf.fbd72b71ac0c221f138dead2fd080c8f.jpg', 'image_62_png_jpg.rf.fc34a50016ca9a173db13ac0c4fa9a2a.jpg', 'image_63_png_jpg.rf.0bfee22468af093b6d2106bb0ec50f42.jpg', 'image_63_png_jpg.rf.e4906da735d9800a033c7f3b74b57187.jpg', 'image_65_png_jpg.rf.3d57e43bf78fb540cc35cf0ead93ec0f.jpg', 'image_65_png_jpg.rf.e75a1d616effb95980eda7b2ffd3e2e0.jpg', 'image_66_png_jpg.rf.c4e87a269e65c0fdac593eac94da6cea.jpg', 'image_66_png_jpg.rf.d5e8da1d18646a59472dbc51c9e2b636.jpg', 'image_67_png_jpg.rf.4411a466666a6ec8c15deb825f5daef6.jpg', 'image_68_png_jpg.rf.a1dca11f4315a13f4b63d5b11c4fdb45.jpg', 'image_69_png_jpg.rf.c7012bf98c7a66464f69341ba7d22495.jpg', 'image_6_jpg.rf.2c29abe5795495465d80a8ebec36aac8.jpg', 'image_70_png_jpg.rf.4ca2072b41cbd41275ced8d853af3c4c.jpg', 'image_70_png_jpg.rf.cb48a784428e19ccbbc7e071683756f1.jpg', 'image_71_png_jpg.rf.508a52dca6ce63a3a2cb66ea5eb91e34.jpg', 'image_71_png_jpg.rf.fb458a57512e6cf32e775edadda0f601.jpg', 'image_72_png_jpg.rf.8436a31bb4fb75ca60c158f65c6887fe.jpg', 'image_72_png_jpg.rf.98c85665d448abf0a37cdb02aedb8907.jpg', 'image_73_png_jpg.rf.4a7c23c8bc96fdfde4d3418539857316.jpg', 'image_73_png_jpg.rf.9430fdd67062418256ca92ccc4eb1853.jpg', 'image_73_png_jpg.rf.94a09d7e6c9b1bef04e557bfb23ec14e.jpg', 'image_73_png_jpg.rf.f769ac762d3f330878323f98b4dc2973.jpg', 'image_74_png_jpg.rf.812822f6f519d96affee5a0b570ec6e8.jpg', 'image_74_png_jpg.rf.b813cae02656b632e30a1d21dbc5d2c5.jpg', 'image_74_png_jpg.rf.e3b4f70308daa181a2f558e8081acc3d.jpg', 'image_76_png_jpg.rf.033e503141f1aae07623320b0e2451e0.jpg', 'image_76_png_jpg.rf.3947502e99d01f70a314381fd028d6aa.jpg', 'image_76_png_jpg.rf.b61f756776d0ebd01cd54b999e88d476.jpg', 'image_77_png_jpg.rf.49472515fc54b952c83c497284d20841.jpg', 'image_77_png_jpg.rf.a775913e44666b842f040f9313ef6d06.jpg', 'image_78_png_jpg.rf.1ee0ae673b160cf94cf6b3b4f7ef66f6.jpg', 'image_79_png_jpg.rf.7e7d8416a76833cbe66224d717d4c4d2.jpg', 'image_7_jpg.rf.aac3e39112ea857f3d3f80286fd78bbd.jpg', 'image_80_png_jpg.rf.2b98980be36ef9676d851a8d8a2cd7b7.jpg', 'image_80_png_jpg.rf.6da9d978d3069db4106e17c7036ffdb6.jpg', 'image_80_png_jpg.rf.fb5df94e5a9fc9bc7c2f73fee09ea8b2.jpg', 'image_81_png_jpg.rf.109dda63918dc4f7841d73743cf8cca3.jpg', 'image_82_png_jpg.rf.be12f1173cb3eb7dc8799d6504535763.jpg', 'image_83_png_jpg.rf.26fdf0d6b468091a60f9a4cb135e9c00.jpg', 'image_83_png_jpg.rf.4cd55ab815733a57563b243f4b1f4b54.jpg', 'image_84_png_jpg.rf.322f2e4bfb555f6d67aa1091a90e7eb0.jpg', 'image_85_png_jpg.rf.f05073d3ad276d0e8cd4db3f618a8f35.jpg', 'image_86_png_jpg.rf.20db03dae0150d1bf90b1735b4dcab9a.jpg', 'image_86_png_jpg.rf.7f85e8764b19b3887334d2690ecb8f13.jpg', 'image_86_png_jpg.rf.88c3fbdba62d81370ca1f0ce470e156f.jpg', 'image_86_png_jpg.rf.e026ebda03cfaa63802ec86c79848657.jpg', 'image_88_png_jpg.rf.04062a120b1ca6c15165b280398216cb.jpg', 'image_88_png_jpg.rf.1f9b0a9507a1c83854b23b8ccf497a18.jpg', 'image_88_png_jpg.rf.7ccedf9889a229355330e844696c4196.jpg', 'image_8_jpg.rf.c3db31b1c1a3607b20fdf595c66089c5.jpg', 'image_8_png_jpg.rf.13a96e35d4bf27fbb305e4254722ab99.jpg', 'image_90_png_jpg.rf.fdd996eebf438696cc0332fb2581c7b6.jpg', 'image_91_png_jpg.rf.303da0f3570b8bed5e4aa69ac609ce36.jpg', 'image_91_png_jpg.rf.c59ca552de95df5683d233481b9578f4.jpg', 'image_91_png_jpg.rf.e302dac0d7762bbe40fe7db32fcc1691.jpg', 'image_92_png_jpg.rf.feecfe4d857ae4feff620f7ba9f62616.jpg', 'image_93_png_jpg.rf.21c5eb2f3a40d81b0fba30dd6ec18b91.jpg', 'image_93_png_jpg.rf.369d79274729e416d98a3dfa6ca4893d.jpg', 'image_93_png_jpg.rf.70bf0f357c47d53235830ea4c6cfa69e.jpg', 'image_95_png_jpg.rf.7cd09206364d6917cf9c788255de68be.jpg', 'image_97_png_jpg.rf.8fd52b5d72ca66b96ca9981ae79d44c5.jpg', 'image_98_png_jpg.rf.85f6ca1dc4e42c344a922ee0c79662d3.jpg', 'image_98_png_jpg.rf.ae62fee2b86f329c76e86bec703e082a.jpg', 'image_99_png_jpg.rf.529e5bd188187fe65ef49eaea996f3d6.jpg', 'image_99_png_jpg.rf.82f5d34aa957344be0b81960bdf005e9.jpg', 'image_99_png_jpg.rf.c46cc5bf38f5d8812fd5d7f12146d3e7.jpg', 'image_99_png_jpg.rf.edbed5c5707e13c4fd027c86fa821ba3.jpg', 'image_9_jpg.rf.a8e4b9ffef87e2c2e6ac1ef200157762.jpg', 'image_9_png_jpg.rf.32ed364d82034c720eaf92d4c5210cf0.jpg', 'image_9_png_jpg.rf.798cdbdb1fd45686c602b426a3623c31.jpg', 'image_9_png_jpg.rf.af438c132e5c90fc40841ab04f288aca.jpg', 'image_9_png_jpg.rf.c68a8c5f04022748c4ca1e548ce0a294.jpg', 'img101_jpg.rf.0b961e2a0762e6c93feeb663bcfa097a.jpg', 'img104_jpg.rf.ab881bb432b76f2fa2ef304a5172743c.jpg', 'img110_jpg.rf.4bff95b0de5a1135d97390e24d262962.jpg', 'img116_jpg.rf.b331aca657f495a479eecf2da2716ef1.jpg', 'img119_jpg.rf.d058c094528b2ca17002f092da3b7fa6.jpg', 'img11_jpg.rf.131ac0311e9b735522e0fe00671d3f5f.jpg', 'img122_jpg.rf.df86afc948ba48faf10a93048d48e302.jpg', 'img125_jpg.rf.3fe950e67d1718330f1ecf2053ffc523.jpg', 'img128_jpg.rf.b2e94bc9adef235c2d38b9738cae33e6.jpg', 'img131_jpg.rf.b8bce0799cf580e0c0732b5f2ed30103.jpg', 'img134_jpg.rf.2a631d0b7e9f5ce228ff41fdb6fee04d.jpg', 'img137_jpg.rf.4ae2988619a71e96788d94bac8be619e.jpg', 'img143_jpg.rf.bd3d4826420807dfe0aed0866ef0a2c0.jpg', 'img146_jpg.rf.4622730a95e4a7dbf04a13e5026c6661.jpg', 'img149_jpg.rf.9add050395ec7fccfc8057e84b135e4f.jpg', 'img14_jpg.rf.70430499b9dbb9779c959983921d2fa2.jpg', 'img152_jpg.rf.795029c93f5730f2c626061353e21eb1.jpg', 'img155_jpg.rf.14b70b24244d1351d14fd734dfd5671f.jpg', 'img158_jpg.rf.9c1f670328c09fab34281ac6c7bb3223.jpg', 'img161_jpg.rf.350ea51de3bffd28f6af6a35863b6350.jpg', 'img17_jpg.rf.fc811fac48c99f868ce1fced76f1fd7b.jpg', 'img20_jpg.rf.08643005414b0e5773ee4c445d829679.jpg', 'img23_jpg.rf.2b5593613961b3fee45183bdac97b492.jpg', 'img26_jpg.rf.7669340909f399081855f4542e3845d2.jpg', 'img29_jpg.rf.c15a714b45567cb90fb7df0825ec33e5.jpg', 'img32_jpg.rf.8fbdbf29ffd155cd392ed7f584f403de.jpg', 'img35_jpg.rf.d84f1bd287e1cdeefd78eda6a5daa15d.jpg', 'img38_jpg.rf.07a0ee096d92b549ec27a323dbaf6419.jpg', 'img41_jpg.rf.350217fd6c876a143ade2e41c6a81e00.jpg', 'img44_jpg.rf.c823a9512aa62e194ef746f8fd4ab800.jpg', 'img53_jpg.rf.4a925c13ffbec90cb26c0b5881e6d641.jpg', 'img56_jpg.rf.2a95a686b59d08a58445360d5c2d020a.jpg', 'img59_jpg.rf.86a8d5f7e8e4c1e96684c86bb92e9215.jpg', 'img5_jpg.rf.e6927c312cd36b9ec198f99684a8525a.jpg', 'img62_jpg.rf.9e5c9a527a7abf4bf9bf86d08d2b9c4d.jpg', 'img68_jpg.rf.2d3fbbd66a1d703b9090c30a5f4cf1c7.jpg', 'img71_jpg.rf.4c898d511af881f52ce10ac815ae80a1.jpg', 'img74_jpg.rf.4c26ea97d35eb849205b20a733f8a66d.jpg', 'img80_jpg.rf.7207203a6a5128e7d12220af14697d10.jpg', 'img86_jpg.rf.0b0ccef033f6ae845142cb2ecb9425d6.jpg', 'img89_jpg.rf.30728f002e303502ca98fb93b4b10993.jpg', 'img8_jpg.rf.e13c5b86376138b3026c84dcfaafbdd2.jpg', 'img95_jpg.rf.886ad01244d8ee9265317e769bf4c0a6.jpg', 'img98_jpg.rf.baec46131ea362aa1633dc12c9b2152d.jpg', 'ivw54f00_1_jpg.rf.2dee3e36dfbe63967c969686f7dec401.jpg', 'jiy01a00-page02_2_jpg.rf.957a6357d9be621af3bc4ab0bab7ae24.jpg', 'jrk44a00_jpg.rf.1c3bd71be158dcc48e269d463c88556b.jpg', 'juo75f00_1_jpg.rf.a1b9f76d3b1a89b64f7b6de820efea89.jpg', 'kci90c00_jpg.rf.c9f69f42e819fedb37c1c33dded78036.jpg', 'kcn64a00_jpg.rf.010b402e67f65433aab41a64bdbfb4e8.jpg', 'keo90c00-page02_2_jpg.rf.2b7a700a8833add83fe79931b0793765.jpg', 'kfw39d00_jpg.rf.aaa79d9333cfea22202f7e8079211379.jpg', 'khz25e00_jpg.rf.9d56c33cdd7436e76f6cd6447fca17e0.jpg', 'kjw13f00_jpg.rf.b912437083ea751bdf49ad16fe1a9562.jpg', 'lwd23f00-page02_1_jpg.rf.f407b36f4e42098957812254284418ff.jpg', 'mbw13f00-page2_2_jpg.rf.f9c01e5ecbd7371b7b32303ce73b9179.jpg', 'mev75d00_2_jpg.rf.f4ea21977c1f130db3a232ef043476e5.jpg', 'MicrosoftTeams-image-100-_png.rf.76e9114ab0d7ee82d9d073c55ac04d78.jpg', 'MicrosoftTeams-image-11-_png.rf.f776c7edd26b55360c135d4cdf9cd212.jpg', 'MicrosoftTeams-image-14-_png.rf.f8432d5188a952589ab04ec2b1a58025.jpg', 'MicrosoftTeams-image-16-_png.rf.aaa7a9f3e45d239c84ea1157af8fb444.jpg', 'MicrosoftTeams-image-17-_png.rf.c1991624a3df3e6afd65ec36cf03a212.jpg', 'MicrosoftTeams-image-18-_png.rf.f33d69df63276cdaf661b4f15395db19.jpg', 'MicrosoftTeams-image-19-_png.rf.ead14316c07aba1b110506c57d839bdd.jpg', 'MicrosoftTeams-image-20-_png.rf.c5b58ca1b531e481b5c25fe513e2513e.jpg', 'MicrosoftTeams-image-21-_png.rf.1a41698bfd6d83d2a3277e32ae928ec9.jpg', 'MicrosoftTeams-image-22-_png.rf.f17dcec9fd4d04122ad35ca79d1222d5.jpg', 'MicrosoftTeams-image-24-_png.rf.4eda7e169fdb1ea37fe51665702f65c2.jpg', 'MicrosoftTeams-image-25-_png.rf.681ac009d1df25a7304d6a1fe7ddb75a.jpg', 'MicrosoftTeams-image-26-_png.rf.48ebfa6a7dfbaf81178b992c5195bc90.jpg', 'MicrosoftTeams-image-28-_png.rf.2ec60da26912de1851cb0667ed304a55.jpg', 'MicrosoftTeams-image-3-_png.rf.75c560e36e3173aa3fbbe107bcb160bd.jpg', 'MicrosoftTeams-image-30-_png.rf.3c72c5e7fcdd5dd1f5734b149a84bc6d.jpg', 'MicrosoftTeams-image-31-_png.rf.2bbeba9965ebf4e1d2fbc8619deb6881.jpg', 'MicrosoftTeams-image-35-_png.rf.ae32c69fe1faf20a95dea5680f13879f.jpg', 'MicrosoftTeams-image-36-_png.rf.831743ac43df03bd21365dda9ded0c94.jpg', 'MicrosoftTeams-image-37-_png.rf.fd47f34858ea8ecc080c94b4f6933ed7.jpg', 'MicrosoftTeams-image-39-_png.rf.ec69407c3dab88f5f5b413258a41e745.jpg', 'MicrosoftTeams-image-4-_png.rf.ba041b5562d2595c7ab04265b72fd641.jpg', 'MicrosoftTeams-image-41-_png.rf.b025af9a67112a0926934c302b4170c6.jpg', 'MicrosoftTeams-image-42-_png.rf.1be52d6d2e77734e8b1d2feb48612f8d.jpg', 'MicrosoftTeams-image-43-_png.rf.7015c764cc80eb2db0240f9ed0ca3311.jpg', 'MicrosoftTeams-image-45-_png.rf.e7ded85192f8ac6390d3175c8cae7c7d.jpg', 'MicrosoftTeams-image-47-_png.rf.bdf36fbd50b73d3b7c17d9344e96afa7.jpg', 'MicrosoftTeams-image-48-_png.rf.b869dd9d737676f0d7dd77efb88ebb24.jpg', 'MicrosoftTeams-image-49-_png.rf.11712f833799f4057f10f4b33c167f90.jpg', 'MicrosoftTeams-image-53-_png.rf.3fe6cad890f2948db3c69fd92d619050.jpg', 'MicrosoftTeams-image-55-_png.rf.78ffa4f25fbc16a8a36928f55ae6be6d.jpg', 'MicrosoftTeams-image-56-_png.rf.d1d9baeff82c9c680de75348fc763808.jpg', 'MicrosoftTeams-image-60-_png.rf.ea761471c2b4137199b547decec34241.jpg', 'MicrosoftTeams-image-61-_png.rf.896b06fa3b11f6e65971df9afc6c30ab.jpg', 'MicrosoftTeams-image-63-_png.rf.0896ba38b8bf97c00bb1ee2b2ca14600.jpg', 'MicrosoftTeams-image-64-_png.rf.53e458daee6fa3661ac5f0d7d0946340.jpg', 'MicrosoftTeams-image-65-_png.rf.0f01fde9787d8ced4d60523ba9b2f39f.jpg', 'MicrosoftTeams-image-66-_png.rf.b94e51f80842fec73297c2c92d3a36d4.jpg', 'MicrosoftTeams-image-67-_png.rf.3735f7241a8d1ced1ac133b9842c9525.jpg', 'MicrosoftTeams-image-68-_png.rf.70f75945443b4cf9b5f4840fb59a785e.jpg', 'MicrosoftTeams-image-69-_png.rf.16f416e6e5335a7853d40dcebb88cf5e.jpg', 'MicrosoftTeams-image-71-_png.rf.7ed9709ae443356e99f11f7c7c8ec42d.jpg', 'MicrosoftTeams-image-72-_png.rf.5c6011aa56c69bbb94675ee2ff3cd40c.jpg', 'MicrosoftTeams-image-74-_png.rf.db2e9efaaea0f096df7760300872e0b0.jpg', 'MicrosoftTeams-image-75-_png.rf.0230775b32b2fe1eef73b477e6d40db8.jpg', 'MicrosoftTeams-image-78-_png.rf.164cdbdbce69397f2b404dfb50607d82.jpg', 'MicrosoftTeams-image-80-_png.rf.7698124732c63684101a138be2933316.jpg', 'MicrosoftTeams-image-83-_png.rf.c27e6d44446fc994c80c14f3e746de63.jpg', 'MicrosoftTeams-image-85-_png.rf.f810586e432843d1588d756da0dde1a7.jpg', 'MicrosoftTeams-image-86-_png.rf.40bd7a6c6f14b867f51b2d92df92109c.jpg', 'MicrosoftTeams-image-87-_png.rf.f08f9d049cf2eb78281fc339031b45d4.jpg', 'MicrosoftTeams-image-89-_png.rf.923904bc7740edb58cc60d34ceae61f5.jpg', 'MicrosoftTeams-image-9-_png.rf.845fce1e5fc91a59377cb8af8c9e3682.jpg', 'MicrosoftTeams-image-92-_png.rf.4a83e073534d50738ec1f2e42ab3e41e.jpg', 'MicrosoftTeams-image-96-_png.rf.5270c5346367210826e56022e4f37932.jpg', 'MicrosoftTeams-image-97-_png.rf.cf7dc63f56db917748f17eaf18bc3125.jpg', 'MicrosoftTeams-image-98-_png.rf.735e86c30019535500b8633570f8c22f.jpg', 'MicrosoftTeams-image_png.rf.7d98262cd4509fbfe814f29592d2c7ef.jpg', 'mta60c00_jpg.rf.42aaf328d45c04338b435d758d7230a3.jpg', 'ncn00d00_jpg.rf.19f53e3270c5e5f84b347c3e9b10ef29.jpg', 'nht43d00-page02_2_jpg.rf.ec5be8cec83794cac48e34accca2aed9.jpg', 'nlu01f00_jpg.rf.4a0dbd295cd13f795ed8e4b140063e02.jpg', 'nrg54f00-page02_1_jpg.rf.36beae6aa6274c3d59d067ac2b7523ee.jpg', 'nul00a00_jpg.rf.e898285c58f811f91ac9fcc21260c46b.jpg', 'osp65f00_jpg.rf.2c0c96bd5a0a1abe624ec85904567f16.jpg', 'pmx82f00-page04_4_jpg.rf.59b4b86c39496a124daec2156bcb239f.jpg', 'pvx38c00-page06_6_jpg.rf.98074b52dcd3fad50fb4a14358bc1d7f.jpg', 'pzb89c00_jpg.rf.9d0927607cb912a829df6c9404a8eddb.jpg', 'qaw85f00_jpg.rf.d3361039b7a5fe8bf8531a2aabbd3be3.jpg', 'qbh54c00-page02_2_jpg.rf.a32ee2a00e3d8621f11969825bac7a7a.jpg', 'qfj59c00_jpg.rf.8359c6b7e699fdc702db7259028ba771.jpg', 'qqu85f00_jpg.rf.735d3733f18afd888af2a5d3abcf40da.jpg', 'rightsignature_PNG_jpg.rf.b35473e13bd87ba0fea3baba22094754.jpg', 'rnf51a00_jpg.rf.e246d867724ba9f5a436ef12c47fdc4a.jpg', 'sfw98c00_jpg.rf.54e91ff2d264151f9f7cd1dec9a61738.jpg', 'sik79d00_jpg.rf.96db54ed2efc64c79053845068e56ed5.jpg', 'syi15f00_jpg.rf.bc2add07592c3b7adb82fd7cb49997dd.jpg', 'tmp42sp89xr_PNG.rf.1422d7decdc8340c772aab73aa649150.jpg', 'tmp4dt7via8_PNG.rf.e38fd1c41bc053ccdabe18ef4da7f473.jpg', 'tmp5ld9hcen_PNG.rf.f4699676bdfc9f8dde0a2457f6ee25e0.jpg', 'tmp672g06_0_PNG.rf.024c1ef5b6ebaafde9c8d57e80afe353.jpg', 'tmp672g06_0_PNG_jpg.rf.5e06e6293ec55a00ab70e83cb13f8c74.jpg', 'tmp84rg4ovj_PNG.rf.ed058c9d2205721e24515f83f366580e.jpg', 'tmp84rg4ovj_PNG_jpg.rf.907c8346c8034370dfca02b4e4668e87.jpg', 'tmp84rg4ovj_PNG_jpg.rf.e0b42810c9690a70b2a77f73341aa2c5.jpg', 'tmp8fiu21s5_PNG_jpg.rf.9352235b3a6b350be709e4d367ede625.jpg', 'tmp9w5txc87_PNG.rf.07076dbc127d227ea3afcd2bb6d326cc.jpg', 'tmp9w5txc87_PNG_jpg.rf.0525ebab4d58c4e73a04f0ea63b48a7c.jpg', 'tmpae22d77y_PNG_jpg.rf.8a91a8615daa7d54d4de47540938ece5.jpg', 'tmpayj47wej_PNG.rf.9609c0dc1738aa2095caa86e96b9a279.jpg', 'tmpayj47wej_PNG_jpg.rf.bc24143aacfe4f9a4c6e098c654637e0.jpg', 'tmpb2o_kx0k_PNG_jpg.rf.0b5576b9cf9615f56a12edfde9a22a6e.jpg', 'tmpbgclc0lt_PNG_jpg.rf.33683beebc21646d1c791ed4562a1fd3.jpg', 'tmpbxw_aiuj_PNG.rf.451177c6b3568f0ce2dedd7e89328943.jpg', 'tmpcxsj0650_PNG.rf.4cd00c4682ad31b9560b8071b440554c.jpg', 'tmpcxsj0650_PNG_jpg.rf.405833cf7376cf25f3880394854671dd.jpg', 'tmpc_30kndd_PNG_jpg.rf.8da5d8683f36b0aeaaeb3cc84b5e524b.jpg', 'tmpdjup300d_PNG_jpg.rf.0f5bf2ec5ef2fc28f0cecce8bd3079ba.jpg', 'tmpfrau3rjx_PNG_jpg.rf.6d05d0c27fe6825a02471c09138a6309.jpg', 'tmpfrm4y9ou_PNG.rf.8ed7a07271cb753116b89635311bcd4c.jpg', 'tmpfrm4y9ou_PNG_jpg.rf.dd5647cdbf5644dfe94bf9988da4b187.jpg', 'tmpfwm056ip_PNG_jpg.rf.d81e03a4f16e3135fa9c9fd220e8c657.jpg', 'tmpf_lqjq5c_PNG.rf.4de51457d02fac20f40a07a7e899d677.jpg', 'tmpf_lqjq5c_PNG_jpg.rf.c9db6bdc8946bd9d50857d39741836e2.jpg', 'tmphcfsphcx_PNG.rf.158e91fe93bce44a0284b6a834fbf642.jpg', 'tmphcfsphcx_PNG_jpg.rf.6a3401179eab77fd64af58f193b21734.jpg', 'tmphe72j8ud_PNG_jpg.rf.4c5e0f8ff0ad04f01b3dc46b6e487fc7.jpg', 'tmphe72j8ud_PNG_jpg.rf.74ed3ce44f6e5ed9a010ebaab0bdab53.jpg', 'tmphg0h65u3_PNG_jpg.rf.edffce1c0ab5a4638e81acc23ee845cd.jpg', 'tmphuk5w827_PNG.rf.4edb8c5191d2ab00b62f163e06485434.jpg', 'tmphuk5w827_PNG_jpg.rf.149eca7ab59c53be5e101e80e230b82a.jpg', 'tmpijqxfe3b_PNG.rf.66d35a47b2a54aa2364831818d2ac2be.jpg', 'tmpkdnaq4if_PNG.rf.5439caad66dedf70e91b09b981bb1d9f.jpg', 'tmpkdnaq4if_PNG_jpg.rf.ada7f64064d10bdfaa4a2db4b4a87e33.jpg', 'tmpkuyw2wn9_PNG.rf.c92a0fcd3d0270ae5fb89586d6313c9f.jpg', 'tmpkuyw2wn9_PNG_jpg.rf.ef841dbee2e1af7d147a10806585f46d.jpg', 'tmploqnfnci_PNG.rf.bd3bc40a3035e6082f752fbc08c6b164.jpg', 'tmploqnfnci_PNG_jpg.rf.4f1d745753f8e80f32d9bed340b7b421.jpg', 'tmplxmpwd1s_PNG.rf.7d15d62ef3b187ed74b86b9f518fd490.jpg', 'tmplxmpwd1s_PNG_jpg.rf.ecc837c9951f4329f47522796cc570a2.jpg', 'tmply5gzzu7_PNG.rf.fc64a0bbec9a727c6e4f2631f161c4ef.jpg', 'tmply5gzzu7_PNG_jpg.rf.e3b65b9945a2978c623f403696b4c00b.jpg', 'tmpmav8t8oi_PNG.rf.0bc052709fc87dd04c45446704438b4e.jpg', 'tmpmav8t8oi_PNG_jpg.rf.08c33f8de0242dbe06d2dad0e7647357.jpg', 'tmpmjqa105z_PNG.rf.73e22496ce83a757e0a09071b6953b35.jpg', 'tmpmjqa105z_PNG_jpg.rf.f4e54d11347d1f9876e7f5ac25464004.jpg', 'tmpnlwewvw0_PNG.rf.9407125c60629d00b04da2e951eaf113.jpg', 'tmpnlwewvw0_PNG_jpg.rf.c9ac5f3780c711c4760188e603557ac3.jpg', 'tmpnze4nhxp_PNG_jpg.rf.1ca897e6e35d6eec7e0d5afbc366aba5.jpg', 'tmpouk7pxxi_PNG_jpg.rf.e0994ba2058c15accd4a81f91091f002.jpg', 'tmpq3zk_91i_PNG.rf.9922fc63d045dfbc6e4183ce4b99aec2.jpg', 'tmpq3zk_91i_PNG_jpg.rf.84c6f35b28ca11122c5f7960850a2218.jpg', 'tmpq6nvmgf5_PNG.rf.636403459ae7ebdf4f6c5a352ea7e295.jpg', 'tmpqyxo3z0q_PNG_jpg.rf.5fb09aafb6f2d4f2130cac783d6760b8.jpg', 'tmpr5w0ulcj_PNG_jpg.rf.54401520f9a4f8edec65c1ba5ea203c8.jpg', 'tmpto_dra7l_PNG.rf.c5f869730dcbb4da4da3a2f9fa61b614.jpg', 'tmptv1s04p9_PNG.rf.9daaf01a37bf30d6ca4e3283f90b5c6b.jpg', 'tmptv1s04p9_PNG_jpg.rf.4be83f3a1e68ce7fc7144c7c285a164b.jpg', 'tmpu3hrb09i_PNG.rf.eba7e63fd494dd6d6167a502a2fe988e.jpg', 'tmpu3hrb09i_PNG_jpg.rf.8777dd76e297d15e4bb9c9292cd098f3.jpg', 'tmpu4s50oq1_PNG.rf.13f86fada31465752df5d84a04b1241e.jpg', 'tmpujnn3xao_PNG.rf.26f11b5ac8cd7e40630dbdd199d6c8b1.jpg', 'tmpujnn3xao_PNG_jpg.rf.98a49c0c29f714d49caa65f933e5c8c2.jpg', 'tmpvob6dej2_PNG.rf.5e6e9b0e83d627efabb12cc4cc81867d.jpg', 'tmpw8es3rrv_PNG_jpg.rf.5312ffc746ad6e3550d6158a0f101376.jpg', 'tmpwwsdmjd7_PNG.rf.f703dcf073f3711800a208322a16a30b.jpg', 'tmpxfhs0i03_PNG.rf.ccfbe7647f52aa274993f00b6abe181c.jpg', 'tmpxfhs0i03_PNG_jpg.rf.57ebea25070513ca571f616b77e30b55.jpg', 'tmp_gb50tic_PNG.rf.a8da4a3e7478b08ab3e74388fe77ee0c.jpg', 'tmp_gb50tic_PNG_jpg.rf.e75c1e04d2465f82c030fe4ffe41440c.jpg', 'tmp_y2flr6p_PNG.rf.77ecebac6f5c3fabde8e44b0439bdaf2.jpg', 'tmp_y2flr6p_PNG_jpg.rf.b670fa8eba3c1f4c48174c831649a383.jpg', 'vad45f00-page03_1_jpg.rf.79ff4cc3a9f83df284cd9882545ad70b.jpg', 'vda05a00_jpg.rf.8267de8db04900919e3a1503f9a9f2cd.jpg', 'wky60e00_jpg.rf.1fa7bd4c94a22c0f575642249a836f68.jpg', 'wry97e00_jpg.rf.6db86b88ca9e7a62778c329b803ec52e.jpg', 'wzt35f00_jpg.rf.37caa08e43e1b487fcdb413f76123fbe.jpg', 'xik90c00_1_jpg.rf.624ed4dcf8bcb000cc10784433b132cc.jpg', 'xjx9aa00-first_jpg.rf.8ae5f881257605a7a1fb5ca42b3d8b16.jpg', 'yda69d00-page02_1_jpg.rf.4716efb487af3102392143af8b80b09d.jpg', 'yoi68d00_jpg.rf.08f12e0f97ae5fbce943cfcbcc4895f9.jpg', 'ytz94a00_jpg.rf.009831ad367715aa9b71df35846f92bd.jpg', 'zlu43d00_jpg.rf.aad5eb3039d21b031eee583386e78c14.jpg', 'zmw13f00_jpg.rf.37cd58e707f655ad0d71a779a08fe43b.jpg', 'zqc25f00_1_jpg.rf.88ebfea5f816e1e06871299398bc526a.jpg']\n",
      "Valid images: ['1058-Khoury-Farjallah_jpg.rf.6e6e4f80c4be2ad9c1f7b9295698b9b2.jpg', '10634-Critchfield-Edan_jpg.rf.dbd588be3d7ff7d61ca4e90d5108cae5.jpg', '12286_AtluriPrasant_jpg.rf.86be68a94b964dad45d71bb57a47cdbb.jpg', '12413-Owens-Dwight_jpg.rf.007c9db0ce845419298584a299b951d0.jpg', '12413-Owens-Dwight_jpg.rf.e025f0591710f7b7726a7a4962ace5de.jpg', '13386-Portnoy-Kevin_jpg.rf.77d4649f221b411297098f2103b16f19.jpg', '14123-Elendu-Uchechukwu_jpg.rf.1fc09b88112cf01ed0aaeada77362868.jpg', '14227-Moise-Rudolph_jpg.rf.e517ca031a2d3dafbba293cd6dd21f7c.jpg', '14251-Swamy-Priya_jpg.rf.25ed7fc9790a87698c8c9f74c88fcd86.jpg', '15083-Ducharme-Stanley_jpg.rf.1972ac4d519de898301d08198d5b584c.jpg', '15385-Walker-III-John_jpg.rf.9da2a6b96d13284809a81e3eb456889d.jpg', '15744-Gluck-Gabriel_jpg.rf.6602698aead7dda61e1a78df4601f7dc.jpg', '15872-Hole-Robert_jpg.rf.02bb35b4c164951c4906c4b787167a57.jpg', '15872-Hole-Robert_jpg.rf.598688054facb2a66f3ec27697ad85df.jpg', '15915_GuptaRakesh_jpg.rf.5864837616058e07ae42d62350cef4a7.jpg', '16315_SagermanScott_jpg.rf.4d7c42e223382e82fd7faf3a2e0f7fee.jpg', '17318_PanzerDale_jpg.rf.9822f2bb7a5131a2eaf6505c391733f3.jpg', '330-Suchy-Theodore_jpg.rf.252d5d12f6737c5e38885206718b69d3.jpg', '3545-Cousin-Daniel_jpg.rf.19a2219dab1cf59fcc0625f62a9a31da.jpg', '6631-Welch-Bruce_jpg.rf.250992bd2b9c285669ca15e8e78ca058.jpg', '6631-Welch-Bruce_jpg.rf.75d870474062cb35dcbaf60f26396b98.jpg', '67498-Agbaje-Ismailu_jpg.rf.72c6a5b415592bdb152a19d59585daa0.jpg', '7691-Rubin-Jerry_jpg.rf.0ed6990339bd63bbfb2668aa90852600.jpg', '8899-Kwon-Brian_jpg.rf.bf1fc64c403c5bd943b92094e411987d.jpg', '8927-Sellman-Michael_jpg.rf.555ee0ec767d39ea1460e86474adbfe0.jpg', '8927-Sellman-Michael_jpg.rf.68f071154d586194c66da2b40fcb15f1.jpg', 'aik94f00-page02_2_jpg.rf.2576f2c5f0e8efe7e2b3555f8e1e6509.jpg', 'bji44a00_jpg.rf.d6a753beeeb91c98045240f643f3ff4a.jpg', 'dvr41a00_jpg.rf.89a68c4432a6196f18dfc93a4cd516f6.jpg', 'fbv15e00_jpg.rf.003637fca6125c22447de9c3a4ed2c5c.jpg', 'fgx54f00_1_jpg.rf.e2c82ec4e7b465eca80e84b7406675f4.jpg', 'fhz25e00_jpg.rf.d8dd210a0637fd95f7b8a1252e58a72e.jpg', 'fpi68d00_jpg.rf.8ae0a6c77a012ea295bf877d1620da40.jpg', 'Frame_108.jpg', 'Frame_132.jpg', 'Frame_134.jpg', 'Frame_154.jpg', 'Frame_160.jpg', 'Frame_184.jpg', 'Frame_186.jpg', 'Frame_244.jpg', 'Frame_246.jpg', 'Frame_268.jpg', 'Frame_276.jpg', 'Frame_298.jpg', 'Frame_300.jpg', 'Frame_302.jpg', 'Frame_312.jpg', 'Frame_320.jpg', 'Frame_322.jpg', 'Frame_326.jpg', 'Frame_328.jpg', 'Frame_338.jpg', 'Frame_342.jpg', 'Frame_346.jpg', 'Frame_348.jpg', 'Frame_358.jpg', 'Frame_376.jpg', 'Frame_400.jpg', 'Frame_414.jpg', 'Frame_422.jpg', 'Frame_434.jpg', 'Frame_440.jpg', 'Frame_442.jpg', 'Frame_48.jpg', 'Frame_58.jpg', 'Frame_68.jpg', 'Frame_94.jpg', 'gcb93e00-page03_3_jpg.rf.f60bd0d64fc8e48853b85d6c8ec5665a.jpg', 'gsa_LAZ02191-Lease-1_Z-05_png_jpg.rf.e1280fe4dd96c20e4dc36c7f24a9b643.jpg', 'image_102_png_jpg.rf.5a9e52a84ebaee289f53345dbffd6db9.jpg', 'image_105_png_jpg.rf.cbba70add941e7d4e2bc6db7f545b271.jpg', 'image_108_png_jpg.rf.becfbac5e35d4457decf267f66cb7fbc.jpg', 'image_110_png_jpg.rf.7ed2776eb2cf47ce55f9cf438fa70bdd.jpg', 'image_113_png_jpg.rf.ad5c3437bb732c8d55298ae6873281f9.jpg', 'image_118_png_jpg.rf.56d3852bf9e09d6f8589dd898e07599c.jpg', 'image_122_png_jpg.rf.22eed508341209635188312498fc3302.jpg', 'image_123_png_jpg.rf.c92efad1588657119078eb77a86d2f22.jpg', 'image_124_png_jpg.rf.07a3241ae3e5fb3de011c4aecbf1f345.jpg', 'image_127_png_jpg.rf.dd47c3da3012ad75b208660f6e567481.jpg', 'image_128_png_jpg.rf.b6a3ceaf8e1e7662f6ebed2e8e1075f7.jpg', 'image_130_png_jpg.rf.42395d25dc0aec5e57b8b5f0cb8465a3.jpg', 'image_131_png_jpg.rf.7c1a35b6a34872c0f88c3bf0c5449256.jpg', 'image_131_png_jpg.rf.8a18de681f85e3c17a50b6027e9108fe.jpg', 'image_131_png_jpg.rf.f18aae6e7fc177aa6b9d2c4f26bad009.jpg', 'image_133_png_jpg.rf.5a25716d70c92777c76be5bf69b17c1b.jpg', 'image_136_png_jpg.rf.34c6a142da26e49e73e1c24da82753ee.jpg', 'image_137_png_jpg.rf.f5ee73efbd2ed2e6e2d0d533b6b707d6.jpg', 'image_13_jpg.rf.820865d2d1275953c67c156886409ec8.jpg', 'image_142_png_jpg.rf.8b6c166d4c6a7f80ea068d63cd66f8bc.jpg', 'image_147_png_jpg.rf.8c2150558f6e585379cbbc3e6dac4e5d.jpg', 'image_147_png_jpg.rf.f2df3f8b9655322f8208692c9984b26a.jpg', 'image_14_jpg.rf.a710d26424488ed4878d2c08f791443b.jpg', 'image_14_png_jpg.rf.2a04ab883949315635c9f2c579f549b6.jpg', 'image_159_png_jpg.rf.7ef5a0a304ca980d266d5ac46049f609.jpg', 'image_165_png_jpg.rf.e62500effef9bc6ab85509a83a842109.jpg', 'image_168_png_jpg.rf.fe0653d9810f9aa576a2847b3078b045.jpg', 'image_16_jpg.rf.38d44079e613634939ab020ec8b2b43d.jpg', 'image_171_png_jpg.rf.c80033f55fa638c37c640d8042cc085d.jpg', 'image_175_png_jpg.rf.a643eadc962b651008a82ecdce0dbc0c.jpg', 'image_185_png_jpg.rf.18c3290e39d89fbda72f5f2cdb521bfe.jpg', 'image_187_png_jpg.rf.48ead5f995080975284c4619197386fa.jpg', 'image_189_png_jpg.rf.aea5615a90121bbb8728bcb4a34e35b4.jpg', 'image_18_png_jpg.rf.ae010d7a0754ef7080700353d6b200f7.jpg', 'image_18_png_jpg.rf.f5bdc8e52d0d737b45f42aa58fbea57f.jpg', 'image_192_png_jpg.rf.c411032d9ccbeef2f4e641fc321bc903.jpg', 'image_194_png_jpg.rf.f3eb7890bcc4274fb9bac0e928e31246.jpg', 'image_196_png_jpg.rf.eb4ec5b012155fc25899da963b8790ca.jpg', 'image_197_png_jpg.rf.2716b121a4510676fc4e5f3d732a576a.jpg', 'image_19_jpg.rf.51df1683f66b837b7670527c44cea43e.jpg', 'image_1_png_jpg.rf.ae4851c5763a02e1c8cf845074f6a5e2.jpg', 'image_20_jpg.rf.4c0469761586cd54b4ec98f38455d962.jpg', 'image_20_png_jpg.rf.6c8ebd410d3b388dbb95e233d9c1821a.jpg', 'image_23_png_jpg.rf.be0392580876e6e066e68c6e1e143930.jpg', 'image_28_jpg.rf.bb266879baddaa2abdecb4790d5c8786.jpg', 'image_28_png_jpg.rf.b19ea6c7dac90957d0886507085aabe4.jpg', 'image_29_png_jpg.rf.50ec3123f39b54631e2f4245e173800d.jpg', 'image_2_png_jpg.rf.a4e126ef28ad1f386d82df7d5eecf2ab.jpg', 'image_31_jpg.rf.9ce335932eb7415e8c1bc3d3dc9b7095.jpg', 'image_33_jpg.rf.9b5b9948b4286b41fbff2a7e1b5050f5.jpg', 'image_34_jpg.rf.96f36046b47eaab0dcf0759a389417fd.jpg', 'image_37_jpg.rf.b9ff73af1b4f925a24de0d0be8ede473.jpg', 'image_40_jpg.rf.f0b2a90f0326f30b518792b4510d9c16.jpg', 'image_40_png_jpg.rf.7229f306b84206dece9907dd43af32cb.jpg', 'image_41_jpg.rf.bfa7829599fa2066fbf77818786cfcad.jpg', 'image_41_png_jpg.rf.c4950048245c49b2d0130d13ca7ffee7.jpg', 'image_44_png_jpg.rf.7c22167d193470eb1ca7ae312367f138.jpg', 'image_45_png_jpg.rf.b667878c15f70f722d6c79570c3b9db9.jpg', 'image_48_png_jpg.rf.226cb25a571923e01479d8d4aeb3be67.jpg', 'image_49_png_jpg.rf.5d0f74d31736473b248c4ad8f0a574fd.jpg', 'image_49_png_jpg.rf.5f9da335e92089a327e0f377012558b1.jpg', 'image_52_png_jpg.rf.1155d4d38bbcb92826202ccc1a5b6bac.jpg', 'image_55_png_jpg.rf.06a122bd9363cfac69a135ce27a844a4.jpg', 'image_55_png_jpg.rf.b32721b923260e9c786373188657a5ba.jpg', 'image_61_png_jpg.rf.66f7f24e56d32b3f531f552deb9578e5.jpg', 'image_63_png_jpg.rf.50da2d16ffe72b814dd8b3b304954544.jpg', 'image_63_png_jpg.rf.f46104dbd39ded813a73192ae7fcc380.jpg', 'image_64_png_jpg.rf.63f2ae1be42e252db504ac46ab581d24.jpg', 'image_72_png_jpg.rf.0e507d80e115e7530659f6923697bd5a.jpg', 'image_74_png_jpg.rf.502bc22b9b9ad9a22b9355f240467ff2.jpg', 'image_75_png_jpg.rf.7f28025e3543902b5bf494af2f7c447a.jpg', 'image_76_png_jpg.rf.4b0112e1ee1777c21dbc5a58a27ff5fc.jpg', 'image_78_png_jpg.rf.549e1ef6462d91b99e9c879fea2cb011.jpg', 'image_7_png_jpg.rf.ca26b9eb5a7e17b977a26416574a0723.jpg', 'image_80_png_jpg.rf.230505cfc7529ab30c671089a6384f34.jpg', 'image_81_png_jpg.rf.3de475f62e4ed613f8f78f81f8de4af0.jpg', 'image_83_png_jpg.rf.057e8ca8877b837e29d71ad391066161.jpg', 'image_83_png_jpg.rf.200c03b95858cc8786ad72d38c056c65.jpg', 'image_85_png_jpg.rf.86a8b844f4f8cb611827887b89fbfc60.jpg', 'image_93_png_jpg.rf.851fce1207ea77e72752f6ccf2bcd331.jpg', 'image_94_png_jpg.rf.f0e67d1a4c6a1a6221ea82653675af1a.jpg', 'image_95_png_jpg.rf.e8f8ac5a5fb5152e40eb67c80967c086.jpg', 'img140_jpg.rf.d90d4dcef6b69ee5c1884dc75731d75d.jpg', 'img164_jpg.rf.fcccd9cb8c64493ee6663b78de25c4f6.jpg', 'img167_jpg.rf.b95fc00cc5c931560e32be998fb6c812.jpg', 'img47_jpg.rf.732b7b2124ae4218217044f10426f9a6.jpg', 'img50_jpg.rf.e247fede0c93f8d63d57df5995461359.jpg', 'img83_jpg.rf.5a836a3cc7bbe0f4dfaaa1f06dc9094f.jpg', 'jci90c00_jpg.rf.c5111eed1e67ed36efb5a40784e3b774.jpg', 'ket24f00_jpg.rf.569cbb0d048d5f7bad6949a9022c773e.jpg', 'kgi60e00_jpg.rf.0c9eaf32d612195c1b8ae703bc0e1c8b.jpg', 'khh96d00_jpg.rf.7a6257f3f3abb2b369819650be2f7d09.jpg', 'kle30a00_jpg.rf.dcbe7031dea44be6eab3b2a4368a6915.jpg', 'lfj35f00_jpg.rf.32176c95d2361ac9f46908fe18f7aea7.jpg', 'MicrosoftTeams-image-1-_png.rf.0056a1e820bd16a3bcc12017bcbfc656.jpg', 'MicrosoftTeams-image-27-_png.rf.4f42167118f26ecfb683a97cc8501479.jpg', 'MicrosoftTeams-image-32-_png.rf.f680cdb37e54cfc97c160e4e1e0efad2.jpg', 'MicrosoftTeams-image-33-_png.rf.c8c3aa1294a2d2cb0fc366b5c5416868.jpg', 'MicrosoftTeams-image-38-_png.rf.b4c4d24eee79c70447207ab32d08af12.jpg', 'MicrosoftTeams-image-40-_png.rf.13c9f06be5c25b0e2268008523c62d72.jpg', 'MicrosoftTeams-image-46-_png.rf.96558bae10a0eb5928a972dcf60ebceb.jpg', 'MicrosoftTeams-image-5-_png.rf.fdbabefe691d568ff136bde97c478add.jpg', 'MicrosoftTeams-image-50-_png.rf.dd7f5cecec879020ab05edebf0e92d2e.jpg', 'MicrosoftTeams-image-51-_png.rf.11c7fb46d9c1144789c2be705792e40b.jpg', 'MicrosoftTeams-image-52-_png.rf.a872d3919559d478e755add9504fc93e.jpg', 'MicrosoftTeams-image-57-_png.rf.4c6ec6dfccb69239ba455902f1c20537.jpg', 'MicrosoftTeams-image-58-_png.rf.411c0daa88749c40cce29edebc52db61.jpg', 'MicrosoftTeams-image-70-_png.rf.050f0559573565a5c95a75c4df63c041.jpg', 'MicrosoftTeams-image-76-_png.rf.3237dee739dc25d88195f27705474b34.jpg', 'MicrosoftTeams-image-77-_png.rf.87dfb890a1f604991b851d43b2d1c4cb.jpg', 'MicrosoftTeams-image-79-_png.rf.6869558e7e1f2d80062ec94dfae51614.jpg', 'MicrosoftTeams-image-81-_png.rf.492c40404fb755cb19095d6a4e89d55a.jpg', 'nir55d00-page02_1_jpg.rf.198d74bcec10610d3d4648e6038e2e8c.jpg', 'njn54c00_jpg.rf.17283b1b09a497b8495288e992f23e39.jpg', 'ozj00a00_jpg.rf.498be0ce6dbe0a58836f8e60e02152db.jpg', 'qat01f00_jpg.rf.fbaad7633e8d55fb2c48719f39b2df2c.jpg', 'rsj41f00-page02_2_jpg.rf.e5fb775860fbeb80b605bb164ce0d812.jpg', 'tmp4dt7via8_PNG_jpg.rf.cb2b68d371b380626967492cbe730b77.jpg', 'tmp5ld9hcen_PNG_jpg.rf.8f3747f60576b85ba28b3151194b3284.jpg', 'tmpae22d77y_PNG.rf.8906dad0a8e20be1b71b8e426ca9307e.jpg', 'tmpb2o_kx0k_PNG.rf.0048da72ed7ffbaa0431214ec53d1e9d.jpg', 'tmpbgclc0lt_PNG.rf.7a9ffe64be15dcb55e0bb64959f0c866.jpg', 'tmpc_30kndd_PNG.rf.efce16256bc5c7262b94d71f02a894e4.jpg', 'tmpdjup300d_PNG.rf.16c7cbc055560b9560fc2cf4d26e0ef3.jpg', 'tmpfrau3rjx_PNG.rf.2daa2fc53528d0baa97911b523162c21.jpg', 'tmpfwm056ip_PNG.rf.5327d72e27a2c6eb1650a718263491dc.jpg', 'tmpgmsjmrcp_PNG.rf.322f48c41638fe73b710a99097bf7cbe.jpg', 'tmphg0h65u3_PNG.rf.6f916d0f316725e210375500fcf2ed88.jpg', 'tmphuk5w827_PNG_jpg.rf.ecd4ed619c17296278aebb6a3115a748.jpg', 'tmpl2vlg2oc_PNG.rf.c2c786baa8770946fdc52ecfe6d51e22.jpg', 'tmpl2vlg2oc_PNG_jpg.rf.89938e5dbd1698f5a39199c05130d078.jpg', 'tmpnze4nhxp_PNG.rf.f12ded9ca9b3e7ca3f95d10aecbcd1d9.jpg', 'tmpnze4nhxp_PNG_jpg.rf.f06b826e1aca1a94623b16c551eb5780.jpg', 'tmpo6zxu_a4_PNG.rf.92e3b5fadd07aaf2b060752a7d6458c0.jpg', 'tmpouk7pxxi_PNG_jpg.rf.c317cbaff439225bfe61c68dd152c1a5.jpg', 'tmpq6nvmgf5_PNG_jpg.rf.9e0a91a85582a2b24c8409469e4eac33.jpg', 'tmpqyxo3z0q_PNG.rf.d2c5942d1b78ab3df35b205b90f9d76c.jpg', 'tmpr5w0ulcj_PNG.rf.f7f981913851ecafa89d9fb08393d341.jpg', 'tmpu4s50oq1_PNG_jpg.rf.0a1d0ba9148d7b57fdfc801fba0be351.jpg', 'tmpvob6dej2_PNG_jpg.rf.0707cbdd909cdfa4e73d46d15a25b479.jpg', 'tmpwwsdmjd7_PNG_jpg.rf.fd140d3f84541d2cbc061be5a6e6d1a4.jpg', 'tmpzuxq932k_PNG.rf.534c52d6790045763c98c1f3f308b4ed.jpg', 'tmpzuxq932k_PNG_jpg.rf.1f488421467e95f95af54d71dd81b1b5.jpg', 'wab91d00-var_jpg.rf.bbfc6fc7165374d15f1e98a448680c14.jpg', 'waq00e00_jpg.rf.a800fff3eae721a577b8799ac1a3c288.jpg', 'wau30a00-page9_7_jpg.rf.93f3aaa496f71ebeca03d4345551a466.jpg', 'wav95e00-page03_1_jpg.rf.2408f8056958cb57550c25a418fae28f.jpg', 'wav95e00-page03_3_jpg.rf.90605e9cff005a5e3160f5d90bae12b3.jpg']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_path = \"./datasets/signature/train/images\"\n",
    "valid_path = \"./datasets/signature/valid/images\"\n",
    "\n",
    "print(\"Train images:\", os.listdir(train_path))\n",
    "print(\"Valid images:\", os.listdir(valid_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "print(\"Torch Version:\", torch.__version__)\n",
    "\n",
    "\n",
    "signature_yaml_path = \"C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml\"  \n",
    "pretrained_model = \"yolo11n.pt\"  \n",
    "output_model_path = \"Assurances/YOLO_Trained/best3.pt\"\n",
    "\n",
    "\n",
    "if not os.path.exists(signature_yaml_path):\n",
    "    raise FileNotFoundError(f\"Le fichier {signature_yaml_path} est introuvable.\")\n",
    "\n",
    "\n",
    "print(\"\\n=== Chargement du modèle YOLO11n ===\")\n",
    "model = YOLO(pretrained_model)\n",
    "\n",
    "\n",
    "print(\"\\n=== Test du modèle ===\")\n",
    "model.info()\n",
    "\n",
    "\n",
    "print(\"\\n=== Début de l'entraînement ===\")\n",
    "results = model.train(\n",
    "    data=signature_yaml_path,\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    val=True,\n",
    "    single_cls=True,\n",
    "    resume=False\n",
    ")\n",
    "\n",
    "os.makedirs(os.path.dirname(output_model_path), exist_ok=True)\n",
    "model.save(output_model_path)\n",
    "print(f\"\\n[INFO] Modèle YOLO11n entraîné sauvegardé dans : {output_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tester le modèle sur documents Assurances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "trained_model_path = \"Assurances/YOLO_Trained/best_epoch_501.pt\"\n",
    "model = YOLO(trained_model_path)\n",
    "\n",
    "test_image = \"./Assurances/Converted_Images/POINT DU JOUR  Dispositions particulières GENERALI du 15 05 2020_page_1.png\"\n",
    "\n",
    "output_folder = \"./results/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "results = model.predict(test_image, conf=0.4)\n",
    "\n",
    "\n",
    "image = cv2.imread(test_image)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "\n",
    "\n",
    "detections = results[0].boxes.xyxy  \n",
    "scores = results[0].boxes.conf  \n",
    "labels = results[0].boxes.cls  \n",
    "\n",
    "print(f\"Nombre de détections : {len(detections)}\")\n",
    "\n",
    "for i, box in enumerate(detections):\n",
    "    x1, y1, x2, y2 = map(int, box)  \n",
    "    confidence = scores[i].item()  \n",
    "\n",
    "    cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(image_rgb, f\"Signature: {confidence:.2f}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "\n",
    "    cropped_signature = image[y1:y2, x1:x2]\n",
    "\n",
    "   \n",
    "    cropped_output_path = os.path.join(output_folder, f\"signature_{i + 1}.png\")\n",
    "    cv2.imwrite(cropped_output_path, cropped_signature)\n",
    "    print(f\"Recadrage enregistré : {cropped_output_path}\")\n",
    "\n",
    "output_image_path = os.path.join(output_folder, \"detections_with_boxes.png\")\n",
    "cv2.imwrite(output_image_path, cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR))\n",
    "print(f\"Image avec détections enregistrée : {output_image_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_yolo_annotations(path):\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(path, file), 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5:\n",
    "                        print(f\"Erreur dans {file}: {line}\")\n",
    "\n",
    "check_yolo_annotations(\"datasets/signature/train/labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def clean_yolo_labels(labels_dir):\n",
    "    \"\"\"\n",
    "    Nettoie les fichiers d'annotations YOLO pour s'assurer qu'ils ne contiennent que des bounding boxes valides.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(labels_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(labels_dir, filename)\n",
    "            valid_lines = []\n",
    "            \n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "        \n",
    "                    if len(parts) == 5:\n",
    "                        try:       \n",
    "                            class_id = int(parts[0])\n",
    "                            x_center, y_center, width, height = map(float, parts[1:])\n",
    "                            valid_lines.append(line.strip())\n",
    "                        except ValueError:\n",
    "                            print(f\"Erreur de format dans {filename}, ligne ignorée : {line.strip()}\")\n",
    "                    else:\n",
    "                        print(f\"Ligne invalide dans {filename}, ligne ignorée : {line.strip()}\")\n",
    "\n",
    "            with open(file_path, \"w\") as f:\n",
    "                for line in valid_lines:\n",
    "                    f.write(line + \"\\n\")\n",
    "\n",
    "            print(f\"Fichier nettoyé : {filename} - {len(valid_lines)} lignes valides.\")\n",
    "\n",
    "labels_directory = \"datasets/signature/train/labels\"\n",
    "clean_yolo_labels(labels_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuster les labels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def clean_yolo_labels(labels_dir):\n",
    "    \"\"\"\n",
    "    Nettoie les fichiers d'annotations YOLO pour s'assurer qu'ils ne contiennent que des bounding boxes valides.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(labels_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(labels_dir, filename)\n",
    "            valid_lines = []\n",
    "            \n",
    "          \n",
    "            with open(file_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                 \n",
    "                    if len(parts) == 5:\n",
    "                        try:\n",
    "                     \n",
    "                            class_id = int(parts[0])\n",
    "                            x_center, y_center, width, height = map(float, parts[1:])\n",
    "                            valid_lines.append(line.strip())\n",
    "                        except ValueError:\n",
    "                            print(f\"Erreur de format dans {filename}, ligne ignorée : {line.strip()}\")\n",
    "                    else:\n",
    "                        print(f\"Ligne invalide dans {filename}, ligne ignorée : {line.strip()}\")\n",
    "\n",
    "            with open(file_path, \"w\") as f:\n",
    "                for line in valid_lines:\n",
    "                    f.write(line + \"\\n\")\n",
    "\n",
    "            print(f\"Fichier nettoyé : {filename} - {len(valid_lines)} lignes valides.\")\n",
    "\n",
    "\n",
    "labels_directory = \"datasets/signature/valid/labels\"\n",
    "clean_yolo_labels(labels_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import settings\n",
    "settings.log_level = 'error'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Début de l'entraînement ===\n",
      "\n",
      "[INFO] Entraînement de l'époque 1 à 5...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train54, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train54\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train54', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:03<00:00, 332.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<00:00, 321.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train54\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 03:37:07 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 03:37:07 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 03:37:07 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 03:37:09 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 03:37:09 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(bdb5853480a347c6bcfc8b2426986f8f) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train54\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.737      1.549      1.557         44        640: 100%|██████████| 32/32 [11:09<00:00, 20.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:55<00:00, 13.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.841      0.829      0.873      0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.619      1.193      1.493         33        640: 100%|██████████| 32/32 [07:14<00:00, 13.56s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:30<00:00,  7.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.828      0.844      0.882      0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.553      1.183      1.453         35        640: 100%|██████████| 32/32 [06:39<00:00, 12.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:34<00:00,  8.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.849      0.839      0.892      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.543      1.136      1.436         35        640: 100%|██████████| 32/32 [06:45<00:00, 12.68s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:34<00:00,  8.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.847      0.763      0.863      0.468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.462      1.053      1.388         21        640: 100%|██████████| 32/32 [11:40<00:00, 21.89s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:58<00:00, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.917      0.889      0.935      0.548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.789 hours.\n",
      "Optimizer stripped from runs\\detect\\train54\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train54\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train54\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:53<00:00, 13.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.917      0.888      0.935      0.548\n",
      "Speed: 6.8ms preprocess, 187.9ms inference, 0.0ms loss, 1.2ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train54\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 5 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_5.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 04:26:12 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpubz3pea5\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 04:26:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 6 à 10...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train542, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train542\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train542', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train542\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 04:26:23 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 04:26:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 04:26:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 04:26:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 04:26:23 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(2acd7ebf7f0d496fbbac15afd25a838f) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train542\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.414      0.983      1.358         44        640: 100%|██████████| 32/32 [8:34:15<00:00, 964.23s/it]    \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:31<00:00,  7.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.858      0.884      0.924      0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G       1.41     0.9386      1.369         33        640: 100%|██████████| 32/32 [05:59<00:00, 11.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:30<00:00,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.859      0.843      0.875      0.508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.392     0.9698      1.351         35        640: 100%|██████████| 32/32 [06:06<00:00, 11.46s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.877      0.831      0.913      0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.418     0.9862       1.36         35        640: 100%|██████████| 32/32 [05:24<00:00, 10.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.899      0.872      0.932      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.386     0.9693      1.346         21        640: 100%|██████████| 32/32 [05:30<00:00, 10.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.916      0.897      0.934      0.547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 9.002 hours.\n",
      "Optimizer stripped from runs\\detect\\train542\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train542\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train542\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:23<00:00,  5.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.916      0.897      0.934      0.547\n",
      "Speed: 3.3ms preprocess, 85.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train542\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 13:27:01 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpuhtn3qe7\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 10 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 13:27:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 11 à 15...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5422, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5422\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5422', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5422\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 13:27:05 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 13:27:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 13:27:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 13:27:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 13:27:05 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(fd4397eddd5045fd949b38b69c4206c6) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5422\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.277     0.8519      1.288         44        640: 100%|██████████| 32/32 [05:27<00:00, 10.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.888      0.883      0.924      0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.254     0.8143       1.28         33        640: 100%|██████████| 32/32 [05:20<00:00, 10.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.87      0.867      0.903       0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.299     0.8694      1.298         35        640: 100%|██████████| 32/32 [05:19<00:00,  9.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.885      0.857      0.906      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.356     0.9226      1.315         35        640: 100%|██████████| 32/32 [05:20<00:00, 10.01s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.856      0.874      0.898      0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.331     0.9134      1.311         21        640: 100%|██████████| 32/32 [05:23<00:00, 10.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.891      0.875      0.916      0.558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.487 hours.\n",
      "Optimizer stripped from runs\\detect\\train5422\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train5422\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train5422\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:23<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.891      0.874      0.915      0.558\n",
      "Speed: 3.4ms preprocess, 85.5ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5422\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 13:56:48 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpqyt72rzh\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 13:56:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 15 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_15.pt\n",
      "\n",
      "[INFO] Entraînement de l'époque 16 à 20...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train54222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train54222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train54222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train54222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 13:56:52 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 13:56:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 13:56:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 13:56:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 13:56:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(14000138066047d494cadf201280bc2b) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train54222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.187     0.7873      1.231         44        640: 100%|██████████| 32/32 [05:24<00:00, 10.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.898      0.864      0.912      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.163     0.7564      1.229         33        640: 100%|██████████| 32/32 [05:22<00:00, 10.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.888       0.83      0.898       0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.238     0.8162      1.252         35        640: 100%|██████████| 32/32 [05:19<00:00, 10.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.839      0.861      0.886      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.276     0.8478      1.276         35        640: 100%|██████████| 32/32 [05:26<00:00, 10.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.888      0.851      0.914      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.292     0.8613      1.289         21        640: 100%|██████████| 32/32 [05:30<00:00, 10.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.895      0.878       0.91      0.552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.491 hours.\n",
      "Optimizer stripped from runs\\detect\\train54222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train54222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train54222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.895      0.877       0.91      0.552\n",
      "Speed: 3.4ms preprocess, 85.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train54222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 14:26:45 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpim9exewm\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 20 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_20.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 14:26:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 21 à 25...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train542222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train542222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train542222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train542222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 14:26:48 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 14:26:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 14:26:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 14:26:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 14:26:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(8dcd76149b464e7c87db58126b14bb30) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train542222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.095     0.7185      1.185         44        640: 100%|██████████| 32/32 [05:26<00:00, 10.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.826      0.848      0.876      0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.101     0.7068      1.192         33        640: 100%|██████████| 32/32 [05:23<00:00, 10.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.855      0.819      0.888      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.186     0.7669      1.221         35        640: 100%|██████████| 32/32 [05:28<00:00, 10.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.771       0.83      0.823      0.439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.231      0.803      1.251         35        640: 100%|██████████| 32/32 [05:26<00:00, 10.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.87      0.817      0.879      0.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.263     0.8322       1.27         21        640: 100%|██████████| 32/32 [05:25<00:00, 10.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.894      0.893      0.922       0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.492 hours.\n",
      "Optimizer stripped from runs\\detect\\train542222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train542222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train542222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:23<00:00,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.894      0.893      0.922      0.549\n",
      "Speed: 3.5ms preprocess, 87.2ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train542222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 14:56:45 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpllcyxgxv\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 14:56:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 25 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_25.pt\n",
      "\n",
      "[INFO] Entraînement de l'époque 26 à 30...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5422222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5422222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5422222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5422222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 14:56:48 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 14:56:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 14:56:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 14:56:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 14:56:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(b257b91209f446bca14cac474d445e29) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5422222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.021     0.6778      1.145         44        640: 100%|██████████| 32/32 [05:30<00:00, 10.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.894      0.871      0.916      0.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.092     0.6899       1.18         33        640: 100%|██████████| 32/32 [05:26<00:00, 10.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.837      0.862      0.874      0.507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.146     0.7449        1.2         35        640: 100%|██████████| 32/32 [05:24<00:00, 10.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.804      0.848      0.842      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.197     0.7722      1.229         35        640: 100%|██████████| 32/32 [05:27<00:00, 10.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.843      0.839      0.874      0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.226     0.7963      1.247         21        640: 100%|██████████| 32/32 [05:28<00:00, 10.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.898      0.906      0.925      0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.494 hours.\n",
      "Optimizer stripped from runs\\detect\\train5422222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train5422222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train5422222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.898      0.906      0.925      0.561\n",
      "Speed: 3.4ms preprocess, 85.3ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5422222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 15:26:52 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp7r1cqch1\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 30 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_30.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 15:26:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 31 à 35...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train54222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train54222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train54222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train54222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 15:26:55 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 15:26:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 15:26:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 15:26:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 15:26:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(3f12cd3f03c74ef6b9b684887dc6941c) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train54222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      0.963     0.6416      1.116         44        640: 100%|██████████| 32/32 [05:31<00:00, 10.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.863       0.84      0.885      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G       1.11     0.6779      1.182         33        640: 100%|██████████| 32/32 [05:25<00:00, 10.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.857      0.848       0.89      0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.221      0.768      1.231         35        640: 100%|██████████| 32/32 [05:23<00:00, 10.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.819      0.915      0.904      0.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.173     0.7583      1.218         35        640: 100%|██████████| 32/32 [05:24<00:00, 10.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.881      0.862      0.895      0.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.221     0.7904      1.249         21        640: 100%|██████████| 32/32 [05:27<00:00, 10.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.896      0.885      0.919      0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.492 hours.\n",
      "Optimizer stripped from runs\\detect\\train54222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train54222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train54222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.896      0.885      0.919      0.553\n",
      "Speed: 3.5ms preprocess, 83.8ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train54222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 15:56:54 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpatg2uwfs\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 35 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_35.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 15:56:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 36 à 40...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train542222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train542222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train542222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train542222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 15:56:57 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 15:56:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 15:56:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 15:56:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 15:56:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(e73e331600a641f4bbea2f1502692017) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train542222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      0.926     0.6083      1.096         44        640: 100%|██████████| 32/32 [05:34<00:00, 10.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.895      0.875      0.922      0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G      1.018     0.6203      1.139         33        640: 100%|██████████| 32/32 [05:45<00:00, 10.79s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:28<00:00,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.89      0.865      0.903      0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.152     0.7041      1.183         35        640: 100%|██████████| 32/32 [05:27<00:00, 10.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.863       0.87        0.9      0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.161     0.7435      1.205         35        640: 100%|██████████| 32/32 [05:27<00:00, 10.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.883      0.875      0.909      0.488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.207     0.7832      1.245         21        640: 100%|██████████| 32/32 [05:27<00:00, 10.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.882      0.879      0.905      0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.501 hours.\n",
      "Optimizer stripped from runs\\detect\\train542222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train542222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train542222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.876      0.879      0.904       0.55\n",
      "Speed: 3.4ms preprocess, 86.2ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train542222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 40 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_40.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 16:27:30 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp1tu7sqnu\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 16:27:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 41 à 45...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5422222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5422222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5422222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5422222222\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 16:27:33 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 16:27:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 16:27:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 16:27:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 16:27:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(cbcaea23082740298d31ebff7545e82e) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5422222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.8788     0.5878      1.075         44        640: 100%|██████████| 32/32 [05:30<00:00, 10.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.885      0.871      0.901      0.514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.9717     0.6007      1.107         33        640: 100%|██████████| 32/32 [05:26<00:00, 10.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.91      0.866      0.903      0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.069     0.6617      1.137         35        640: 100%|██████████| 32/32 [05:22<00:00, 10.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.815      0.806      0.821      0.415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.139     0.7184      1.178         35        640: 100%|██████████| 32/32 [05:22<00:00, 10.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.867      0.877      0.912       0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.174     0.7541       1.22         21        640: 100%|██████████| 32/32 [05:31<00:00, 10.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.887      0.906      0.908      0.538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.492 hours.\n",
      "Optimizer stripped from runs\\detect\\train5422222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train5422222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train5422222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.888      0.906      0.908      0.539\n",
      "Speed: 3.2ms preprocess, 84.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5422222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 16:57:32 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp8qlgwj7_\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 45 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_45.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 16:57:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 46 à 50...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train54222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train54222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train54222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train54222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 16:57:34 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 16:57:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 16:57:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 16:57:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 16:57:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(b5d526e84a9746d3a95cec903f7667dc) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train54222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.8495     0.5633      1.053         44        640: 100%|██████████| 32/32 [05:29<00:00, 10.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.893      0.866      0.901      0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.9716     0.5855        1.1         33        640: 100%|██████████| 32/32 [05:26<00:00, 10.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.81      0.853      0.857      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.051     0.6505       1.14         35        640: 100%|██████████| 32/32 [05:22<00:00, 10.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.838       0.79      0.837      0.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.083     0.6802       1.15         35        640: 100%|██████████| 32/32 [05:23<00:00, 10.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.823      0.853      0.887       0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.147     0.7269      1.208         21        640: 100%|██████████| 32/32 [05:24<00:00, 10.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.871      0.893      0.903      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.490 hours.\n",
      "Optimizer stripped from runs\\detect\\train54222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train54222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train54222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.87      0.893      0.903      0.534\n",
      "Speed: 3.2ms preprocess, 83.7ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train54222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 17:27:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpgcrim8ms\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 17:27:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 50 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_50.pt\n",
      "\n",
      "[INFO] Entraînement de l'époque 51 à 55...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train542222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train542222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train542222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train542222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 17:27:28 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 17:27:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 17:27:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 17:27:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 17:27:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(6ab64ef1f32f4766b37fa302d7f7788a) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train542222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.8071     0.5412      1.033         44        640: 100%|██████████| 32/32 [05:29<00:00, 10.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.837      0.844      0.868      0.408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.9057     0.5679       1.06         33        640: 100%|██████████| 32/32 [05:24<00:00, 10.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.864      0.821      0.855       0.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      1.025      0.625       1.12         35        640: 100%|██████████| 32/32 [05:23<00:00, 10.10s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.845      0.812      0.842      0.476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.068     0.6712      1.142         35        640: 100%|██████████| 32/32 [05:21<00:00, 10.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.878      0.868      0.905      0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.139     0.7187      1.202         21        640: 100%|██████████| 32/32 [05:25<00:00, 10.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.894      0.904      0.909      0.553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.490 hours.\n",
      "Optimizer stripped from runs\\detect\\train542222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train542222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train542222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.891      0.902      0.909      0.554\n",
      "Speed: 3.4ms preprocess, 83.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train542222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 17:57:19 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpmh5w_5_i\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 17:57:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 55 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_55.pt\n",
      "\n",
      "[INFO] Entraînement de l'époque 56 à 60...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5422222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5422222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5422222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5422222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 17:57:22 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 17:57:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 17:57:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 17:57:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 17:57:22 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(72965e7437234242bc682caefa27cbe0) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5422222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.7658     0.5207      1.016         44        640: 100%|██████████| 32/32 [05:29<00:00, 10.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.882      0.905      0.917      0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.9009     0.5525      1.049         33        640: 100%|██████████| 32/32 [05:23<00:00, 10.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.842      0.839      0.846      0.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      0.961     0.6024      1.086         35        640: 100%|██████████| 32/32 [05:21<00:00, 10.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.792      0.799      0.829      0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.034     0.6511      1.123         35        640: 100%|██████████| 32/32 [05:21<00:00, 10.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.874      0.868      0.877      0.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G       1.12     0.6995      1.191         21        640: 100%|██████████| 32/32 [05:26<00:00, 10.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.881      0.866      0.907      0.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.490 hours.\n",
      "Optimizer stripped from runs\\detect\\train5422222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train5422222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train5422222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.881      0.866      0.907      0.566\n",
      "Speed: 3.2ms preprocess, 84.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5422222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 18:27:11 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpgmug4z0n\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 18:27:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 60 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_60.pt\n",
      "\n",
      "[INFO] Entraînement de l'époque 61 à 65...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train54222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train54222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train54222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train54222222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 18:27:14 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 18:27:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 18:27:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 18:27:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 18:27:14 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(2a09b6429f3d4194a2ec0f8d1390515f) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train54222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.7466     0.5106      1.001         44        640: 100%|██████████| 32/32 [05:29<00:00, 10.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.884      0.882      0.899      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.8072     0.5086      1.012         33        640: 100%|██████████| 32/32 [05:24<00:00, 10.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.903      0.879      0.882      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G     0.9302     0.5799      1.075         35        640: 100%|██████████| 32/32 [05:23<00:00, 10.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.848      0.849      0.866      0.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.037     0.6344       1.12         35        640: 100%|██████████| 32/32 [05:28<00:00, 10.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.866      0.844      0.882      0.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.107     0.6828      1.184         21        640: 100%|██████████| 32/32 [05:49<00:00, 10.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:28<00:00,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.887      0.884      0.903      0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.499 hours.\n",
      "Optimizer stripped from runs\\detect\\train54222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train54222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train54222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:34<00:00,  8.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.887      0.884      0.903      0.562\n",
      "Speed: 3.7ms preprocess, 124.9ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train54222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 18:57:53 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp14xanbaa\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 65 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_65.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 18:57:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 66 à 70...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train542222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train542222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train542222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train542222222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 18:57:59 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 18:57:59 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 18:57:59 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 18:57:59 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 18:57:59 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(cd9deef6093a4445b9e403a9c5789c14) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train542222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.7033     0.4853     0.9859         44        640: 100%|██████████| 32/32 [06:07<00:00, 11.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:28<00:00,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.917       0.84      0.907      0.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.7637     0.4931     0.9937         33        640: 100%|██████████| 32/32 [05:42<00:00, 10.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:35<00:00,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.908      0.842      0.906      0.534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G     0.9383     0.5639      1.072         35        640: 100%|██████████| 32/32 [09:40<00:00, 18.14s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:49<00:00, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.876      0.852      0.879      0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.052     0.6327      1.129         35        640: 100%|██████████| 32/32 [07:04<00:00, 13.26s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.867      0.875       0.89      0.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.144      0.687      1.204         21        640: 100%|██████████| 32/32 [05:31<00:00, 10.35s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.84      0.902      0.892      0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.618 hours.\n",
      "Optimizer stripped from runs\\detect\\train542222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train542222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train542222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.908      0.841      0.906      0.534\n",
      "Speed: 3.2ms preprocess, 86.6ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train542222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 19:35:32 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp2qgsznhb\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 70 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_70.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 19:35:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 71 à 75...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5422222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5422222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5422222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5422222222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 19:35:35 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 19:35:35 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 19:35:35 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 19:35:35 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 19:35:35 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(dc50a1353f874cafa5cb18a82b3f3057) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5422222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.7671     0.5039     0.9993         44        640: 100%|██████████| 32/32 [05:50<00:00, 10.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:29<00:00,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.888      0.848      0.913      0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.7626     0.4854     0.9858         33        640: 100%|██████████| 32/32 [05:59<00:00, 11.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:29<00:00,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.861      0.859      0.888      0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G     0.9617     0.5811      1.077         35        640: 100%|██████████| 32/32 [06:01<00:00, 11.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:29<00:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.885      0.839      0.891       0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.057     0.6336      1.138         35        640: 100%|██████████| 32/32 [06:18<00:00, 11.84s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:29<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.922       0.84       0.89      0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.141     0.6906      1.202         21        640: 100%|██████████| 32/32 [06:32<00:00, 12.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:34<00:00,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.877      0.894      0.907      0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.557 hours.\n",
      "Optimizer stripped from runs\\detect\\train5422222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train5422222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train5422222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:30<00:00,  7.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.877      0.894      0.907      0.533\n",
      "Speed: 3.9ms preprocess, 109.6ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5422222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 75 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_75.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 20:09:38 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpkhf516v1\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 20:09:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 76 à 80...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train54222222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train54222222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train54222222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train54222222222222222\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 20:09:43 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 20:09:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 20:09:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 20:09:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 20:09:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(7b5ff65e27f44ca297234ce1bb3546ae) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train54222222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.6877     0.4737     0.9761         44        640: 100%|██████████| 32/32 [10:14<00:00, 19.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:29<00:00,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.892      0.871      0.911      0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.7237     0.4728     0.9687         33        640: 100%|██████████| 32/32 [06:49<00:00, 12.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:28<00:00,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.848      0.862      0.871      0.504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G     0.8923     0.5539      1.046         35        640: 100%|██████████| 32/32 [05:27<00:00, 10.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.824      0.879      0.843      0.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.026     0.6186      1.123         35        640: 100%|██████████| 32/32 [05:15<00:00,  9.85s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.859      0.804      0.875      0.487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.096     0.6702      1.168         21        640: 100%|██████████| 32/32 [05:14<00:00,  9.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.861      0.888      0.895      0.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.592 hours.\n",
      "Optimizer stripped from runs\\detect\\train54222222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train54222222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train54222222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.891      0.871      0.911      0.518\n",
      "Speed: 3.2ms preprocess, 84.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train54222222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 80 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_80.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 20:45:42 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp4lfpx6tp\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 20:45:42 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 81 à 85...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train542222222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train542222222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train542222222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train542222222222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 20:45:45 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 20:45:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 20:45:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 20:45:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 20:45:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(1bac3446a5204a72be0867ffb5663591) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train542222222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.6373     0.4374     0.9467         44        640: 100%|██████████| 32/32 [05:18<00:00,  9.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224        0.9      0.881      0.925      0.549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.7359     0.4757     0.9712         33        640: 100%|██████████| 32/32 [05:19<00:00,  9.98s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.859      0.871      0.889      0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G      0.878     0.5511      1.038         35        640: 100%|██████████| 32/32 [05:18<00:00,  9.95s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.829      0.862      0.857      0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.014     0.6153      1.115         35        640: 100%|██████████| 32/32 [05:13<00:00,  9.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.902      0.835      0.891      0.517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.101     0.6753      1.184         21        640: 100%|██████████| 32/32 [05:14<00:00,  9.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:24<00:00,  6.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.89      0.902      0.901      0.516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.478 hours.\n",
      "Optimizer stripped from runs\\detect\\train542222222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train542222222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train542222222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.898      0.879      0.925       0.55\n",
      "Speed: 3.2ms preprocess, 83.8ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train542222222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 21:14:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpe8vwkr60\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 85 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_85.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 21:14:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 86 à 90...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5422222222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5422222222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5422222222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5422222222222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 21:14:53 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 21:14:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 21:14:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 21:14:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 21:14:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(2e4c6e70b03a47b28e66aaf65640a795) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5422222222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.5695     0.4039      0.923         44        640: 100%|██████████| 32/32 [05:17<00:00,  9.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.883      0.911      0.925      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.7654     0.4759     0.9815         33        640: 100%|██████████| 32/32 [05:15<00:00,  9.85s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.869      0.887      0.901      0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G     0.9175     0.5571      1.059         35        640: 100%|██████████| 32/32 [05:15<00:00,  9.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.836      0.868      0.859      0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G       1.03     0.6175      1.121         35        640: 100%|██████████| 32/32 [05:14<00:00,  9.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:25<00:00,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.795      0.848      0.867      0.499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.112     0.6867      1.179         21        640: 100%|██████████| 32/32 [06:05<00:00, 11.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:30<00:00,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.868      0.907      0.899      0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.492 hours.\n",
      "Optimizer stripped from runs\\detect\\train5422222222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train5422222222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train5422222222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.884      0.911      0.925      0.546\n",
      "Speed: 3.8ms preprocess, 92.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5422222222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 90 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_90.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 21:44:58 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpdf39in1z\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 21:44:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 91 à 95...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train54222222222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train54222222222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train54222222222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train54222222222222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 21:45:03 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 21:45:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 21:45:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 21:45:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 21:45:03 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(503a37edd7874fa09167a1fae490ce22) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train54222222222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.5584     0.3939      0.914         44        640: 100%|██████████| 32/32 [06:25<00:00, 12.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:30<00:00,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.892      0.897      0.921      0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.7855     0.4839      0.985         33        640: 100%|██████████| 32/32 [09:14<00:00, 17.34s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:44<00:00, 11.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.898      0.884      0.908      0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G     0.9186     0.5571      1.055         35        640: 100%|██████████| 32/32 [10:50<00:00, 20.31s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:51<00:00, 12.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.844      0.857      0.875      0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G       1.01     0.6061      1.118         35        640: 100%|██████████| 32/32 [10:37<00:00, 19.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:49<00:00, 12.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.872      0.848      0.887      0.498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.115     0.6765      1.179         21        640: 100%|██████████| 32/32 [10:28<00:00, 19.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:45<00:00, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.887      0.884      0.905      0.509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.862 hours.\n",
      "Optimizer stripped from runs\\detect\\train54222222222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train54222222222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train54222222222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:36<00:00,  9.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224       0.89      0.897      0.921      0.554\n",
      "Speed: 4.1ms preprocess, 132.6ms inference, 0.0ms loss, 1.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train54222222222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 95 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_95.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 22:37:32 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpi93rws67\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 22:37:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 96 à 100...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/best_epoch_101.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=5, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train542222222222222222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train542222222222222222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 499/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train542222222222222222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 1010 images, 25 backgrounds, 0 corrupt: 100%|██████████| 1010/1010 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 218 images, 7 backgrounds, 0 corrupt: 100%|██████████| 218/218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train542222222222222222222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 22:37:37 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/23 22:37:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/23 22:37:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/23 22:37:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/23 22:37:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(183798ac237145b5bd6466594ee029d2) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train542222222222222222222\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G     0.5749      0.395     0.9107         44        640: 100%|██████████| 32/32 [09:05<00:00, 17.06s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:41<00:00, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.905      0.888      0.918      0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.7677      0.486     0.9817         33        640: 100%|██████████| 32/32 [10:28<00:00, 19.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:44<00:00, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.875      0.888      0.897      0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/5         0G        0.9     0.5628      1.049         35        640: 100%|██████████| 32/32 [06:07<00:00, 11.50s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.841      0.848      0.876      0.493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        4/5         0G      1.019     0.6218      1.117         35        640: 100%|██████████| 32/32 [05:31<00:00, 10.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.938      0.786      0.884      0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        5/5         0G      1.119     0.6828      1.192         21        640: 100%|██████████| 32/32 [05:26<00:00, 10.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:26<00:00,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.866      0.868      0.878       0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5 epochs completed in 0.662 hours.\n",
      "Optimizer stripped from runs\\detect\\train542222222222222222222\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from runs\\detect\\train542222222222222222222\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating runs\\detect\\train542222222222222222222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 4/4 [00:22<00:00,  5.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        218        224      0.905      0.893      0.918      0.539\n",
      "Speed: 3.3ms preprocess, 85.8ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train542222222222222222222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/23 23:17:49 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp28ewfdnj\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/23 23:17:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Modèle sauvegardé après 100 époques dans : Assurances/YOLO_Trained/best_epoch_vf3.pt/best_epoch_100.pt\n",
      "\n",
      "=== Entraînement terminé ===\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "signature_yaml_path = \"C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml\"\n",
    "pretrained_model = \"yolov8s.pt\"\n",
    "output_model_dir = \"Assurances/YOLO_Trained/best_epoch_vf4.pt/\"\n",
    "\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "model = YOLO(pretrained_model)\n",
    "\n",
    "total_epochs = 100\n",
    "save_interval = 5\n",
    "imgsz = 640\n",
    "batch_size = 32\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:///\" + os.path.abspath(\"mlruns\"))\n",
    "mlflow.set_experiment(\"YOLOv8_Signature_Detection\")\n",
    "\n",
    "def download_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return np.array(img)\n",
    "\n",
    "def test_model_on_image(model_path, image_url, conf_threshold=0.3):\n",
    "    model_test = YOLO(model_path)\n",
    "    img = download_image_from_url(image_url)\n",
    "\n",
    "    results = model_test.predict(source=img, conf=conf_threshold)\n",
    "\n",
    "    image_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    for box in results[0].boxes.xyxy:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    cropped_signatures = []\n",
    "    for i, box in enumerate(results[0].boxes.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cropped_signature = img[y1:y2, x1:x2]\n",
    "        cropped_signatures.append(cropped_signature)\n",
    "\n",
    "    output_image_path = \"detection_result.png\"\n",
    "    cv2.imwrite(output_image_path, image_bgr)\n",
    "\n",
    "    return output_image_path, cropped_signatures\n",
    "\n",
    "\n",
    "print(\"\\n=== Début de l'entraînement ===\")\n",
    "\n",
    "for start_epoch in range(0, total_epochs, save_interval):\n",
    "    end_epoch = min(start_epoch + save_interval, total_epochs)\n",
    "    print(f\"\\n[INFO] Entraînement de l'époque {start_epoch + 1} à {end_epoch}...\")\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        results = model.train(\n",
    "            data=signature_yaml_path,\n",
    "            epochs=save_interval,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch_size,\n",
    "            val=True,\n",
    "            single_cls=True,\n",
    "            workers=4,\n",
    "            amp=True,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        last_model_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.pt\")\n",
    "        epoch_model_path = os.path.join(output_model_dir, f\"best_epoch_{end_epoch}.pt\")\n",
    "\n",
    "        if os.path.exists(last_model_path):\n",
    "            shutil.copy(last_model_path, epoch_model_path)\n",
    "            print(f\"[INFO] Modèle sauvegardé après {end_epoch} époques dans : {epoch_model_path}\")\n",
    "\n",
    "            mlflow.log_param(\"epochs\", end_epoch)\n",
    "            mlflow.log_param(\"imgsz\", imgsz)\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "            metrics = results.results_dict\n",
    "            mlflow.log_metrics({\n",
    "                \"precision\": metrics.get('metrics/precision(B)', 0),\n",
    "                \"recall\": metrics.get('metrics/recall(B)', 0),\n",
    "                \"mAP50\": metrics.get('metrics/mAP50(B)', 0),\n",
    "                \"mAP50-95\": metrics.get('metrics/mAP50-95(B)', 0)\n",
    "            })\n",
    "\n",
    "            mlflow.pytorch.log_model(model.model, artifact_path=f\"yolo_signature_epoch_{end_epoch}\")\n",
    "            mlflow.log_artifact(epoch_model_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"[WARNING] Aucun modèle best.pt trouvé après {end_epoch} époques.\")\n",
    "\n",
    "    mlflow.end_run()  \n",
    "\n",
    "print(\"\\n=== Entraînement terminé ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Début de l'entraînement ===\n",
      "\n",
      "[INFO] Entraînement de l'époque 1 à 10...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train55, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train55\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train55', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels... 616 images, 4 backgrounds, 0 corrupt: 100%|██████████| 616/616 [00:01<00:00, 403.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels... 148 images, 7 backgrounds, 0 corrupt: 100%|██████████| 148/148 [00:00<00:00, 318.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train55\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 00:30:25 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/24 00:30:25 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/24 00:30:25 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/24 00:30:25 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/24 00:30:25 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(bd30cf7eed8f4a5d93265aa715c4e4cd) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train55\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.501      4.523      1.345          8        640: 100%|██████████| 20/20 [11:30<00:00, 34.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.62      0.584      0.598      0.361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.466       1.81      1.325         13        640: 100%|██████████| 20/20 [08:36<00:00, 25.83s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.262      0.468      0.328      0.119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.511      1.504      1.367          8        640: 100%|██████████| 20/20 [08:37<00:00, 25.85s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154     0.0307      0.338      0.174     0.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.433      1.292      1.326          7        640: 100%|██████████| 20/20 [08:31<00:00, 25.55s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154     0.0333       0.37     0.0213    0.00632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.468      1.241      1.345          8        640: 100%|██████████| 20/20 [08:18<00:00, 24.94s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154     0.0383      0.519     0.0296     0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.402      1.134      1.338          9        640: 100%|██████████| 20/20 [08:37<00:00, 25.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.114      0.662      0.104     0.0457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.402      1.097       1.31          8        640: 100%|██████████| 20/20 [08:17<00:00, 24.86s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.683      0.773      0.813      0.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G       1.27      0.953      1.228          8        640: 100%|██████████| 20/20 [08:14<00:00, 24.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.784      0.799      0.844       0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.202     0.8768      1.202         11        640: 100%|██████████| 20/20 [08:22<00:00, 25.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.863      0.779      0.852      0.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G       1.16     0.8375      1.187         10        640: 100%|██████████| 20/20 [08:54<00:00, 26.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.829      0.883      0.903      0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 1.585 hours.\n",
      "Optimizer stripped from runs\\detect\\train55\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train55\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train55\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:34<00:00, 11.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.829      0.883      0.903      0.598\n",
      "Speed: 4.1ms preprocess, 206.7ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train55\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 10 époques dans : Assurances/YOLO_Trained/best_epoch_vf4.pt/best_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 02:06:10 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp85g6tasc\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/24 02:06:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 11 à 20...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train552, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train552\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train552', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 616 images, 4 backgrounds, 0 corrupt: 100%|██████████| 616/616 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 148 images, 7 backgrounds, 0 corrupt: 100%|██████████| 148/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train552\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 02:06:13 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/24 02:06:13 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/24 02:06:13 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/24 02:06:13 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/24 02:06:13 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(6ad7ac15bb0448939c5dc9d6aed69e33) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train552\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.116     0.7799      1.151          8        640: 100%|██████████| 20/20 [08:21<00:00, 25.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.834      0.816      0.861      0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.237     0.8997      1.213         13        640: 100%|██████████| 20/20 [08:21<00:00, 25.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.828      0.813      0.875      0.529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.221     0.8936      1.201          8        640: 100%|██████████| 20/20 [08:13<00:00, 24.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.625      0.753       0.68      0.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.222     0.8965      1.206          7        640: 100%|██████████| 20/20 [08:03<00:00, 24.17s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:39<00:00, 13.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.679      0.632      0.646      0.399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.202     0.8788      1.192          8        640: 100%|██████████| 20/20 [08:22<00:00, 25.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.634      0.623      0.545      0.346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.208     0.8653      1.209          9        640: 100%|██████████| 20/20 [08:40<00:00, 26.04s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:39<00:00, 13.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.802      0.688      0.771      0.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.183     0.8361      1.186          8        640: 100%|██████████| 20/20 [08:32<00:00, 25.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:39<00:00, 13.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.78      0.792       0.84      0.513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.126     0.7691      1.156          8        640: 100%|██████████| 20/20 [08:35<00:00, 25.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.824      0.818      0.845      0.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G        1.1     0.7247      1.135         11        640: 100%|██████████| 20/20 [08:32<00:00, 25.63s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.817       0.89        0.9      0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G      1.025     0.6944        1.1         10        640: 100%|██████████| 20/20 [08:08<00:00, 24.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.839      0.847      0.904      0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 1.517 hours.\n",
      "Optimizer stripped from runs\\detect\\train552\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train552\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train552\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:34<00:00, 11.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.839      0.846      0.904      0.599\n",
      "Speed: 3.9ms preprocess, 207.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train552\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 20 époques dans : Assurances/YOLO_Trained/best_epoch_vf4.pt/best_epoch_20.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 03:37:54 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp2ttw6ryt\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/24 03:37:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 21 à 30...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5522, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5522\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5522', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 616 images, 4 backgrounds, 0 corrupt: 100%|██████████| 616/616 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 148 images, 7 backgrounds, 0 corrupt: 100%|██████████| 148/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5522\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 03:37:57 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/24 03:37:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/24 03:37:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/24 03:37:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/24 03:37:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(6bbe7be66f104ac196c10038b84fddd5) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5522\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.9653     0.6164      1.064          8        640: 100%|██████████| 20/20 [08:17<00:00, 24.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.801      0.805      0.871      0.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.052     0.7124      1.104         13        640: 100%|██████████| 20/20 [08:30<00:00, 25.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.82      0.786      0.839      0.503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G       1.09     0.7369      1.124          8        640: 100%|██████████| 20/20 [08:28<00:00, 25.41s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:39<00:00, 13.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.714      0.584       0.63      0.434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.085     0.7739      1.143          7        640: 100%|██████████| 20/20 [08:39<00:00, 25.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.788      0.651      0.757      0.441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.093     0.7945      1.142          8        640: 100%|██████████| 20/20 [08:55<00:00, 26.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:39<00:00, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.851       0.85      0.894      0.555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.064     0.7326      1.129          9        640: 100%|██████████| 20/20 [08:23<00:00, 25.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.773      0.818      0.829      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.123     0.7389       1.15          8        640: 100%|██████████| 20/20 [08:18<00:00, 24.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.817      0.825      0.859      0.545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G       1.04     0.6891      1.118          8        640: 100%|██████████| 20/20 [08:27<00:00, 25.36s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.82      0.812      0.859       0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G      1.037     0.6573      1.113         11        640: 100%|██████████| 20/20 [08:48<00:00, 26.43s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.852      0.787      0.873      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.9552     0.6081      1.064         10        640: 100%|██████████| 20/20 [08:34<00:00, 25.75s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.885      0.901      0.921      0.632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 1.542 hours.\n",
      "Optimizer stripped from runs\\detect\\train5522\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train5522\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train5522\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:33<00:00, 11.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.885      0.901      0.921      0.632\n",
      "Speed: 3.5ms preprocess, 203.5ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5522\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 30 époques dans : Assurances/YOLO_Trained/best_epoch_vf4.pt/best_epoch_30.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 05:11:05 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpmp4mpcx1\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/24 05:11:05 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 31 à 40...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train55222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train55222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train55222', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 616 images, 4 backgrounds, 0 corrupt: 100%|██████████| 616/616 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 148 images, 7 backgrounds, 0 corrupt: 100%|██████████| 148/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train55222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 05:11:08 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/24 05:11:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/24 05:11:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/24 05:11:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/24 05:11:08 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(90c66706c98049c0b5a8a87d564c7275) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train55222\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.8631     0.5472      1.014          8        640: 100%|██████████| 20/20 [08:37<00:00, 25.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.891      0.853      0.915      0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G     0.9288     0.6115       1.04         13        640: 100%|██████████| 20/20 [08:21<00:00, 25.07s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.838      0.318      0.367      0.279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.006     0.6509      1.061          8        640: 100%|██████████| 20/20 [08:26<00:00, 25.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.842      0.795      0.858      0.522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.006     0.6856      1.098          7        640: 100%|██████████| 20/20 [08:21<00:00, 25.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.799      0.857      0.848      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.017     0.6881      1.086          8        640: 100%|██████████| 20/20 [08:19<00:00, 24.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.711      0.851      0.781      0.512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.001     0.6494      1.107          9        640: 100%|██████████| 20/20 [08:36<00:00, 25.82s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.85      0.825      0.872      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G     0.9904     0.6466      1.086          8        640: 100%|██████████| 20/20 [08:25<00:00, 25.25s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.782      0.906      0.884      0.562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.9705      0.622      1.081          8        640: 100%|██████████| 20/20 [08:33<00:00, 25.67s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.865       0.89      0.902       0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.9779     0.6321      1.085         11        640: 100%|██████████| 20/20 [08:30<00:00, 25.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.845       0.89      0.894      0.594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.9122     0.5737      1.045         10        640: 100%|██████████| 20/20 [08:22<00:00, 25.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.853      0.869      0.905      0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 1.527 hours.\n",
      "Optimizer stripped from runs\\detect\\train55222\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train55222\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train55222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:34<00:00, 11.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.891      0.852      0.915      0.609\n",
      "Speed: 3.7ms preprocess, 207.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train55222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 40 époques dans : Assurances/YOLO_Trained/best_epoch_vf4.pt/best_epoch_40.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 06:43:23 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpf81nguw5\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/24 06:43:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 41 à 50...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train552222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train552222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train552222', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 616 images, 4 backgrounds, 0 corrupt: 100%|██████████| 616/616 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 148 images, 7 backgrounds, 0 corrupt: 100%|██████████| 148/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train552222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 06:43:27 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/24 06:43:27 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/24 06:43:27 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/24 06:43:27 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/24 06:43:27 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(204b376c418a4313a68f0fd1b9a5a577) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train552222\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.7336     0.4791     0.9596          8        640: 100%|██████████| 20/20 [08:26<00:00, 25.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.849      0.864      0.887      0.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G     0.9525     0.5881      1.044         13        640: 100%|██████████| 20/20 [08:44<00:00, 26.24s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.911      0.864      0.914      0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G     0.9726     0.6446       1.05          8        640: 100%|██████████| 20/20 [08:22<00:00, 25.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.733      0.747      0.749      0.486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G     0.9653     0.6697      1.069          7        640: 100%|██████████| 20/20 [08:14<00:00, 24.71s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.705      0.806      0.793      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G     0.9816     0.7322      1.067          8        640: 100%|██████████| 20/20 [08:23<00:00, 25.18s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.706      0.805      0.809      0.494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G      1.012     0.6776       1.09          9        640: 100%|██████████| 20/20 [08:16<00:00, 24.80s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.781      0.877      0.883      0.566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.015     0.6654      1.094          8        640: 100%|██████████| 20/20 [08:08<00:00, 24.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:37<00:00, 12.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.808      0.874      0.873      0.539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.9468     0.6269      1.076          8        640: 100%|██████████| 20/20 [08:18<00:00, 24.95s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.892      0.825      0.904      0.592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.9415        0.6      1.078         11        640: 100%|██████████| 20/20 [08:30<00:00, 25.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.858      0.857      0.898      0.604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.9223     0.5528      1.052         10        640: 100%|██████████| 20/20 [08:37<00:00, 25.88s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.904      0.859      0.923      0.605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 1.519 hours.\n",
      "Optimizer stripped from runs\\detect\\train552222\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train552222\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train552222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:33<00:00, 11.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.904      0.859      0.923      0.606\n",
      "Speed: 3.7ms preprocess, 205.9ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train552222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 50 époques dans : Assurances/YOLO_Trained/best_epoch_vf4.pt/best_epoch_50.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 08:15:14 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmp3sa0rvfx\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/24 08:15:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 51 à 60...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train5522222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5522222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train5522222', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 616 images, 4 backgrounds, 0 corrupt: 100%|██████████| 616/616 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 148 images, 7 backgrounds, 0 corrupt: 100%|██████████| 148/148 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train5522222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 08:15:17 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/24 08:15:17 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/24 08:15:17 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/24 08:15:17 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/24 08:15:17 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(7149eafc35564179b8264f3ba2b34ab5) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5522222\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.7812     0.4963     0.9775          8        640: 100%|██████████| 20/20 [08:10<00:00, 24.52s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.902        0.9      0.919      0.627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G     0.8527     0.5389     0.9986         13        640: 100%|██████████| 20/20 [08:22<00:00, 25.13s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.855       0.87        0.9      0.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G     0.8854     0.5676      1.007          8        640: 100%|██████████| 20/20 [08:07<00:00, 24.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.79      0.825      0.856      0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      0.912     0.5956      1.047          7        640: 100%|██████████| 20/20 [09:35<00:00, 28.78s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.839       0.88      0.877      0.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G     0.9112     0.6025      1.043          8        640: 100%|██████████| 20/20 [08:51<00:00, 26.59s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.812      0.708      0.808      0.489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G     0.9285     0.6176      1.074          9        640: 100%|██████████| 20/20 [08:34<00:00, 25.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.839      0.812      0.882      0.587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G     0.9388     0.6255      1.068          8        640: 100%|██████████| 20/20 [1:15:34<00:00, 226.75s/it]  \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:38<00:00, 12.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.884      0.792      0.852      0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.9052     0.5807      1.051          8        640: 100%|██████████| 20/20 [10:37<00:00, 31.90s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [01:10<00:00, 23.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.82      0.711      0.783      0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.8981     0.5668      1.048         11        640: 100%|██████████| 20/20 [13:44<00:00, 41.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [01:13<00:00, 24.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.839      0.846      0.891      0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.8587     0.5164      1.028         10        640: 100%|██████████| 20/20 [14:29<00:00, 43.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [01:13<00:00, 24.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.912       0.89      0.936      0.619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 2.918 hours.\n",
      "Optimizer stripped from runs\\detect\\train5522222\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train5522222\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train5522222\\weights\\best.pt...\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 168 layers, 11,125,971 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [01:07<00:00, 22.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.902        0.9      0.919      0.626\n",
      "Speed: 2.9ms preprocess, 392.8ms inference, 0.0ms loss, 7.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train5522222\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 60 époques dans : Assurances/YOLO_Trained/best_epoch_vf4.pt/best_epoch_60.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 11:11:44 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: C:\\Users\\ilyes\\AppData\\Local\\Temp\\tmpw116e4tx\\model\\data, flavor: pytorch). Fall back to return ['torch==2.5.1', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/02/24 11:11:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 61 à 70...\n",
      "New https://pypi.org/project/ultralytics/8.3.78 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.49  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train55222222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train55222222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
      "Model summary: 225 layers, 11,135,987 parameters, 11,135,971 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train55222222', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\train\\labels.cache... 616 images, 4 backgrounds, 0 corrupt: 100%|██████████| 616/616 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\signature\\valid\\labels.cache... 148 images, 7 backgrounds, 0 corrupt: 100%|██████████| 148/148 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train55222222\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/24 11:11:53 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/24 11:11:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/24 11:11:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/24 11:11:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/24 11:11:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(99b4571ee6034376bbbb14919985419e) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train55222222\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G       0.66     0.4295     0.9283          8        640: 100%|██████████| 20/20 [11:54<00:00, 35.72s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:43<00:00, 14.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154      0.862      0.903      0.916      0.634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G     0.8696     0.5187     0.9986         13        640: 100%|██████████| 20/20 [10:38<00:00, 31.95s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:47<00:00, 15.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        148        154       0.82      0.818      0.832      0.559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G     0.8923     0.5816      1.018         33        640:  70%|███████   | 14/20 [09:41<04:09, 41.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "signature_yaml_path = \"C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml\"\n",
    "pretrained_model = \"yolov8s.pt\"\n",
    "output_model_dir = \"Assurances/YOLO_Trained/best_epoch_vf4.pt/\"\n",
    "\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "model = YOLO(pretrained_model)\n",
    "\n",
    "total_epochs = 100\n",
    "save_interval = 10\n",
    "imgsz = 640\n",
    "batch_size = 32\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:///\" + os.path.abspath(\"mlruns\"))\n",
    "mlflow.set_experiment(\"YOLOv8_Signature_Detection\")\n",
    "\n",
    "def download_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return np.array(img)\n",
    "\n",
    "def test_model_on_image(model_path, image_url, conf_threshold=0.3):\n",
    "    model_test = YOLO(model_path)\n",
    "    img = download_image_from_url(image_url)\n",
    "\n",
    "    results = model_test.predict(source=img, conf=conf_threshold)\n",
    "\n",
    "    image_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    for box in results[0].boxes.xyxy:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    cropped_signatures = []\n",
    "    for i, box in enumerate(results[0].boxes.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cropped_signature = img[y1:y2, x1:x2]\n",
    "        cropped_signatures.append(cropped_signature)\n",
    "\n",
    "    output_image_path = \"detection_result.png\"\n",
    "    cv2.imwrite(output_image_path, image_bgr)\n",
    "\n",
    "    return output_image_path, cropped_signatures\n",
    "\n",
    "\n",
    "print(\"\\n=== Début de l'entraînement ===\")\n",
    "\n",
    "for start_epoch in range(0, total_epochs, save_interval):\n",
    "    end_epoch = min(start_epoch + save_interval, total_epochs)\n",
    "    print(f\"\\n[INFO] Entraînement de l'époque {start_epoch + 1} à {end_epoch}...\")\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        results = model.train(\n",
    "            data=signature_yaml_path,\n",
    "            epochs=save_interval,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch_size,\n",
    "            val=True,\n",
    "            single_cls=True,\n",
    "            workers=4,\n",
    "            amp=True,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        last_model_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.pt\")\n",
    "        epoch_model_path = os.path.join(output_model_dir, f\"best_epoch_{end_epoch}.pt\")\n",
    "\n",
    "        if os.path.exists(last_model_path):\n",
    "            shutil.copy(last_model_path, epoch_model_path)\n",
    "            print(f\"[INFO] Modèle sauvegardé après {end_epoch} époques dans : {epoch_model_path}\")\n",
    "\n",
    "            mlflow.log_param(\"epochs\", end_epoch)\n",
    "            mlflow.log_param(\"imgsz\", imgsz)\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "            metrics = results.results_dict\n",
    "            mlflow.log_metrics({\n",
    "                \"precision\": metrics.get('metrics/precision(B)', 0),\n",
    "                \"recall\": metrics.get('metrics/recall(B)', 0),\n",
    "                \"mAP50\": metrics.get('metrics/mAP50(B)', 0),\n",
    "                \"mAP50-95\": metrics.get('metrics/mAP50-95(B)', 0)\n",
    "            })\n",
    "\n",
    "            mlflow.pytorch.log_model(model.model, artifact_path=f\"yolo_signature_epoch_{end_epoch}\")\n",
    "            mlflow.log_artifact(epoch_model_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"[WARNING] Aucun modèle best.pt trouvé après {end_epoch} époques.\")\n",
    "\n",
    "    mlflow.end_run()  \n",
    "\n",
    "print(\"\\n=== Entraînement terminé ===\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stamps Detection Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Début de l'entraînement ===\n",
      "\n",
      "[INFO] Entraînement de l'époque 1 à 10...\n",
      "Ultralytics 8.3.78  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/stamp-detect/data.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train64, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train64\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,136,374 parameters, 11,136,358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train64', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\train\\labels.cache... 2933 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2933/2933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 369, len(boxes) = 5578. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\valid\\labels.cache... 758 images, 0 backgrounds, 0 corrupt: 100%|██████████| 758/758 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 59, len(boxes) = 1436. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train64\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 02:17:42 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/25 02:17:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/25 02:17:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/25 02:17:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/25 02:17:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(80aa6e6cf9334281b37bf0b8b71993f8) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train64\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G      1.369      1.594       1.45         39        640: 100%|██████████| 92/92 [1:01:55<00:00, 40.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:23<00:00, 16.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.735      0.721      0.744      0.473\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G       1.29       1.01       1.41         39        640: 100%|██████████| 92/92 [38:34<00:00, 25.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:15<00:00, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.793       0.73      0.782      0.506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.215      0.895      1.362         41        640: 100%|██████████| 92/92 [38:24<00:00, 25.05s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:19<00:00, 16.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.769      0.728      0.784      0.452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.176     0.8457      1.333         42        640: 100%|██████████| 92/92 [38:22<00:00, 25.02s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:16<00:00, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.796      0.807      0.801      0.531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.126     0.7984       1.31         32        640: 100%|██████████| 92/92 [38:29<00:00, 25.11s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:16<00:00, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.917      0.868      0.928      0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G       1.09     0.7344      1.274         36        640: 100%|██████████| 92/92 [38:40<00:00, 25.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:21<00:00, 16.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.941      0.876      0.935      0.653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G      1.054     0.6787      1.249         37        640: 100%|██████████| 92/92 [38:11<00:00, 24.91s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:16<00:00, 16.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.956      0.883      0.943       0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G      1.013     0.6499      1.219         39        640: 100%|██████████| 92/92 [38:34<00:00, 25.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:15<00:00, 16.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.959      0.893      0.951      0.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.9762     0.6152      1.192         34        640: 100%|██████████| 92/92 [38:38<00:00, 25.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:16<00:00, 16.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.972      0.907       0.96      0.714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.9352     0.5762      1.171         41        640: 100%|██████████| 92/92 [38:39<00:00, 25.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:16<00:00, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.973      0.911      0.967      0.725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 7.368 hours.\n",
      "Optimizer stripped from runs\\detect\\train64\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train64\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train64\\weights\\best.pt...\n",
      "Ultralytics 8.3.78  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 72 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [02:53<00:00, 14.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.973      0.911      0.967      0.725\n",
      "             signature        758       1436      0.973      0.911      0.967      0.725\n",
      "Speed: 3.9ms preprocess, 216.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train64\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 10 époques dans : Assurances/YOLO_Trained/best_epoch_vf5.pt/best_epoch_10.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 09:43:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 11 à 20...\n",
      "Ultralytics 8.3.78  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/stamp-detect/data.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train642, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train642\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,136,374 parameters, 11,136,358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train642', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\train\\labels.cache... 2933 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2933/2933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 369, len(boxes) = 5578. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\valid\\labels.cache... 758 images, 0 backgrounds, 0 corrupt: 100%|██████████| 758/758 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 59, len(boxes) = 1436. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\detect\\train642\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 09:43:15 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/25 09:43:15 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/25 09:43:15 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/25 09:43:15 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/25 09:43:15 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(1c72f1c7792e45f494b4aea27e6d79f2) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train642\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.9413     0.5841      1.172         39        640: 100%|██████████| 92/92 [37:55<00:00, 24.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:14<00:00, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.943      0.891      0.948      0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      1.029     0.6742      1.227         39        640: 100%|██████████| 92/92 [38:08<00:00, 24.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:17<00:00, 16.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.911      0.848      0.921       0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      1.031     0.6733      1.234         41        640: 100%|██████████| 92/92 [38:40<00:00, 25.22s/it] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:18<00:00, 16.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.915      0.873      0.937      0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G      1.025     0.6945      1.231         42        640: 100%|██████████| 92/92 [38:55<00:00, 25.39s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:17<00:00, 16.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.913      0.847      0.909      0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G      1.008     0.6619      1.217         32        640: 100%|██████████| 92/92 [38:27<00:00, 25.08s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:17<00:00, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436       0.94      0.895      0.945      0.682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G     0.9737     0.5989       1.19         36        640: 100%|██████████| 92/92 [38:49<00:00, 25.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:15<00:00, 16.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.929      0.896      0.946      0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G     0.9419     0.5761      1.174         37        640: 100%|██████████| 92/92 [41:02<00:00, 26.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:24<00:00, 17.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.957      0.904      0.955      0.692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.9101     0.5558      1.152         39        640: 100%|██████████| 92/92 [38:33<00:00, 25.15s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:19<00:00, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.971      0.922      0.966      0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.8839     0.5298      1.141         34        640: 100%|██████████| 92/92 [38:38<00:00, 25.20s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:26<00:00, 17.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.969      0.929      0.968      0.745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.8487     0.4995      1.123         41        640: 100%|██████████| 92/92 [38:30<00:00, 25.12s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:18<00:00, 16.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.979      0.926      0.974      0.754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 7.025 hours.\n",
      "Optimizer stripped from runs\\detect\\train642\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train642\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train642\\weights\\best.pt...\n",
      "Ultralytics 8.3.78  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 72 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [02:52<00:00, 14.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.979      0.927      0.974      0.754\n",
      "             signature        758       1436      0.979      0.927      0.974      0.754\n",
      "Speed: 3.5ms preprocess, 216.0ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train642\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 20 époques dans : Assurances/YOLO_Trained/best_epoch_vf5.pt/best_epoch_20.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 16:47:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 21 à 30...\n",
      "New https://pypi.org/project/ultralytics/8.3.79 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.78  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/stamp-detect/data.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train6422, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train6422\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,136,374 parameters, 11,136,358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train6422', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\train\\labels.cache... 2933 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2933/2933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 369, len(boxes) = 5578. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\valid\\labels.cache... 758 images, 0 backgrounds, 0 corrupt: 100%|██████████| 758/758 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 59, len(boxes) = 1436. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "Plotting labels to runs\\detect\\train6422\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/25 16:48:00 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/25 16:48:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/25 16:48:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/25 16:48:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/25 16:48:00 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(144880b16c66477e8a3dd5fb6b74e5da) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train6422\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.8428     0.4934      1.112         39        640: 100%|██████████| 92/92 [37:58<00:00, 24.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:15<00:00, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.959      0.917      0.967      0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G      0.923     0.5568      1.153         39        640: 100%|██████████| 92/92 [38:18<00:00, 24.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:17<00:00, 16.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.956      0.875      0.943       0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G     0.9212     0.5687      1.162         41        640: 100%|██████████| 92/92 [38:12<00:00, 24.92s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:15<00:00, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.944       0.86       0.94      0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G     0.9343     0.5826       1.17         42        640: 100%|██████████| 92/92 [38:56<00:00, 25.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:14<00:00, 16.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.911      0.864      0.926      0.678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10         0G     0.9189     0.5724      1.163         32        640: 100%|██████████| 92/92 [39:04<00:00, 25.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:16<00:00, 16.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.949      0.843      0.911      0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10         0G     0.8988     0.5401      1.144         36        640: 100%|██████████| 92/92 [42:05<00:00, 27.45s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:15<00:00, 16.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.977      0.912      0.969      0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10         0G     0.8734     0.5115      1.133         37        640: 100%|██████████| 92/92 [41:02<00:00, 26.76s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:31<00:00, 17.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.973      0.917      0.967      0.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10         0G     0.8472     0.4909      1.115         39        640: 100%|██████████| 92/92 [1:13:24<00:00, 47.87s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:34<00:00, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.972       0.93      0.972      0.743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10         0G     0.8352     0.4739      1.107         34        640: 100%|██████████| 92/92 [52:03<00:00, 33.96s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:26<00:00, 17.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.971      0.938      0.976      0.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10         0G     0.8068     0.4576      1.093         41        640: 100%|██████████| 92/92 [42:49<00:00, 27.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:26<00:00, 17.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.968      0.945      0.977      0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 7.970 hours.\n",
      "Optimizer stripped from runs\\detect\\train6422\\weights\\last.pt, 22.5MB\n",
      "Optimizer stripped from runs\\detect\\train6422\\weights\\best.pt, 22.5MB\n",
      "\n",
      "Validating runs\\detect\\train6422\\weights\\best.pt...\n",
      "Ultralytics 8.3.78  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Model summary (fused): 72 layers, 11,126,358 parameters, 0 gradients, 28.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [02:59<00:00, 14.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.968      0.946      0.977      0.763\n",
      "             signature        758       1436      0.968      0.946      0.977      0.763\n",
      "Speed: 3.5ms preprocess, 220.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\train6422\u001b[0m\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mresults logged to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "[INFO] Modèle sauvegardé après 30 époques dans : Assurances/YOLO_Trained/best_epoch_vf5.pt/best_epoch_30.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/26 00:49:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Entraînement de l'époque 31 à 40...\n",
      "New https://pypi.org/project/ultralytics/8.3.79 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.78  Python-3.12.4 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=C:/Users/ilyes/intelligIA/datasets/stamp-detect/data.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train64222, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.0, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train64222\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2116822  ultralytics.nn.modules.head.Detect           [2, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,136,374 parameters, 11,136,358 gradients, 28.6 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train64222', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\train\\labels.cache... 2933 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2933/2933 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 369, len(boxes) = 5578. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\ilyes\\intelligIA\\datasets\\stamp-detect\\valid\\labels.cache... 758 images, 0 backgrounds, 0 corrupt: 100%|██████████| 758/758 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING  Box and segment counts should be equal, but got len(segments) = 59, len(boxes) = 1436. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n",
      "Plotting labels to runs\\detect\\train64222\\labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/02/26 00:49:41 INFO mlflow.bedrock: Enabled auto-tracing for Bedrock. Note that MLflow can only trace boto3 service clients that are created after this call. If you have already created one, please recreate the client by calling `boto3.client`.\n",
      "2025/02/26 00:49:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for boto3.\n",
      "2025/02/26 00:49:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for keras.\n",
      "2025/02/26 00:49:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n",
      "2025/02/26 00:49:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(c36950f1754046ac89c22d62e423cf3d) to file:///c:\\Users\\ilyes\\intelligIA\\mlruns\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train64222\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10         0G     0.7852     0.4442      1.076         39        640: 100%|██████████| 92/92 [41:54<00:00, 27.34s/it] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:29<00:00, 17.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.965      0.942      0.975      0.747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10         0G     0.8419     0.4949      1.104         39        640: 100%|██████████| 92/92 [57:54<00:00, 37.77s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [06:56<00:00, 34.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.961      0.927      0.969      0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10         0G      0.859     0.5036      1.116         41        640: 100%|██████████| 92/92 [55:44<00:00, 36.35s/it]   \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 12/12 [03:17<00:00, 16.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        758       1436      0.961      0.902      0.959      0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10         0G     0.8771     0.5269      1.134         59        640:  91%|█████████▏| 84/92 [54:29<05:11, 38.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[INFO] Entraînement de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mépoque \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m à \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run(nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 64\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature_yaml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     74\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     last_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     77\u001b[0m     epoch_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_model_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:810\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:389\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    385\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    386\u001b[0m     )\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "signature_yaml_path = \"C:/Users/ilyes/intelligIA/datasets/stamp-detect/data.yaml\"\n",
    "pretrained_model = \"yolov8s.pt\"\n",
    "output_model_dir = \"Assurances/YOLO_Trained/best_epoch_vf5.pt/\"\n",
    "\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "model = YOLO(pretrained_model)\n",
    "\n",
    "total_epochs = 100\n",
    "save_interval = 10\n",
    "imgsz = 640\n",
    "batch_size = 32\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:///\" + os.path.abspath(\"mlruns\"))\n",
    "mlflow.set_experiment(\"YOLOv8_Stamps_Detection\")\n",
    "\n",
    "def download_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return np.array(img)\n",
    "\n",
    "def test_model_on_image(model_path, image_url, conf_threshold=0.3):\n",
    "    model_test = YOLO(model_path)\n",
    "    img = download_image_from_url(image_url)\n",
    "\n",
    "    results = model_test.predict(source=img, conf=conf_threshold)\n",
    "\n",
    "    image_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    for box in results[0].boxes.xyxy:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    cropped_signatures = []\n",
    "    for i, box in enumerate(results[0].boxes.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cropped_signature = img[y1:y2, x1:x2]\n",
    "        cropped_signatures.append(cropped_signature)\n",
    "\n",
    "    output_image_path = \"detection_result.png\"\n",
    "    cv2.imwrite(output_image_path, image_bgr)\n",
    "\n",
    "    return output_image_path, cropped_signatures\n",
    "\n",
    "\n",
    "print(\"\\n=== Début de l'entraînement ===\")\n",
    "\n",
    "for start_epoch in range(0, total_epochs, save_interval):\n",
    "    end_epoch = min(start_epoch + save_interval, total_epochs)\n",
    "    print(f\"\\n[INFO] Entraînement de l'époque {start_epoch + 1} à {end_epoch}...\")\n",
    "\n",
    "    with mlflow.start_run(nested=True):\n",
    "        results = model.train(\n",
    "            data=signature_yaml_path,\n",
    "            epochs=save_interval,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch_size,\n",
    "            val=True,\n",
    "            single_cls=True,\n",
    "            workers=4,\n",
    "            amp=True,\n",
    "            verbose=True\n",
    "        )\n",
    "\n",
    "        last_model_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.pt\")\n",
    "        epoch_model_path = os.path.join(output_model_dir, f\"best_epoch_{end_epoch}.pt\")\n",
    "\n",
    "        if os.path.exists(last_model_path):\n",
    "            shutil.copy(last_model_path, epoch_model_path)\n",
    "            print(f\"[INFO] Modèle sauvegardé après {end_epoch} époques dans : {epoch_model_path}\")\n",
    "\n",
    "            mlflow.log_param(\"epochs\", end_epoch)\n",
    "            mlflow.log_param(\"imgsz\", imgsz)\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "            metrics = results.results_dict\n",
    "            mlflow.log_metrics({\n",
    "                \"precision\": metrics.get('metrics/precision(B)', 0),\n",
    "                \"recall\": metrics.get('metrics/recall(B)', 0),\n",
    "                \"mAP50\": metrics.get('metrics/mAP50(B)', 0),\n",
    "                \"mAP50-95\": metrics.get('metrics/mAP50-95(B)', 0)\n",
    "            })\n",
    "\n",
    "            mlflow.pytorch.log_model(model.model, artifact_path=f\"yolo_stamp_epoch_{end_epoch}\")\n",
    "            mlflow.log_artifact(epoch_model_path)\n",
    "\n",
    "        else:\n",
    "            print(f\"[WARNING] Aucun modèle best.pt trouvé après {end_epoch} époques.\")\n",
    "\n",
    "    mlflow.end_run()  \n",
    "\n",
    "print(\"\\n=== Entraînement terminé ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\ilyes\\intelligIA\\datasets\\Er075045GANA020242602H1_page-0001.jpg: 480x640 1 signature, 418.5ms\n",
      "Speed: 43.7ms preprocess, 418.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Nombre total de détections (avant filtrage) : 1\n",
      "✅ Signature enregistrée : ./results/signature_1.png\n",
      "✅ Image avec signatures détectées enregistrée : ./results/detections_with_boxes.png\n",
      "✅ Nombre final de signatures détectées (après filtrage) : 1\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "trained_model_path = \"Assurances/YOLO_Trained/best_epoch_vf6.pt/best_epoch_30.pt\"\n",
    "model = YOLO(trained_model_path)\n",
    "\n",
    "test_image = \"./datasets/Er075045GANA020242602H1_page-0001.jpg\"\n",
    "#test_image = \"./Assurances/Converted_Images/2_page_1.png\"\n",
    "\n",
    "\n",
    "output_folder = \"./results/\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "results = model.predict(test_image, conf=0.5) \n",
    "\n",
    "\n",
    "image = cv2.imread(test_image)\n",
    "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  \n",
    "\n",
    "detections = results[0].boxes.xyxy  \n",
    "scores = results[0].boxes.conf  \n",
    "labels = results[0].boxes.cls  \n",
    "\n",
    "\n",
    "min_width = 30  \n",
    "min_height = 10\n",
    "max_ratio = 5.0  \n",
    "\n",
    "filtered_detections = []\n",
    "print(f\"Nombre total de détections (avant filtrage) : {len(detections)}\")\n",
    "\n",
    "for i, box in enumerate(detections):\n",
    "    x1, y1, x2, y2 = map(int, box)\n",
    "    confidence = scores[i].item()\n",
    "    \n",
    "  \n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    ratio = width / (height + 1e-5)  \n",
    "\n",
    "\n",
    "    if width >= min_width and height >= min_height and ratio < max_ratio and confidence > 0.5:\n",
    "        filtered_detections.append((x1, y1, x2, y2, confidence))\n",
    "    \n",
    "      \n",
    "        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(image_rgb, f\"Signature: {confidence:.2f}\", (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "        \n",
    "     \n",
    "        cropped_signature = image[y1:y2, x1:x2]\n",
    "        cropped_output_path = os.path.join(output_folder, f\"signature_{i + 1}.png\")\n",
    "        cv2.imwrite(cropped_output_path, cropped_signature)\n",
    "        print(f\"✅ Signature enregistrée : {cropped_output_path}\")\n",
    "\n",
    "\n",
    "output_image_path = os.path.join(output_folder, \"detections_with_boxes.png\")\n",
    "cv2.imwrite(output_image_path, cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR))\n",
    "print(f\"✅ Image avec signatures détectées enregistrée : {output_image_path}\")\n",
    "\n",
    "print(f\"✅ Nombre final de signatures détectées (après filtrage) : {len(filtered_detections)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001FA0D0D5130>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\ilyes\\anaconda3\\Lib\\threading.py\", line 1533, in enumerate\n",
      "    def enumerate():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Début de l'entraînement ===\n",
      "[INFO] Entraînement de l'époque 1 à 10...\n",
      "Ultralytics 8.3.88  Python-3.12.3 torch-2.5.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=Assurances/YOLO_Trained/yolov8sFinal.pt, data=C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml, epochs=10, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=4, project=None, name=train72, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=True, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.3, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.0001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=5.0, cls=1.0, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train72\n",
      "Overriding class names with single class.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m notf  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (c:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorboard\\compat\\__init__.py)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 156\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m choices[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    155\u001b[0m fake_trial \u001b[38;5;241m=\u001b[39m FakeTrial()\n\u001b[1;32m--> 156\u001b[0m \u001b[43mtrain_with_hyperparams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_trial\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m, in \u001b[0;36mtrain_with_hyperparams\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     76\u001b[0m end_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_epoch \u001b[38;5;241m+\u001b[39m save_interval, total_epochs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INFO] Entraînement de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mépoque \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_epoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m à \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature_yaml_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbox\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m last_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m epoch_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_model_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ultralytics\\engine\\model.py:804\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    802\u001b[0m     args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt_path\n\u001b[1;32m--> 804\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrainer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresume\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:156\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[1;34m(self, cfg, overrides, _callbacks)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m _callbacks \u001b[38;5;129;01mor\u001b[39;00m callbacks\u001b[38;5;241m.\u001b[39mget_default_callbacks()\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n\u001b[1;32m--> 156\u001b[0m     \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_integration_callbacks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ultralytics\\utils\\callbacks\\base.py:208\u001b[0m, in \u001b[0;36madd_integration_callbacks\u001b[1;34m(instance)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneptune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m neptune_cb\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mraytune\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m tune_cb\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m tb_cb\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m wb_cb\n\u001b[0;32m    211\u001b[0m callbacks_list\u001b[38;5;241m.\u001b[39mextend([clear_cb, comet_cb, dvc_cb, mlflow_cb, neptune_cb, tune_cb, tb_cb, wb_cb])\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\ultralytics\\utils\\callbacks\\tensorboard.py:7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01multralytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LOGGER, SETTINGS, TESTS_RUNNING, colorstr\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# WARNING: do not move SummaryWriter import due to protobuf bug https://github.com/ultralytics/ultralytics/pull/4674\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensorboard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m TESTS_RUNNING  \u001b[38;5;66;03m# do not log pytest\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SETTINGS[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorboard\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# verify integration is enabled\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\torch\\utils\\tensorboard\\__init__.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tensorboard\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileWriter, SummaryWriter  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecord_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecordWriter  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\torch\\utils\\tensorboard\\writer.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwriter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_file_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EventFileWriter\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_convert_np\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_np\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_embedding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_embedding_info, make_mat, make_sprite, make_tsv, write_pbtxt\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_onnx_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_onnx_graph\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pytorch_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m graph\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\torch\\utils\\tensorboard\\_embedding.py:10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorboard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplugins\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprojector_config_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EmbeddingInfo\n\u001b[1;32m---> 10\u001b[0m _HAS_GFILE_JOIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mhasattr\u001b[39m(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241m.\u001b[39mgfile, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gfile_join\u001b[39m(a, b):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# The join API is different between tensorboard's TF stub and TF:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# https://github.com/tensorflow/tensorboard/issues/6080\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# We need to try both because `tf` may point to either the stub or the real TF.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _HAS_GFILE_JOIN:\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorboard\\lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[1;34m(self, attr_name)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[43mload_once\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m, attr_name)\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorboard\\lazy.py:97\u001b[0m, in \u001b[0;36m_memoize.<locals>.wrapper\u001b[1;34m(arg)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m lock:\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m cache\u001b[38;5;241m.\u001b[39mget(arg, nothing) \u001b[38;5;129;01mis\u001b[39;00m nothing:\n\u001b[1;32m---> 97\u001b[0m             cache[arg] \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cache[arg]\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorboard\\lazy.py:50\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.load_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m load_once\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mload_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     load_once\u001b[38;5;241m.\u001b[39mloading \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorboard\\compat\\__init__.py:45\u001b[0m, in \u001b[0;36mtf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 45\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tensorflow\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\__init__.py:49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[0;32m     47\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\__init__.py:11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m eager_context\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m feature_column\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m combinations\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interim\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\_api\\v2\\__internal__\\distribute\\combinations\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.distribute.combinations namespace\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m env \u001b[38;5;66;03m# line: 456\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m generate \u001b[38;5;66;03m# line: 365\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m in_main_process \u001b[38;5;66;03m# line: 418\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\distribute\\combinations.py:33\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session\n\u001b[1;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_all_reduce_strategy\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multi_process_runner\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\distribute\\collective_all_reduce_strategy.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensorflow_server_pb2\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_ops \u001b[38;5;28;01mas\u001b[39;00m cross_device_ops_lib\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_lib\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cross_device_utils\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_utils\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_utils.py:22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, List, Optional, Union\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m values \u001b[38;5;28;01mas\u001b[39;00m value_lib\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop_util\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\distribute\\values.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m struct_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute_lib\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m packed_distributed_variable \u001b[38;5;28;01mas\u001b[39;00m packed\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce_util\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:205\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_ctx \u001b[38;5;28;01mas\u001b[39;00m autograph_ctx\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m api \u001b[38;5;28;01mas\u001b[39;00m autograph\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataset_ops\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m collective_util\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m device_util\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\data\\__init__.py:21\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\__init__.py:99\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_sparse_batch\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\service\\__init__.py:419\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis module contains:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_dataset\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\data_service_ops.py:23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data_service_pb2\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_server_lib\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils_exp\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\compression_ops.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompress\u001b[39m(element):\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\data\\util\\structure.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_logging \u001b[38;5;28;01mas\u001b[39;00m logging\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m internal\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\__init__.py:28\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Ragged Tensors.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03mThis package defines ops for manipulating ragged tensors (`tf.RaggedTensor`),\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;124;03mAPI docstring: tensorflow.ragged\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_tensor\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_tensor.py:3149\u001b[0m\n\u001b[0;32m   3144\u001b[0m RaggedOrDense \u001b[38;5;241m=\u001b[39m typing\u001b[38;5;241m.\u001b[39mUnion[Ragged, core_types\u001b[38;5;241m.\u001b[39mTensorLike]\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;66;03m# RaggedTensor must import ragged_ops to ensure that all dispatched ragged ops\u001b[39;00m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;66;03m# are registered. Ragged ops import RaggedTensor, so import at bottom of the\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;66;03m# file to avoid a partially-initialized module error.\u001b[39;00m\n\u001b[1;32m-> 3149\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_ops  \u001b[38;5;66;03m# pylint: disable=unused-import, g-bad-import-order, g-import-not-at-top\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_ops.py:41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_gather_ops\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_getitem\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_image_ops\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_map_ops\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mragged\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ragged_math_ops\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\ragged\\ragged_image_ops.py:24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_ops\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m map_fn\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\image_ops.py:159\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgen_image_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_ops_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# TODO(drpng): remove these once internal use has discontinued.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_ops_impl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _Check3DImage\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:40\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_image_ops\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m math_ops\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn_impl\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn_ops\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random_ops\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m candidate_sampling_ops\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cond \u001b[38;5;28;01mas\u001b[39;00m tf_cond\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ctc_ops  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_gradient\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m embedding_ops\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\ctc_ops.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m array_ops_stack\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m custom_gradient\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_ops\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_array_ops\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\custom_gradient.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Decorator to overrides the gradient for a function.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backprop\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m record\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:45\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_array_ops\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_math_ops\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gradients_impl  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel_for\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_flow_ops \u001b[38;5;28;01mas\u001b[39;00m pfor_ops\n",
      "File \u001b[1;32mc:\\Users\\ilyes\\intelligIA\\myenv\\Lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rnn_grad  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sdca_ops  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sets  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_grad  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_grad  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:991\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1087\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1186\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import requests\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "def get_hyperparams(trial):\n",
    "    \"\"\"\n",
    "    Cette fonction illustre comment récupérer\n",
    "    des hyperparamètres (ex: Optuna).\n",
    "    \"\"\"\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5, step=0.1)\n",
    "    lr0 = trial.suggest_float(\"lr0\", 1e-5, 1e-1, log=True)\n",
    "    box = trial.suggest_float(\"box\", 3.0, 7.0, step=1.0)\n",
    "    cls = trial.suggest_float(\"cls\", 0.5, 1.5, step=0.2)\n",
    "    opt = trial.suggest_categorical(\"optimizer\", [\"AdamW\", \"RMSProp\"])\n",
    "    return dropout, lr0, box, cls, opt\n",
    "\n",
    "\n",
    "signature_yaml_path = \"C:/Users/ilyes/intelligIA/datasets/signature/signature.yaml\"\n",
    "pretrained_model = \"Assurances/YOLO_Trained/yolov8sFinal.pt\"\n",
    "\n",
    "output_model_dir = \"Assurances/YOLO_Trained/best_epoch_vf9.pt/\"\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "total_epochs = 100\n",
    "save_interval = 10\n",
    "imgsz = 640\n",
    "batch_size = 32\n",
    "\n",
    "def download_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content))\n",
    "    return np.array(img)\n",
    "\n",
    "def test_model_on_image(model_path, image_url, conf_threshold=0.3):\n",
    "    model_test = YOLO(model_path)\n",
    "    img = download_image_from_url(image_url)\n",
    "\n",
    "    results = model_test.predict(source=img, conf=conf_threshold)\n",
    "\n",
    "    image_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    for box in results[0].boxes.xyxy:\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "    cropped_signatures = []\n",
    "    for i, box in enumerate(results[0].boxes.xyxy):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cropped_signature = img[y1:y2, x1:x2]\n",
    "        cropped_signatures.append(cropped_signature)\n",
    "\n",
    "    output_image_path = \"detection_result.png\"\n",
    "    cv2.imwrite(output_image_path, image_bgr)\n",
    "\n",
    "    return output_image_path, cropped_signatures\n",
    "\n",
    "def train_with_hyperparams(trial):\n",
    "    \"\"\"\n",
    "    Lance l'entraînement en utilisant les hyperparamètres\n",
    "    suggérés par 'trial', puis tente d'exporter le meilleur .pt en .onnx à la fin.\n",
    "    \"\"\"\n",
    "\n",
    "    dropout, lr0, box, cls, optimizer_name = get_hyperparams(trial)\n",
    "\n",
    "    \n",
    "    model = YOLO(pretrained_model)\n",
    "\n",
    "    print(\"\\n=== Début de l'entraînement ===\")\n",
    "    best_model_path = None\n",
    "    for start_epoch in range(0, total_epochs, save_interval):\n",
    "        end_epoch = min(start_epoch + save_interval, total_epochs)\n",
    "        print(f\"[INFO] Entraînement de l'époque {start_epoch + 1} à {end_epoch}...\")\n",
    "\n",
    "        results = model.train(\n",
    "            data=signature_yaml_path,\n",
    "            epochs=save_interval,\n",
    "            imgsz=imgsz,\n",
    "            batch=batch_size,\n",
    "            val=True,\n",
    "            single_cls=True,\n",
    "            workers=4,\n",
    "            amp=True,\n",
    "            verbose=True,\n",
    "            optimizer=optimizer_name,\n",
    "            lr0=lr0,\n",
    "            box=box,\n",
    "            cls=cls,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        last_model_path = os.path.join(\"runs\", \"detect\", \"train\", \"weights\", \"best.pt\")\n",
    "        epoch_model_path = os.path.join(output_model_dir, f\"best_epoch_{end_epoch}.pt\")\n",
    "\n",
    "        if os.path.exists(last_model_path):\n",
    "            shutil.copy(last_model_path, epoch_model_path)\n",
    "            print(f\"[INFO] Modèle sauvegardé après {end_epoch} époques dans : {epoch_model_path}\")\n",
    "            best_model_path = epoch_model_path\n",
    "        else:\n",
    "            print(f\"[WARNING] Aucun modèle best.pt trouvé après {end_epoch} époques.\")\n",
    "\n",
    "    print(\"\\n=== Entraînement terminé ===\")\n",
    "    if best_model_path and os.path.exists(best_model_path):\n",
    "        print(\"[INFO] Exportation du modèle au format ONNX...\")\n",
    "        try:\n",
    "            final_model = YOLO(best_model_path)\n",
    "            final_model.export(format=\"onnx\", imgsz=imgsz, opset=12)\n",
    "\n",
    "\n",
    "            onnx_exported_path = os.path.join(\"runs\", \"export\", \"model.onnx\")\n",
    "            if os.path.exists(onnx_exported_path):\n",
    "                final_onnx_path = os.path.join(output_model_dir, \"best_model.onnx\")\n",
    "                shutil.move(onnx_exported_path, final_onnx_path)\n",
    "                print(f\"[INFO] Modèle ONNX final sauvegardé ici : {final_onnx_path}\")\n",
    "            else:\n",
    "                print(\"[WARNING] Fichier .onnx non trouvé dans runs/export/. Vérifiez l'export.\")\n",
    "        except ImportError as e:\n",
    "            print(\"[ERROR] Échec de l'export ONNX :\")\n",
    "            print(e)\n",
    "            print(\"Assurez-vous que onnx et onnxruntime sont installés correctement,\")\n",
    "            print(\"ainsi que les Redistribuables Visual C++ (Windows).\")\n",
    "        except Exception as e:\n",
    "            print(\"[ERROR] Une erreur inattendue est survenue lors de l'export ONNX :\")\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"[WARNING] Aucune sauvegarde .pt pour l'export ONNX.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    class FakeTrial:\n",
    "        \"\"\"\n",
    "        Classe simulant un objet 'trial' d'Optuna \n",
    "        pour illustrer l'appel de la fonction.\n",
    "        \"\"\"\n",
    "        def suggest_float(self, name, low, high, step=None, log=False):\n",
    "            if name == \"dropout\":\n",
    "                return 0.3\n",
    "            elif name == \"lr0\":\n",
    "                return 1e-4\n",
    "            elif name == \"box\":\n",
    "                return 5.0\n",
    "            elif name == \"cls\":\n",
    "                return 1.0\n",
    "            return 0.0\n",
    "\n",
    "        def suggest_categorical(self, name, choices):\n",
    "            if name == \"optimizer\":\n",
    "                return \"AdamW\"\n",
    "            return choices[0]\n",
    "\n",
    "    fake_trial = FakeTrial()\n",
    "    train_with_hyperparams(fake_trial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Modele :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier CSV 'signature_models.csv' a été créé avec succès !\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "csv_filename = \"signature_models.csv\"\n",
    "\n",
    "data = [\n",
    "    [\"Model\", \"Epoch\", \"GPU_mem\", \"box_loss\", \"cls_loss\", \"dfl_loss\", \"Instances\", \"Size\", \"P\", \"R\", \"mAP50\", \"mAP50-95\", \"inference_time_ms\"],\n",
    "    [\"yolo11n\", \"10/10\", \"0G\", \"1.15\", \"0.6359\", \"1.208\", \"49\", \"640\", \"0.898\", \"0.88\", \"0.923\", \"0.557\", \"300\"],\n",
    "    [\"yolo8s_finetune\", \"9/10\", \"0G\", \"0.654\", \"0.969\", \"1.053\", \"17\", \"640\", \"0.935\", \"0.912\", \"0.955\", \"0.668\", \"150\"],\n",
    "    [\"yolo8s\", \"10/10\", \"0G\", \"0.652\", \"0.9374\", \"1.093\", \"16\", \"640\", \"0.943\", \"0.916\", \"0.95\", \"0.662\", \"120\"],\n",
    "    [\"yolo8n\", \"1/5\", \"0G\", \"0.6373\", \"0.4374\", \"0.9467\", \"44\", \"640\", \"0.9\", \"0.881\", \"0.925\", \"0.549\", \"80\"]\n",
    "]\n",
    "\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(data)\n",
    "\n",
    "print(f\"Le fichier CSV '{csv_filename}' a été créé avec succès !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"signature_models.csv\")\n",
    "\n",
    "\n",
    "x = df[\"inference_time_ms\"]  \n",
    "y = df[\"mAP50\"]             \n",
    "labels = df[\"Model\"]        \n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x, y, color=\"royalblue\")\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    plt.text(x[i] + 3, y[i], labels[i], fontsize=9)\n",
    "\n",
    "\n",
    "plt.xlabel(\"Inference Time (ms) - CPU\")\n",
    "plt.ylabel(\"mAP50\")\n",
    "plt.title(\"Speed vs. Precision (mAP50)\")\n",
    "\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
